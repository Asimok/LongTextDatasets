nohup: ignoring input
nohup: ignoring input
Traceback (most recent call last):
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/runpy.py", line 183, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/runpy.py", line 109, in _get_module_details
    __import__(pkg_name)
  File "/data2/maqi/LongTextDatasets/LongTextModels/main/main.py", line 5, in <module>
    from LongTextModels.model.trainer import Trainer
  File "/data2/maqi/LongTextDatasets/LongTextModels/model/trainer.py", line 13, in <module>
    from LongTextModels.dataloader.dataLoader import load_dataset
  File "/data2/maqi/LongTextDatasets/LongTextModels/dataloader/dataLoader.py", line 13, in <module>
    logger = get_logger(log_name="DropDataloader")
  File "/data2/maqi/LongTextDatasets/LongTextModels/tools/logger.py", line 41, in get_logger
    file_handler = logging.FileHandler(log_dir)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/logging/__init__.py", line 1087, in __init__
    StreamHandler.__init__(self, self._open())
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/logging/__init__.py", line 1116, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding)
FileNotFoundError: [Errno 2] No such file or directory: '/data2/maqi/LongTextDatasets/LongTextModels/output/exp_5-5/logs/log.txt'
nohup: ignoring input
Some weights of the model checkpoint at /data2/wangbingchao/database/bert_pretrained/bert-base-uncased were not used when initializing SentenceChoice: ['bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'cls.seq_relationship.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.pooler.dense.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'cls.predictions.bias', 'bert.encoder.layer.8.output.dense.bias', 'cls.predictions.decoder.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.bias']
- This IS expected if you are initializing SentenceChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SentenceChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of SentenceChoice were not initialized from the model checkpoint at /data2/wangbingchao/database/bert_pretrained/bert-base-uncased and are newly initialized: ['c_lstm.1.bias_ih_l2_reverse', 'c_lstm.2.bias_ih_l1_reverse', 'c_lstm.0.weight_hh_l1', 'attention.k_proj.bias', 'c_lstm.0.bias_ih_l0_reverse', 'c_lstm.0.weight_ih_l2', 'q_lstm.weight_hh_l0_reverse', 'c_lstm.2.bias_ih_l0_reverse', 'c_lstm.1.weight_hh_l1_reverse', 'c_lstm.0.weight_hh_l2', 'c_lstm.0.weight_hh_l2_reverse', 'attention.out_proj.bias', 'liner.bias', 'c_lstm.2.weight_hh_l1', 'c_lstm.1.weight_hh_l1', 'bert.weight', 'c_lstm.2.bias_hh_l0', 'c_lstm.0.bias_ih_l2_reverse', 'c_lstm.0.bias_hh_l1', 'c_lstm.2.weight_hh_l2_reverse', 'c_lstm.1.bias_ih_l0_reverse', 'attention.v_proj.bias', 'c_lstm.0.bias_hh_l1_reverse', 'q_lstm.bias_hh_l0', 'q_lstm.weight_hh_l1', 'c_lstm.1.weight_ih_l0', 'c_lstm.0.weight_hh_l0', 'c_lstm.1.bias_hh_l2', 'c_lstm.1.weight_ih_l2_reverse', 'c_lstm.2.bias_hh_l1', 'c_lstm.0.bias_hh_l2', 'q_lstm.weight_ih_l0_reverse', 'q_lstm.bias_hh_l1', 'q_lstm.weight_ih_l0', 'c_lstm.0.bias_ih_l2', 'c_lstm.2.bias_hh_l1_reverse', 'c_lstm.1.bias_hh_l0', 'q_lstm.bias_ih_l2_reverse', 'c_lstm.0.weight_ih_l1_reverse', 'q_lstm.bias_hh_l2_reverse', 'c_lstm.2.weight_hh_l0', 'c_lstm.2.weight_ih_l1', 'c_lstm.0.weight_ih_l1', 'q_lstm.bias_ih_l0', 'q_lstm.weight_ih_l2_reverse', 'c_lstm.0.bias_ih_l0', 'q_lstm.weight_hh_l1_reverse', 'c_lstm.2.bias_hh_l2_reverse', 'c_lstm.1.weight_hh_l0', 'c_lstm.1.bias_hh_l0_reverse', 'c_lstm.1.bias_ih_l0', 'c_lstm.2.bias_hh_l0_reverse', 'c_lstm.2.bias_hh_l2', 'c_lstm.1.weight_hh_l2_reverse', 'q_lstm.weight_hh_l2_reverse', 'c_lstm.1.bias_hh_l2_reverse', 'q_lstm.bias_ih_l0_reverse', 'attention.v_proj.weight', 'q_lstm.weight_ih_l1', 'c_lstm.2.weight_hh_l2', 'c_lstm.0.bias_ih_l1_reverse', 'c_lstm.0.weight_ih_l0_reverse', 'c_lstm.1.weight_ih_l1', 'c_lstm.2.weight_ih_l1_reverse', 'q_lstm.bias_ih_l1', 'c_lstm.0.weight_hh_l0_reverse', 'c_lstm.1.bias_hh_l1', 'c_lstm.2.weight_hh_l1_reverse', 'q_lstm.weight_ih_l2', 'c_lstm.0.weight_ih_l2_reverse', 'c_lstm.1.bias_ih_l1_reverse', 'c_lstm.1.weight_ih_l2', 'c_lstm.1.weight_ih_l0_reverse', 'c_lstm.2.weight_ih_l0_reverse', 'c_lstm.1.weight_hh_l2', 'c_lstm.0.weight_hh_l1_reverse', 'c_lstm.2.bias_ih_l0', 'attention.k_proj.weight', 'c_lstm.1.bias_ih_l1', 'c_lstm.1.weight_ih_l1_reverse', 'c_lstm.2.weight_ih_l0', 'q_lstm.bias_hh_l2', 'c_lstm.2.bias_ih_l2', 'q_lstm.bias_ih_l1_reverse', 'c_lstm.0.bias_hh_l0', 'c_lstm.1.weight_hh_l0_reverse', 'attention.q_proj.weight', 'c_lstm.0.weight_ih_l0', 'attention.out_proj.weight', 'q_lstm.bias_hh_l0_reverse', 'c_lstm.1.bias_ih_l2', 'c_lstm.0.bias_ih_l1', 'c_lstm.0.bias_hh_l2_reverse', 'c_lstm.2.weight_ih_l2_reverse', 'attention.q_proj.bias', 'q_lstm.weight_hh_l2', 'q_lstm.weight_ih_l1_reverse', 'q_lstm.bias_ih_l2', 'q_lstm.weight_hh_l0', 'c_lstm.2.bias_ih_l1', 'c_lstm.1.bias_hh_l1_reverse', 'q_lstm.bias_hh_l1_reverse', 'c_lstm.0.bias_hh_l0_reverse', 'liner.weight', 'c_lstm.2.weight_ih_l2', 'c_lstm.2.bias_ih_l2_reverse', 'c_lstm.2.weight_hh_l0_reverse']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
创建文件夹...
[22-05-05 12:41:03] --Trainer-- INFO: Load pretrained model from file /data2/wangbingchao/database/bert_pretrained/bert-base-uncased ...
[22-05-05 12:41:03] --Trainer-- INFO: Load pretrained model config finished!!!
[22-05-05 12:41:03] --Trainer-- INFO: Define model...
[22-05-05 12:41:05] --Trainer-- INFO: use cuda to train
[22-05-05 12:41:25] --Trainer-- INFO: Use Multi-GPUs[0, 1, 2]
[22-05-05 12:41:25] --DropDataloader-- INFO: Loading dataset from cached file /data2/maqi/LongTextDatasets/LongTextModels/cache/cache_train_hotpot_dev_fullwiki_v1.json_bert-base-uncased
[22-05-05 12:41:26] --Trainer-- INFO: Define model finished!!!
[22-05-05 12:41:26] --Trainer-- INFO: model config output dir: /data2/maqi/LongTextDatasets/LongTextModels/output/exp_5-5/config.txt
 0%|                                                                                                     |0/309[00:00<?]Epoch: 1/4:  0%|                                                                                         |0/309[00:00<?]/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/modules/rnn.py:662: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1616554827596/work/aten/src/ATen/native/cudnn/RNN.cpp:915.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Epoch: 1/4:  0%|                                                                             |0/309[00:31<?, loss=0.693]Epoch: 1/4:  0%|                                   |0/309[00:31<?, Iter=0, Loss=0.693147, UsedTime=0:00:31, lr=0.000000]Epoch: 1/4:  0%|                             |1/309[00:31<2:39:17, Iter=0, Loss=0.693147, UsedTime=0:00:31, lr=0.000000]Epoch: 1/4:  1%|▍                                                                      |2/309[00:37<2:38:46, loss=0.693]Epoch: 1/4:  1%|▏                            |2/309[00:37<2:38:46, Iter=1, Loss=0.693147, UsedTime=0:00:37, lr=0.000001]Epoch: 1/4:  1%|▎                              |3/309[00:37<53:49, Iter=1, Loss=0.693147, UsedTime=0:00:37, lr=0.000001]Epoch: 1/4:  1%|▉                                                                        |4/309[00:44<53:38, loss=0.693]Epoch: 1/4:  1%|▍                              |4/309[00:44<53:38, Iter=2, Loss=0.693147, UsedTime=0:00:44, lr=0.000002]Epoch: 1/4:  2%|▌                              |5/309[00:44<34:39, Iter=2, Loss=0.693147, UsedTime=0:00:44, lr=0.000002]Epoch: 1/4:  2%|█▏                                                                       |5/309[00:51<34:39, loss=0.693]Epoch: 1/4:  2%|▌                              |5/309[00:51<34:39, Iter=3, Loss=0.693147, UsedTime=0:00:51, lr=0.000003]Epoch: 1/4:  2%|▌                              |6/309[00:51<34:23, Iter=3, Loss=0.693147, UsedTime=0:00:51, lr=0.000003]Epoch: 1/4:  2%|█▋                                                                       |7/309[00:57<34:16, loss=0.693]Epoch: 1/4:  2%|▋                              |7/309[00:57<34:16, Iter=4, Loss=0.693147, UsedTime=0:00:57, lr=0.000004]Epoch: 1/4:  3%|▊                              |8/309[00:57<26:38, Iter=4, Loss=0.693147, UsedTime=0:00:57, lr=0.000004]Epoch: 1/4:  3%|█▉                                                                       |8/309[01:04<26:38, loss=0.693]Epoch: 1/4:  3%|▊                              |8/309[01:04<26:38, Iter=5, Loss=0.693147, UsedTime=0:01:04, lr=0.000005]Epoch: 1/4:  3%|▉                              |9/309[01:04<28:12, Iter=5, Loss=0.693147, UsedTime=0:01:04, lr=0.000005]Epoch: 1/4:  3%|██▏                                                                      |9/309[01:11<28:12, loss=0.693]Epoch: 1/4:  3%|▉                              |9/309[01:11<28:12, Iter=6, Loss=0.693147, UsedTime=0:01:11, lr=0.000006]Epoch: 1/4:  3%|▉                             |10/309[01:11<29:29, Iter=6, Loss=0.693147, UsedTime=0:01:11, lr=0.000006]Epoch: 1/4:  4%|██▌                                                                     |11/309[01:18<29:23, loss=0.693]Epoch: 1/4:  4%|█                             |11/309[01:18<29:23, Iter=7, Loss=0.693147, UsedTime=0:01:18, lr=0.000007]Epoch: 1/4:  4%|█▏                            |12/309[01:18<24:02, Iter=7, Loss=0.693147, UsedTime=0:01:18, lr=0.000007]Epoch: 1/4:  4%|██▊                                                                     |12/309[01:24<24:02, loss=0.693]Epoch: 1/4:  4%|█▏                            |12/309[01:24<24:02, Iter=8, Loss=0.693147, UsedTime=0:01:24, lr=0.000008]Epoch: 1/4:  4%|█▎                            |13/309[01:24<26:03, Iter=8, Loss=0.693147, UsedTime=0:01:24, lr=0.000008]Epoch: 1/4:  4%|███                                                                     |13/309[01:31<26:03, loss=0.693]Epoch: 1/4:  4%|█▎                            |13/309[01:31<26:03, Iter=9, Loss=0.693147, UsedTime=0:01:31, lr=0.000009]Epoch: 1/4:  5%|█▎                            |14/309[01:31<27:41, Iter=9, Loss=0.693147, UsedTime=0:01:31, lr=0.000009]Epoch: 1/4:  5%|███▎                                                                    |14/309[01:38<27:41, loss=0.693]Epoch: 1/4:  5%|█▎                           |14/309[01:38<27:41, Iter=10, Loss=0.693147, UsedTime=0:01:38, lr=0.000010]Epoch: 1/4:  5%|█▍                           |15/309[01:38<29:05, Iter=10, Loss=0.693147, UsedTime=0:01:38, lr=0.000010]Epoch: 1/4:  5%|███▋                                                                    |16/309[01:45<28:59, loss=0.693]Epoch: 1/4:  5%|█▌                           |16/309[01:45<28:59, Iter=11, Loss=0.693147, UsedTime=0:01:45, lr=0.000011]Epoch: 1/4:  6%|█▌                           |17/309[01:45<23:34, Iter=11, Loss=0.693147, UsedTime=0:01:45, lr=0.000011]Epoch: 1/4:  6%|███▉                                                                    |17/309[01:51<23:34, loss=0.693]Epoch: 1/4:  6%|█▌                           |17/309[01:51<23:34, Iter=12, Loss=0.693147, UsedTime=0:01:51, lr=0.000012]Epoch: 1/4:  6%|█▋                           |18/309[01:51<25:43, Iter=12, Loss=0.693147, UsedTime=0:01:51, lr=0.000012]Epoch: 1/4:  6%|████▏                                                                   |18/309[01:58<25:43, loss=0.693]Epoch: 1/4:  6%|█▋                           |18/309[01:58<25:43, Iter=13, Loss=0.693147, UsedTime=0:01:58, lr=0.000013]Epoch: 1/4:  6%|█▊                           |19/309[01:58<27:27, Iter=13, Loss=0.693147, UsedTime=0:01:58, lr=0.000013]Epoch: 1/4:  6%|████▍                                                                   |19/309[02:05<27:27, loss=0.693]Epoch: 1/4:  6%|█▊                           |19/309[02:05<27:27, Iter=14, Loss=0.693147, UsedTime=0:02:05, lr=0.000014]Epoch: 1/4:  6%|█▉                           |20/309[02:05<28:45, Iter=14, Loss=0.693147, UsedTime=0:02:05, lr=0.000014]Epoch: 1/4:  6%|████▋                                                                   |20/309[02:12<28:45, loss=0.693]Epoch: 1/4:  6%|█▉                           |20/309[02:12<28:45, Iter=15, Loss=0.693147, UsedTime=0:02:12, lr=0.000015]Epoch: 1/4:  7%|█▉                           |21/309[02:12<29:39, Iter=15, Loss=0.693147, UsedTime=0:02:12, lr=0.000015]Epoch: 1/4:  7%|█████▏                                                                  |22/309[02:19<29:33, loss=0.693]Epoch: 1/4:  7%|██                           |22/309[02:19<29:33, Iter=16, Loss=0.693147, UsedTime=0:02:19, lr=0.000016]Epoch: 1/4:  7%|██▏                          |23/309[02:19<23:40, Iter=16, Loss=0.693147, UsedTime=0:02:19, lr=0.000016]Epoch: 1/4:  7%|█████▎                                                                  |23/309[02:25<23:40, loss=0.693]Epoch: 1/4:  7%|██▏                          |23/309[02:25<23:40, Iter=17, Loss=0.693147, UsedTime=0:02:25, lr=0.000017]Epoch: 1/4:  8%|██▎                          |24/309[02:25<25:39, Iter=17, Loss=0.693147, UsedTime=0:02:25, lr=0.000017]Epoch: 1/4:  8%|█████▌                                                                  |24/309[02:32<25:39, loss=0.693]Epoch: 1/4:  8%|██▎                          |24/309[02:32<25:39, Iter=18, Loss=0.693147, UsedTime=0:02:32, lr=0.000018]Epoch: 1/4:  8%|██▎                          |25/309[02:32<27:12, Iter=18, Loss=0.693147, UsedTime=0:02:32, lr=0.000018]Epoch: 1/4:  8%|█████▊                                                                  |25/309[02:39<27:12, loss=0.693]Epoch: 1/4:  8%|██▎                          |25/309[02:39<27:12, Iter=19, Loss=0.693147, UsedTime=0:02:39, lr=0.000019]Epoch: 1/4:  8%|██▍                          |26/309[02:39<28:23, Iter=19, Loss=0.693147, UsedTime=0:02:39, lr=0.000019]Epoch: 1/4:  8%|██████                                                                  |26/309[02:46<28:23, loss=0.693]Epoch: 1/4:  8%|██▍                          |26/309[02:46<28:23, Iter=20, Loss=0.693147, UsedTime=0:02:46, lr=0.000020]Epoch: 1/4:  9%|██▌                          |27/309[02:46<29:14, Iter=20, Loss=0.693147, UsedTime=0:02:46, lr=0.000020]Epoch: 1/4:  9%|██████▎                                                                 |27/309[02:52<29:14, loss=0.693]Epoch: 1/4:  9%|██▌                          |27/309[02:52<29:14, Iter=21, Loss=0.693147, UsedTime=0:02:52, lr=0.000021]Epoch: 1/4:  9%|██▋                          |28/309[02:52<29:50, Iter=21, Loss=0.693147, UsedTime=0:02:52, lr=0.000021]Epoch: 1/4:  9%|██████▊                                                                 |29/309[02:59<29:44, loss=0.693]Epoch: 1/4:  9%|██▋                          |29/309[02:59<29:44, Iter=22, Loss=0.693147, UsedTime=0:02:59, lr=0.000022]Epoch: 1/4: 10%|██▊                          |30/309[02:59<23:24, Iter=22, Loss=0.693147, UsedTime=0:02:59, lr=0.000022]Epoch: 1/4: 10%|██████▉                                                                 |30/309[03:06<23:24, loss=0.693]Epoch: 1/4: 10%|██▊                          |30/309[03:06<23:24, Iter=23, Loss=0.693147, UsedTime=0:03:06, lr=0.000023]Epoch: 1/4: 10%|██▉                          |31/309[03:06<25:24, Iter=23, Loss=0.693147, UsedTime=0:03:06, lr=0.000023]Epoch: 1/4: 10%|███████▏                                                                |31/309[03:13<25:24, loss=0.693]Epoch: 1/4: 10%|██▉                          |31/309[03:13<25:24, Iter=24, Loss=0.693147, UsedTime=0:03:13, lr=0.000024]Epoch: 1/4: 10%|███                          |32/309[03:13<27:04, Iter=24, Loss=0.693147, UsedTime=0:03:13, lr=0.000024]Epoch: 1/4: 10%|███████▍                                                                |32/309[03:20<27:04, loss=0.693]Epoch: 1/4: 10%|███                          |32/309[03:20<27:04, Iter=25, Loss=0.693147, UsedTime=0:03:20, lr=0.000025]Epoch: 1/4: 11%|███                          |33/309[03:20<28:06, Iter=25, Loss=0.693147, UsedTime=0:03:20, lr=0.000025]Epoch: 1/4: 11%|███████▋                                                                |33/309[03:27<28:06, loss=0.693]Epoch: 1/4: 11%|███                          |33/309[03:27<28:06, Iter=26, Loss=0.693147, UsedTime=0:03:27, lr=0.000026]Epoch: 1/4: 11%|███▏                         |34/309[03:27<28:52, Iter=26, Loss=0.693147, UsedTime=0:03:27, lr=0.000026]Epoch: 1/4: 11%|███████▉                                                                |34/309[03:33<28:52, loss=0.693]Epoch: 1/4: 11%|███▏                         |34/309[03:33<28:52, Iter=27, Loss=0.693147, UsedTime=0:03:33, lr=0.000027]Epoch: 1/4: 11%|███▎                         |35/309[03:33<29:22, Iter=27, Loss=0.693147, UsedTime=0:03:33, lr=0.000027]Epoch: 1/4: 11%|████████▏                                                               |35/309[03:40<29:22, loss=0.693]Epoch: 1/4: 11%|███▎                         |35/309[03:40<29:22, Iter=28, Loss=0.693147, UsedTime=0:03:40, lr=0.000028]Epoch: 1/4: 12%|███▍                         |36/309[03:40<29:46, Iter=28, Loss=0.693147, UsedTime=0:03:40, lr=0.000028]Epoch: 1/4: 12%|████████▌                                                               |37/309[03:47<29:39, loss=0.693]Epoch: 1/4: 12%|███▍                         |37/309[03:47<29:39, Iter=29, Loss=0.693147, UsedTime=0:03:47, lr=0.000029]Epoch: 1/4: 12%|███▌                         |38/309[03:47<23:06, Iter=29, Loss=0.693147, UsedTime=0:03:47, lr=0.000029]Epoch: 1/4: 12%|████████▊                                                               |38/309[03:54<23:06, loss=0.693]Epoch: 1/4: 12%|███▌                         |38/309[03:54<23:06, Iter=30, Loss=0.693147, UsedTime=0:03:54, lr=0.000030]Epoch: 1/4: 13%|███▋                         |39/309[03:54<24:51, Iter=30, Loss=0.693147, UsedTime=0:03:54, lr=0.000030]Epoch: 1/4: 13%|█████████                                                               |39/309[04:00<24:51, loss=0.693]Epoch: 1/4: 13%|███▋                         |39/309[04:00<24:51, Iter=31, Loss=0.693147, UsedTime=0:04:00, lr=0.000031]Epoch: 1/4: 13%|███▊                         |40/309[04:00<26:11, Iter=31, Loss=0.693147, UsedTime=0:04:00, lr=0.000031]Epoch: 1/4: 13%|█████████▎                                                              |40/309[04:07<26:11, loss=0.693]Epoch: 1/4: 13%|███▊                         |40/309[04:07<26:11, Iter=32, Loss=0.693147, UsedTime=0:04:07, lr=0.000032]Epoch: 1/4: 13%|███▊                         |41/309[04:07<27:14, Iter=32, Loss=0.693147, UsedTime=0:04:07, lr=0.000032]Epoch: 1/4: 13%|█████████▌                                                              |41/309[04:14<27:14, loss=0.693]Epoch: 1/4: 13%|███▊                         |41/309[04:14<27:14, Iter=33, Loss=0.693147, UsedTime=0:04:14, lr=0.000033]Epoch: 1/4: 14%|███▉                         |42/309[04:14<27:58, Iter=33, Loss=0.693147, UsedTime=0:04:14, lr=0.000033]Epoch: 1/4: 14%|█████████▊                                                              |42/309[04:21<27:58, loss=0.693]Epoch: 1/4: 14%|███▉                         |42/309[04:21<27:58, Iter=34, Loss=0.693147, UsedTime=0:04:21, lr=0.000034]Epoch: 1/4: 14%|████                         |43/309[04:21<28:33, Iter=34, Loss=0.693147, UsedTime=0:04:21, lr=0.000034]Epoch: 1/4: 14%|██████████                                                              |43/309[04:28<28:33, loss=0.693]Epoch: 1/4: 14%|████                         |43/309[04:28<28:33, Iter=35, Loss=0.693147, UsedTime=0:04:28, lr=0.000035]Epoch: 1/4: 14%|████▏                        |44/309[04:28<28:55, Iter=35, Loss=0.693147, UsedTime=0:04:28, lr=0.000035]Epoch: 1/4: 14%|██████████▎                                                             |44/309[04:34<28:55, loss=0.693]Epoch: 1/4: 14%|████▏                        |44/309[04:34<28:55, Iter=36, Loss=0.693147, UsedTime=0:04:34, lr=0.000036]Epoch: 1/4: 15%|████▏                        |45/309[04:34<29:07, Iter=36, Loss=0.693147, UsedTime=0:04:34, lr=0.000036]Epoch: 1/4: 15%|██████████▋                                                             |46/309[04:41<29:00, loss=0.693]Epoch: 1/4: 15%|████▎                        |46/309[04:41<29:00, Iter=37, Loss=0.693147, UsedTime=0:04:41, lr=0.000037]Epoch: 1/4: 15%|████▍                        |47/309[04:41<22:30, Iter=37, Loss=0.693147, UsedTime=0:04:41, lr=0.000037]Epoch: 1/4: 15%|██████████▉                                                             |47/309[04:48<22:30, loss=0.693]Epoch: 1/4: 15%|████▍                        |47/309[04:48<22:30, Iter=38, Loss=0.693147, UsedTime=0:04:48, lr=0.000038]Epoch: 1/4: 16%|████▌                        |48/309[04:48<24:10, Iter=38, Loss=0.693147, UsedTime=0:04:48, lr=0.000038]Epoch: 1/4: 16%|███████████▏                                                            |48/309[04:55<24:10, loss=0.693]Epoch: 1/4: 16%|████▌                        |48/309[04:55<24:10, Iter=39, Loss=0.693147, UsedTime=0:04:55, lr=0.000039]Epoch: 1/4: 16%|████▌                        |49/309[04:55<25:32, Iter=39, Loss=0.693147, UsedTime=0:04:55, lr=0.000039]Epoch: 1/4: 16%|███████████▍                                                            |49/309[05:02<25:32, loss=0.693]Epoch: 1/4: 16%|████▌                        |49/309[05:02<25:32, Iter=40, Loss=0.693147, UsedTime=0:05:02, lr=0.000040]Epoch: 1/4: 16%|████▋                        |50/309[05:02<26:29, Iter=40, Loss=0.693147, UsedTime=0:05:02, lr=0.000040]Epoch: 1/4: 16%|███████████▋                                                            |50/309[05:08<26:29, loss=0.693]Epoch: 1/4: 16%|████▋                        |50/309[05:08<26:29, Iter=41, Loss=0.693147, UsedTime=0:05:08, lr=0.000041]Epoch: 1/4: 17%|████▊                        |51/309[05:08<27:08, Iter=41, Loss=0.693147, UsedTime=0:05:08, lr=0.000041]Epoch: 1/4: 17%|███████████▉                                                            |51/309[05:15<27:08, loss=0.693]Epoch: 1/4: 17%|████▊                        |51/309[05:15<27:08, Iter=42, Loss=0.693147, UsedTime=0:05:15, lr=0.000042]Epoch: 1/4: 17%|████▉                        |52/309[05:15<27:40, Iter=42, Loss=0.693147, UsedTime=0:05:15, lr=0.000042]Epoch: 1/4: 17%|████████████                                                            |52/309[05:22<27:40, loss=0.693]Epoch: 1/4: 17%|████▉                        |52/309[05:22<27:40, Iter=43, Loss=0.693147, UsedTime=0:05:22, lr=0.000043]Epoch: 1/4: 17%|████▉                        |53/309[05:22<27:57, Iter=43, Loss=0.693147, UsedTime=0:05:22, lr=0.000043]Epoch: 1/4: 17%|████████████▎                                                           |53/309[05:29<27:57, loss=0.693]Epoch: 1/4: 17%|████▉                        |53/309[05:29<27:57, Iter=44, Loss=0.693147, UsedTime=0:05:29, lr=0.000044]Epoch: 1/4: 17%|█████                        |54/309[05:29<28:07, Iter=44, Loss=0.693147, UsedTime=0:05:29, lr=0.000044]Epoch: 1/4: 17%|████████████▌                                                           |54/309[05:36<28:07, loss=0.693]Epoch: 1/4: 17%|█████                        |54/309[05:36<28:07, Iter=45, Loss=0.693147, UsedTime=0:05:36, lr=0.000045]Epoch: 1/4: 18%|█████▏                       |55/309[05:36<28:13, Iter=45, Loss=0.693147, UsedTime=0:05:36, lr=0.000045]Epoch: 1/4: 18%|█████████████                                                           |56/309[05:42<28:06, loss=0.693]Epoch: 1/4: 18%|█████▎                       |56/309[05:42<28:06, Iter=46, Loss=0.693147, UsedTime=0:05:42, lr=0.000046]Epoch: 1/4: 18%|█████▎                       |57/309[05:42<21:44, Iter=46, Loss=0.693147, UsedTime=0:05:42, lr=0.000046]Epoch: 1/4: 18%|█████████████▎                                                          |57/309[05:49<21:44, loss=0.693]Epoch: 1/4: 18%|█████▎                       |57/309[05:49<21:44, Iter=47, Loss=0.693147, UsedTime=0:05:49, lr=0.000047]Epoch: 1/4: 19%|█████▍                       |58/309[05:49<23:18, Iter=47, Loss=0.693147, UsedTime=0:05:49, lr=0.000047]Epoch: 1/4: 19%|█████████████▌                                                          |58/309[05:56<23:18, loss=0.693]Epoch: 1/4: 19%|█████▍                       |58/309[05:56<23:18, Iter=48, Loss=0.693147, UsedTime=0:05:56, lr=0.000048]Epoch: 1/4: 19%|█████▌                       |59/309[05:56<24:32, Iter=48, Loss=0.693147, UsedTime=0:05:56, lr=0.000048]Epoch: 1/4: 19%|█████████████▋                                                          |59/309[06:03<24:32, loss=0.693]Epoch: 1/4: 19%|█████▌                       |59/309[06:03<24:32, Iter=49, Loss=0.693147, UsedTime=0:06:03, lr=0.000049]Epoch: 1/4: 19%|█████▋                       |60/309[06:03<25:26, Iter=49, Loss=0.693147, UsedTime=0:06:03, lr=0.000049]Epoch: 1/4: 19%|█████████████▉                                                          |60/309[06:10<25:26, loss=0.693]Epoch: 1/4: 19%|█████▋                       |60/309[06:10<25:26, Iter=50, Loss=0.693147, UsedTime=0:06:10, lr=0.000050]Epoch: 1/4: 20%|█████▋                       |61/309[06:10<26:06, Iter=50, Loss=0.693147, UsedTime=0:06:10, lr=0.000050]Epoch: 1/4: 20%|██████████████▏                                                         |61/309[06:16<26:06, loss=0.693]Epoch: 1/4: 20%|█████▋                       |61/309[06:16<26:06, Iter=51, Loss=0.693147, UsedTime=0:06:16, lr=0.000051]Epoch: 1/4: 20%|█████▊                       |62/309[06:16<26:34, Iter=51, Loss=0.693147, UsedTime=0:06:16, lr=0.000051]Epoch: 1/4: 20%|██████████████▍                                                         |62/309[06:23<26:34, loss=0.693]Epoch: 1/4: 20%|█████▊                       |62/309[06:23<26:34, Iter=52, Loss=0.693147, UsedTime=0:06:23, lr=0.000052]Epoch: 1/4: 20%|█████▉                       |63/309[06:23<26:56, Iter=52, Loss=0.693147, UsedTime=0:06:23, lr=0.000052]Epoch: 1/4: 20%|██████████████▋                                                         |63/309[06:30<26:56, loss=0.693]Epoch: 1/4: 20%|█████▉                       |63/309[06:30<26:56, Iter=53, Loss=0.693147, UsedTime=0:06:30, lr=0.000053]Epoch: 1/4: 21%|██████                       |64/309[06:30<27:04, Iter=53, Loss=0.693147, UsedTime=0:06:30, lr=0.000053]Epoch: 1/4: 21%|██████████████▉                                                         |64/309[06:37<27:04, loss=0.693]Epoch: 1/4: 21%|██████                       |64/309[06:37<27:04, Iter=54, Loss=0.693147, UsedTime=0:06:37, lr=0.000054]Epoch: 1/4: 21%|██████                       |65/309[06:37<27:10, Iter=54, Loss=0.693147, UsedTime=0:06:37, lr=0.000054]Epoch: 1/4: 21%|███████████████▏                                                        |65/309[06:44<27:10, loss=0.693]Epoch: 1/4: 21%|██████                       |65/309[06:44<27:10, Iter=55, Loss=0.693147, UsedTime=0:06:44, lr=0.000055]Epoch: 1/4: 21%|██████▏                      |66/309[06:44<27:12, Iter=55, Loss=0.693147, UsedTime=0:06:44, lr=0.000055]Epoch: 1/4: 22%|███████████████▌                                                        |67/309[06:50<27:05, loss=0.693]Epoch: 1/4: 22%|██████▎                      |67/309[06:50<27:05, Iter=56, Loss=0.693147, UsedTime=0:06:50, lr=0.000056]Epoch: 1/4: 22%|██████▍                      |68/309[06:50<20:50, Iter=56, Loss=0.693147, UsedTime=0:06:50, lr=0.000056]Epoch: 1/4: 22%|███████████████▊                                                        |68/309[06:57<20:50, loss=0.693]Epoch: 1/4: 22%|██████▍                      |68/309[06:57<20:50, Iter=57, Loss=0.693147, UsedTime=0:06:57, lr=0.000057]Epoch: 1/4: 22%|██████▍                      |69/309[06:57<22:22, Iter=57, Loss=0.693147, UsedTime=0:06:57, lr=0.000057]Epoch: 1/4: 22%|████████████████                                                        |69/309[07:04<22:22, loss=0.693]Epoch: 1/4: 22%|██████▍                      |69/309[07:04<22:22, Iter=58, Loss=0.693147, UsedTime=0:07:04, lr=0.000058]Epoch: 1/4: 23%|██████▌                      |70/309[07:04<23:33, Iter=58, Loss=0.693147, UsedTime=0:07:04, lr=0.000058]Epoch: 1/4: 23%|████████████████▎                                                       |70/309[07:11<23:33, loss=0.693]Epoch: 1/4: 23%|██████▌                      |70/309[07:11<23:33, Iter=59, Loss=0.693147, UsedTime=0:07:11, lr=0.000059]Epoch: 1/4: 23%|██████▋                      |71/309[07:11<24:24, Iter=59, Loss=0.693147, UsedTime=0:07:11, lr=0.000059]Epoch: 1/4: 23%|████████████████▌                                                       |71/309[07:18<24:24, loss=0.693]Epoch: 1/4: 23%|██████▋                      |71/309[07:18<24:24, Iter=60, Loss=0.693147, UsedTime=0:07:18, lr=0.000060]Epoch: 1/4: 23%|██████▊                      |72/309[07:18<24:57, Iter=60, Loss=0.693147, UsedTime=0:07:18, lr=0.000060]Epoch: 1/4: 23%|████████████████▊                                                       |72/309[07:24<24:57, loss=0.693]Epoch: 1/4: 23%|██████▊                      |72/309[07:24<24:57, Iter=61, Loss=0.693147, UsedTime=0:07:24, lr=0.000061]Epoch: 1/4: 24%|██████▊                      |73/309[07:24<25:23, Iter=61, Loss=0.693147, UsedTime=0:07:24, lr=0.000061]Epoch: 1/4: 24%|█████████████████                                                       |73/309[07:31<25:23, loss=0.693]Epoch: 1/4: 24%|██████▊                      |73/309[07:31<25:23, Iter=62, Loss=0.693147, UsedTime=0:07:31, lr=0.000062]Epoch: 1/4: 24%|██████▉                      |74/309[07:31<25:38, Iter=62, Loss=0.693147, UsedTime=0:07:31, lr=0.000062]Epoch: 1/4: 24%|█████████████████▏                                                      |74/309[07:38<25:38, loss=0.693]Epoch: 1/4: 24%|██████▉                      |74/309[07:38<25:38, Iter=63, Loss=0.693147, UsedTime=0:07:38, lr=0.000063]Epoch: 1/4: 24%|███████                      |75/309[07:38<25:47, Iter=63, Loss=0.693147, UsedTime=0:07:38, lr=0.000063]Epoch: 1/4: 24%|█████████████████▍                                                      |75/309[07:45<25:47, loss=0.693]Epoch: 1/4: 24%|███████                      |75/309[07:45<25:47, Iter=64, Loss=0.693147, UsedTime=0:07:45, lr=0.000064]Epoch: 1/4: 25%|███████▏                     |76/309[07:45<25:54, Iter=64, Loss=0.693147, UsedTime=0:07:45, lr=0.000064]Epoch: 1/4: 25%|█████████████████▋                                                      |76/309[07:52<25:54, loss=0.693]Epoch: 1/4: 25%|███████▏                     |76/309[07:52<25:54, Iter=65, Loss=0.693147, UsedTime=0:07:52, lr=0.000065]Epoch: 1/4: 25%|███████▏                     |77/309[07:52<25:54, Iter=65, Loss=0.693147, UsedTime=0:07:52, lr=0.000065]Epoch: 1/4: 25%|█████████████████▉                                                      |77/309[07:58<25:54, loss=0.693]Epoch: 1/4: 25%|███████▏                     |77/309[07:58<25:54, Iter=66, Loss=0.693147, UsedTime=0:07:58, lr=0.000066]Epoch: 1/4: 25%|███████▎                     |78/309[07:58<25:55, Iter=66, Loss=0.693147, UsedTime=0:07:58, lr=0.000066]Epoch: 1/4: 26%|██████████████████▍                                                     |79/309[08:05<25:48, loss=0.693]Epoch: 1/4: 26%|███████▍                     |79/309[08:05<25:48, Iter=67, Loss=0.693147, UsedTime=0:08:05, lr=0.000067]Epoch: 1/4: 26%|███████▌                     |80/309[08:05<19:49, Iter=67, Loss=0.693147, UsedTime=0:08:05, lr=0.000067]Epoch: 1/4: 26%|██████████████████▋                                                     |80/309[08:12<19:49, loss=0.693]Epoch: 1/4: 26%|███████▌                     |80/309[08:12<19:49, Iter=68, Loss=0.693147, UsedTime=0:08:12, lr=0.000068]Epoch: 1/4: 26%|███████▌                     |81/309[08:12<21:12, Iter=68, Loss=0.693147, UsedTime=0:08:12, lr=0.000068]Epoch: 1/4: 26%|██████████████████▊                                                     |81/309[08:19<21:12, loss=0.693]Epoch: 1/4: 26%|███████▌                     |81/309[08:19<21:12, Iter=69, Loss=0.693147, UsedTime=0:08:19, lr=0.000069]Epoch: 1/4: 27%|███████▋                     |82/309[08:19<22:16, Iter=69, Loss=0.693147, UsedTime=0:08:19, lr=0.000069]Epoch: 1/4: 27%|███████████████████                                                     |82/309[08:25<22:16, loss=0.693]Epoch: 1/4: 27%|███████▋                     |82/309[08:25<22:16, Iter=70, Loss=0.693147, UsedTime=0:08:25, lr=0.000070]Epoch: 1/4: 27%|███████▊                     |83/309[08:25<23:07, Iter=70, Loss=0.693147, UsedTime=0:08:25, lr=0.000070]Epoch: 1/4: 27%|███████████████████▎                                                    |83/309[08:32<23:07, loss=0.693]Epoch: 1/4: 27%|███████▊                     |83/309[08:32<23:07, Iter=71, Loss=0.693147, UsedTime=0:08:32, lr=0.000071]Epoch: 1/4: 27%|███████▉                     |84/309[08:32<23:45, Iter=71, Loss=0.693147, UsedTime=0:08:32, lr=0.000071]Epoch: 1/4: 27%|███████████████████▌                                                    |84/309[08:39<23:45, loss=0.693]Epoch: 1/4: 27%|███████▉                     |84/309[08:39<23:45, Iter=72, Loss=0.693147, UsedTime=0:08:39, lr=0.000072]Epoch: 1/4: 28%|███████▉                     |85/309[08:39<24:08, Iter=72, Loss=0.693147, UsedTime=0:08:39, lr=0.000072]Epoch: 1/4: 28%|███████████████████▊                                                    |85/309[08:46<24:08, loss=0.693]Epoch: 1/4: 28%|███████▉                     |85/309[08:46<24:08, Iter=73, Loss=0.693147, UsedTime=0:08:46, lr=0.000073]Epoch: 1/4: 28%|████████                     |86/309[08:46<24:23, Iter=73, Loss=0.693147, UsedTime=0:08:46, lr=0.000073]Epoch: 1/4: 28%|████████████████████                                                    |86/309[08:53<24:23, loss=0.693]Epoch: 1/4: 28%|████████                     |86/309[08:53<24:23, Iter=74, Loss=0.693147, UsedTime=0:08:53, lr=0.000074]Epoch: 1/4: 28%|████████▏                    |87/309[08:53<24:32, Iter=74, Loss=0.693147, UsedTime=0:08:53, lr=0.000074]Epoch: 1/4: 28%|████████████████████▎                                                   |87/309[09:00<24:32, loss=0.693]Epoch: 1/4: 28%|████████▏                    |87/309[09:00<24:32, Iter=75, Loss=0.693147, UsedTime=0:09:00, lr=0.000075]Epoch: 1/4: 28%|████████▎                    |88/309[09:00<24:41, Iter=75, Loss=0.693147, UsedTime=0:09:00, lr=0.000075]Epoch: 1/4: 28%|████████████████████▌                                                   |88/309[09:06<24:41, loss=0.693]Epoch: 1/4: 28%|████████▎                    |88/309[09:06<24:41, Iter=76, Loss=0.693147, UsedTime=0:09:06, lr=0.000076]Epoch: 1/4: 29%|████████▎                    |89/309[09:06<24:43, Iter=76, Loss=0.693147, UsedTime=0:09:06, lr=0.000076]Epoch: 1/4: 29%|████████████████████▋                                                   |89/309[09:13<24:43, loss=0.693]Epoch: 1/4: 29%|████████▎                    |89/309[09:13<24:43, Iter=77, Loss=0.693147, UsedTime=0:09:13, lr=0.000077]Epoch: 1/4: 29%|████████▍                    |90/309[09:13<24:38, Iter=77, Loss=0.693147, UsedTime=0:09:13, lr=0.000077]Epoch: 1/4: 29%|████████████████████▉                                                   |90/309[09:20<24:38, loss=0.693]Epoch: 1/4: 29%|████████▍                    |90/309[09:20<24:38, Iter=78, Loss=0.693147, UsedTime=0:09:20, lr=0.000078]Epoch: 1/4: 29%|████████▌                    |91/309[09:20<24:34, Iter=78, Loss=0.693147, UsedTime=0:09:20, lr=0.000078]Epoch: 1/4: 30%|█████████████████████▍                                                  |92/309[09:27<24:27, loss=0.693]Epoch: 1/4: 30%|████████▋                    |92/309[09:27<24:27, Iter=79, Loss=0.693147, UsedTime=0:09:27, lr=0.000079]Epoch: 1/4: 30%|████████▋                    |93/309[09:27<18:45, Iter=79, Loss=0.693147, UsedTime=0:09:27, lr=0.000079]Epoch: 1/4: 30%|█████████████████████▋                                                  |93/309[09:33<18:45, loss=0.693]Epoch: 1/4: 30%|████████▋                    |93/309[09:33<18:45, Iter=80, Loss=0.693147, UsedTime=0:09:33, lr=0.000080]Epoch: 1/4: 30%|████████▊                    |94/309[09:33<20:04, Iter=80, Loss=0.693147, UsedTime=0:09:33, lr=0.000080]Epoch: 1/4: 30%|█████████████████████▉                                                  |94/309[09:40<20:04, loss=0.693]Epoch: 1/4: 30%|████████▊                    |94/309[09:40<20:04, Iter=81, Loss=0.693147, UsedTime=0:09:40, lr=0.000081]Epoch: 1/4: 31%|████████▉                    |95/309[09:40<21:06, Iter=81, Loss=0.693147, UsedTime=0:09:40, lr=0.000081]Epoch: 1/4: 31%|██████████████████████▏                                                 |95/309[09:47<21:06, loss=0.693]Epoch: 1/4: 31%|████████▉                    |95/309[09:47<21:06, Iter=82, Loss=0.693147, UsedTime=0:09:47, lr=0.000082]Epoch: 1/4: 31%|█████████                    |96/309[09:47<21:52, Iter=82, Loss=0.693147, UsedTime=0:09:47, lr=0.000082]Epoch: 1/4: 31%|██████████████████████▎                                                 |96/309[09:54<21:52, loss=0.693]Epoch: 1/4: 31%|█████████                    |96/309[09:54<21:52, Iter=83, Loss=0.693147, UsedTime=0:09:54, lr=0.000083]Epoch: 1/4: 31%|█████████                    |97/309[09:54<22:24, Iter=83, Loss=0.693147, UsedTime=0:09:54, lr=0.000083]Epoch: 1/4: 31%|██████████████████████▌                                                 |97/309[10:01<22:24, loss=0.693]Epoch: 1/4: 31%|█████████                    |97/309[10:01<22:24, Iter=84, Loss=0.693147, UsedTime=0:10:01, lr=0.000084]Epoch: 1/4: 32%|█████████▏                   |98/309[10:01<22:46, Iter=84, Loss=0.693147, UsedTime=0:10:01, lr=0.000084]Epoch: 1/4: 32%|██████████████████████▊                                                 |98/309[10:08<22:46, loss=0.693]Epoch: 1/4: 32%|█████████▏                   |98/309[10:08<22:46, Iter=85, Loss=0.693147, UsedTime=0:10:08, lr=0.000085]Epoch: 1/4: 32%|█████████▎                   |99/309[10:08<22:58, Iter=85, Loss=0.693147, UsedTime=0:10:08, lr=0.000085]Epoch: 1/4: 32%|███████████████████████                                                 |99/309[10:14<22:58, loss=0.693]Epoch: 1/4: 32%|█████████▎                   |99/309[10:14<22:58, Iter=86, Loss=0.693147, UsedTime=0:10:14, lr=0.000086]Epoch: 1/4: 32%|█████████                   |100/309[10:14<23:08, Iter=86, Loss=0.693147, UsedTime=0:10:14, lr=0.000086]Epoch: 1/4: 32%|██████████████████████▉                                                |100/309[10:21<23:08, loss=0.693]Epoch: 1/4: 32%|█████████                   |100/309[10:21<23:08, Iter=87, Loss=0.693147, UsedTime=0:10:21, lr=0.000087]Epoch: 1/4: 33%|█████████▏                  |101/309[10:21<23:12, Iter=87, Loss=0.693147, UsedTime=0:10:21, lr=0.000087]Epoch: 1/4: 33%|███████████████████████▏                                               |101/309[10:28<23:12, loss=0.693]Epoch: 1/4: 33%|█████████▏                  |101/309[10:28<23:12, Iter=88, Loss=0.693147, UsedTime=0:10:28, lr=0.000088]Epoch: 1/4: 33%|█████████▏                  |102/309[10:28<23:11, Iter=88, Loss=0.693147, UsedTime=0:10:28, lr=0.000088]Epoch: 1/4: 33%|███████████████████████▍                                               |102/309[10:35<23:11, loss=0.693]Epoch: 1/4: 33%|█████████▏                  |102/309[10:35<23:11, Iter=89, Loss=0.693147, UsedTime=0:10:35, lr=0.000089]Epoch: 1/4: 33%|█████████▎                  |103/309[10:35<23:08, Iter=89, Loss=0.693147, UsedTime=0:10:35, lr=0.000089]Epoch: 1/4: 33%|███████████████████████▋                                               |103/309[10:42<23:08, loss=0.693]Epoch: 1/4: 33%|█████████▎                  |103/309[10:42<23:08, Iter=90, Loss=0.693147, UsedTime=0:10:42, lr=0.000090]Epoch: 1/4: 34%|█████████▍                  |104/309[10:42<23:04, Iter=90, Loss=0.693147, UsedTime=0:10:42, lr=0.000090]Epoch: 1/4: 34%|███████████████████████▉                                               |104/309[10:48<23:04, loss=0.693]Epoch: 1/4: 34%|█████████▍                  |104/309[10:48<23:04, Iter=91, Loss=0.693147, UsedTime=0:10:48, lr=0.000091]Epoch: 1/4: 34%|█████████▌                  |105/309[10:48<23:00, Iter=91, Loss=0.693147, UsedTime=0:10:48, lr=0.000091]Epoch: 1/4: 34%|████████████████████████▎                                              |106/309[10:55<22:53, loss=0.693]Epoch: 1/4: 34%|█████████▌                  |106/309[10:55<22:53, Iter=92, Loss=0.693147, UsedTime=0:10:55, lr=0.000092]Epoch: 1/4: 35%|█████████▋                  |107/309[10:55<17:33, Iter=92, Loss=0.693147, UsedTime=0:10:55, lr=0.000092]Epoch: 1/4: 35%|████████████████████████▌                                              |107/309[11:02<17:33, loss=0.693]Epoch: 1/4: 35%|█████████▋                  |107/309[11:02<17:33, Iter=93, Loss=0.693147, UsedTime=0:11:02, lr=0.000093]Epoch: 1/4: 35%|█████████▊                  |108/309[11:02<18:46, Iter=93, Loss=0.693147, UsedTime=0:11:02, lr=0.000093]Epoch: 1/4: 35%|████████████████████████▊                                              |108/309[11:09<18:46, loss=0.693]Epoch: 1/4: 35%|█████████▊                  |108/309[11:09<18:46, Iter=94, Loss=0.693147, UsedTime=0:11:09, lr=0.000094]Epoch: 1/4: 35%|█████████▉                  |109/309[11:09<19:41, Iter=94, Loss=0.693147, UsedTime=0:11:09, lr=0.000094]nohup: ignoring input
Some weights of the model checkpoint at /data2/wangbingchao/database/bert_pretrained/bert-base-uncased were not used when initializing SentenceChoice: ['bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'cls.predictions.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias']
- This IS expected if you are initializing SentenceChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SentenceChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of SentenceChoice were not initialized from the model checkpoint at /data2/wangbingchao/database/bert_pretrained/bert-base-uncased and are newly initialized: ['c_lstm.1.weight_hh_l1_reverse', 'c_lstm.0.bias_hh_l2_reverse', 'c_lstm.2.weight_hh_l0_reverse', 'q_lstm.bias_hh_l2_reverse', 'attention.out_proj.weight', 'c_lstm.0.weight_ih_l1_reverse', 'c_lstm.0.bias_hh_l1', 'c_lstm.2.bias_ih_l1_reverse', 'q_lstm.weight_hh_l1_reverse', 'c_lstm.1.weight_ih_l1', 'q_lstm.weight_hh_l2_reverse', 'c_lstm.2.weight_ih_l1_reverse', 'q_lstm.weight_hh_l1', 'c_lstm.1.bias_hh_l1', 'c_lstm.2.bias_hh_l0_reverse', 'c_lstm.2.bias_hh_l2', 'c_lstm.1.weight_hh_l0_reverse', 'c_lstm.2.weight_hh_l1', 'q_lstm.weight_ih_l0', 'c_lstm.0.weight_ih_l2', 'q_lstm.weight_hh_l0_reverse', 'c_lstm.1.weight_hh_l0', 'q_lstm.weight_ih_l2', 'c_lstm.2.bias_ih_l1', 'c_lstm.2.bias_hh_l1_reverse', 'c_lstm.1.weight_ih_l1_reverse', 'attention.out_proj.bias', 'c_lstm.2.bias_ih_l0_reverse', 'c_lstm.1.bias_ih_l0', 'c_lstm.0.weight_hh_l0_reverse', 'c_lstm.0.weight_ih_l0', 'c_lstm.1.weight_hh_l2_reverse', 'c_lstm.2.weight_ih_l2', 'bert.weight', 'c_lstm.0.weight_ih_l2_reverse', 'attention.q_proj.weight', 'c_lstm.0.bias_ih_l2_reverse', 'c_lstm.1.weight_hh_l1', 'q_lstm.bias_ih_l0', 'c_lstm.1.bias_ih_l0_reverse', 'c_lstm.0.weight_ih_l0_reverse', 'q_lstm.weight_ih_l2_reverse', 'c_lstm.1.bias_hh_l0', 'c_lstm.1.weight_ih_l2_reverse', 'q_lstm.weight_ih_l0_reverse', 'c_lstm.0.bias_hh_l0', 'c_lstm.0.weight_hh_l1', 'c_lstm.0.bias_hh_l0_reverse', 'c_lstm.1.weight_ih_l2', 'c_lstm.1.bias_ih_l1_reverse', 'q_lstm.bias_ih_l2_reverse', 'c_lstm.2.bias_ih_l0', 'c_lstm.2.weight_hh_l0', 'attention.k_proj.bias', 'liner.weight', 'c_lstm.1.bias_ih_l2_reverse', 'c_lstm.1.weight_hh_l2', 'c_lstm.0.bias_ih_l2', 'q_lstm.weight_hh_l2', 'q_lstm.bias_hh_l1', 'c_lstm.2.bias_hh_l0', 'c_lstm.0.bias_ih_l0', 'c_lstm.2.weight_ih_l2_reverse', 'q_lstm.bias_ih_l1', 'q_lstm.weight_hh_l0', 'q_lstm.bias_ih_l0_reverse', 'c_lstm.0.bias_ih_l0_reverse', 'liner.bias', 'q_lstm.bias_hh_l2', 'c_lstm.2.weight_ih_l0_reverse', 'c_lstm.2.bias_ih_l2_reverse', 'c_lstm.2.weight_ih_l1', 'c_lstm.2.bias_hh_l1', 'c_lstm.1.bias_ih_l1', 'q_lstm.bias_hh_l1_reverse', 'q_lstm.bias_ih_l2', 'q_lstm.weight_ih_l1', 'c_lstm.0.bias_ih_l1', 'c_lstm.0.weight_hh_l2_reverse', 'c_lstm.1.weight_ih_l0', 'c_lstm.1.bias_ih_l2', 'c_lstm.2.bias_ih_l2', 'c_lstm.0.bias_ih_l1_reverse', 'c_lstm.0.bias_hh_l1_reverse', 'c_lstm.1.bias_hh_l2', 'q_lstm.bias_ih_l1_reverse', 'attention.v_proj.bias', 'q_lstm.weight_ih_l1_reverse', 'c_lstm.1.bias_hh_l2_reverse', 'c_lstm.0.weight_hh_l0', 'c_lstm.1.bias_hh_l0_reverse', 'c_lstm.1.weight_ih_l0_reverse', 'attention.q_proj.bias', 'c_lstm.2.weight_hh_l2_reverse', 'c_lstm.2.weight_ih_l0', 'c_lstm.2.weight_hh_l2', 'attention.k_proj.weight', 'q_lstm.bias_hh_l0', 'c_lstm.0.weight_ih_l1', 'c_lstm.2.bias_hh_l2_reverse', 'c_lstm.0.weight_hh_l1_reverse', 'attention.v_proj.weight', 'c_lstm.1.bias_hh_l1_reverse', 'q_lstm.bias_hh_l0_reverse', 'c_lstm.2.weight_hh_l1_reverse', 'c_lstm.0.weight_hh_l2', 'c_lstm.0.bias_hh_l2']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
创建文件夹...
[22-05-05 12:53:22] --Trainer-- INFO: Load pretrained model from file /data2/wangbingchao/database/bert_pretrained/bert-base-uncased ...
[22-05-05 12:53:22] --Trainer-- INFO: Load pretrained model config finished!!!
[22-05-05 12:53:22] --Trainer-- INFO: Define model...
[22-05-05 12:53:24] --Trainer-- INFO: use cuda to train
[22-05-05 12:53:32] --Trainer-- INFO: Use Multi-GPUs[0, 1, 2]
[22-05-05 12:53:32] --DropDataloader-- INFO: Loading dataset from cached file /data2/maqi/LongTextDatasets/LongTextModels/cache/cache_train_hotpot_dev_fullwiki_v1.json_bert-base-uncased
[22-05-05 12:53:33] --Trainer-- INFO: Define model finished!!!
[22-05-05 12:53:33] --Trainer-- INFO: model config output dir: /data2/maqi/LongTextDatasets/LongTextModels/output/exp_5-5/config.txt
[22-05-05 12:53:33] --Trainer-- INFO: ***** Running evaluation *****
[22-05-05 12:53:33] --Trainer-- INFO: ***** Prepare for Test *****
[22-05-05 12:53:33] --Trainer-- INFO: Load dataset from file hotpot_dev_fullwiki_v1.json ...
[22-05-05 12:53:33] --DropDataloader-- INFO: Creating dataset ...
Tokenizer:   0%|          | 0/7405 [00:00<?, ?it/s]Tokenizer:   0%|          | 1/7405 [00:00<24:45,  4.98it/s]Tokenizer:   0%|          | 2/7405 [00:00<22:40,  5.44it/s]Tokenizer:   0%|          | 3/7405 [00:00<23:55,  5.16it/s]Tokenizer:   0%|          | 4/7405 [00:00<20:08,  6.12it/s]Tokenizer:   0%|          | 5/7405 [00:00<22:09,  5.57it/s]Tokenizer:   0%|          | 6/7405 [00:01<23:19,  5.29it/s]Tokenizer:   0%|          | 7/7405 [00:01<26:03,  4.73it/s]Tokenizer:   0%|          | 8/7405 [00:01<32:34,  3.78it/s]Tokenizer:   0%|          | 9/7405 [00:01<29:40,  4.15it/s]Tokenizer:   0%|          | 10/7405 [00:02<29:25,  4.19it/s]Tokenizer:   0%|          | 11/7405 [00:02<29:11,  4.22it/s]Tokenizer:   0%|          | 12/7405 [00:02<28:39,  4.30it/s]Tokenizer:   0%|          | 13/7405 [00:02<30:39,  4.02it/s]Tokenizer:   0%|          | 14/7405 [00:03<34:22,  3.58it/s]Tokenizer:   0%|          | 15/7405 [00:03<27:55,  4.41it/s]Tokenizer:   0%|          | 16/7405 [00:03<28:50,  4.27it/s]Tokenizer:   0%|          | 17/7405 [00:03<26:43,  4.61it/s]Tokenizer:   0%|          | 18/7405 [00:03<24:57,  4.93it/s]Tokenizer:   0%|          | 19/7405 [00:04<25:44,  4.78it/s]Tokenizer:   0%|          | 20/7405 [00:04<23:33,  5.22it/s]Tokenizer:   0%|          | 21/7405 [00:04<23:19,  5.28it/s]Tokenizer:   0%|          | 22/7405 [00:04<24:17,  5.07it/s]Tokenizer:   0%|          | 23/7405 [00:04<25:14,  4.88it/s]Tokenizer:   0%|          | 24/7405 [00:05<23:12,  5.30it/s]Tokenizer:   0%|          | 25/7405 [00:05<21:29,  5.72it/s]Tokenizer:   0%|          | 26/7405 [00:05<20:41,  5.94it/s]Tokenizer:   0%|          | 27/7405 [00:05<23:20,  5.27it/s]Tokenizer:   0%|          | 29/7405 [00:05<18:02,  6.81it/s]Tokenizer:   0%|          | 30/7405 [00:06<18:37,  6.60it/s]Tokenizer:   0%|          | 31/7405 [00:06<19:21,  6.35it/s]Tokenizer:   0%|          | 32/7405 [00:06<19:22,  6.34it/s]Tokenizer:   0%|          | 33/7405 [00:06<18:21,  6.69it/s]Tokenizer:   0%|          | 34/7405 [00:06<20:24,  6.02it/s]Tokenizer:   0%|          | 35/7405 [00:06<20:33,  5.98it/s]Tokenizer:   0%|          | 36/7405 [00:07<22:23,  5.49it/s]Tokenizer:   0%|          | 37/7405 [00:07<20:45,  5.91it/s]Tokenizer:   1%|          | 38/7405 [00:07<18:52,  6.51it/s]Tokenizer:   1%|          | 39/7405 [00:07<19:53,  6.17it/s]Tokenizer:   1%|          | 40/7405 [00:07<20:18,  6.04it/s]Tokenizer:   1%|          | 41/7405 [00:07<22:06,  5.55it/s]Tokenizer:   1%|          | 42/7405 [00:08<22:03,  5.56it/s]Tokenizer:   1%|          | 43/7405 [00:08<26:46,  4.58it/s]Tokenizer:   1%|          | 44/7405 [00:08<26:49,  4.57it/s]Tokenizer:   1%|          | 45/7405 [00:08<31:21,  3.91it/s]Tokenizer:   1%|          | 46/7405 [00:09<27:06,  4.52it/s]Tokenizer:   1%|          | 47/7405 [00:09<29:00,  4.23it/s]Tokenizer:   1%|          | 48/7405 [00:09<28:15,  4.34it/s]Tokenizer:   1%|          | 49/7405 [00:09<27:52,  4.40it/s]Tokenizer:   1%|          | 50/7405 [00:10<29:18,  4.18it/s]Tokenizer:   1%|          | 51/7405 [00:10<26:46,  4.58it/s]Tokenizer:   1%|          | 52/7405 [00:10<25:37,  4.78it/s]Tokenizer:   1%|          | 53/7405 [00:10<24:18,  5.04it/s]Tokenizer:   1%|          | 54/7405 [00:10<22:40,  5.40it/s]Tokenizer:   1%|          | 55/7405 [00:10<22:10,  5.52it/s]Tokenizer:   1%|          | 56/7405 [00:11<22:52,  5.35it/s]Tokenizer:   1%|          | 57/7405 [00:11<22:32,  5.43it/s]Tokenizer:   1%|          | 58/7405 [00:11<23:09,  5.29it/s]Tokenizer:   1%|          | 59/7405 [00:11<22:52,  5.35it/s]Tokenizer:   1%|          | 60/7405 [00:11<20:50,  5.87it/s]Tokenizer:   1%|          | 61/7405 [00:12<23:45,  5.15it/s]Tokenizer:   1%|          | 62/7405 [00:12<22:22,  5.47it/s]Tokenizer:   1%|          | 63/7405 [00:12<21:24,  5.72it/s]Tokenizer:   1%|          | 64/7405 [00:12<19:28,  6.28it/s]Tokenizer:   1%|          | 65/7405 [00:12<19:13,  6.36it/s]Tokenizer:   1%|          | 66/7405 [00:12<19:33,  6.25it/s]Tokenizer:   1%|          | 67/7405 [00:12<19:13,  6.36it/s]Tokenizer:   1%|          | 68/7405 [00:13<23:15,  5.26it/s]Tokenizer:   1%|          | 69/7405 [00:13<23:58,  5.10it/s]Tokenizer:   1%|          | 70/7405 [00:13<21:41,  5.64it/s]Tokenizer:   1%|          | 71/7405 [00:13<22:07,  5.52it/s]Tokenizer:   1%|          | 72/7405 [00:13<19:57,  6.12it/s]Tokenizer:   1%|          | 73/7405 [00:14<20:32,  5.95it/s]Tokenizer:   1%|          | 74/7405 [00:14<21:17,  5.74it/s]Tokenizer:   1%|          | 75/7405 [00:14<19:04,  6.41it/s]Tokenizer:   1%|          | 77/7405 [00:14<16:46,  7.28it/s]Tokenizer:   1%|          | 78/7405 [00:14<18:33,  6.58it/s]Tokenizer:   1%|          | 79/7405 [00:14<19:02,  6.41it/s]Tokenizer:   1%|          | 80/7405 [00:15<18:06,  6.74it/s]Tokenizer:   1%|          | 81/7405 [00:15<18:52,  6.47it/s]Tokenizer:   1%|          | 82/7405 [00:15<19:31,  6.25it/s]Tokenizer:   1%|          | 83/7405 [00:15<21:59,  5.55it/s]Tokenizer:   1%|          | 84/7405 [00:15<20:33,  5.94it/s]Tokenizer:   1%|          | 85/7405 [00:15<21:09,  5.77it/s]Tokenizer:   1%|          | 86/7405 [00:16<21:27,  5.68it/s]Tokenizer:   1%|          | 87/7405 [00:16<19:46,  6.17it/s]Tokenizer:   1%|          | 88/7405 [00:16<19:57,  6.11it/s]Tokenizer:   1%|          | 89/7405 [00:16<19:15,  6.33it/s]Tokenizer:   1%|          | 90/7405 [00:16<18:18,  6.66it/s]Tokenizer:   1%|          | 91/7405 [00:16<19:35,  6.22it/s]Tokenizer:   1%|          | 92/7405 [00:17<19:53,  6.13it/s]Tokenizer:   1%|▏         | 93/7405 [00:17<19:47,  6.16it/s]Tokenizer:   1%|▏         | 94/7405 [00:17<20:47,  5.86it/s]Tokenizer:   1%|▏         | 95/7405 [00:17<20:39,  5.90it/s]Tokenizer:   1%|▏         | 96/7405 [00:17<22:20,  5.45it/s]Tokenizer:   1%|▏         | 97/7405 [00:18<23:23,  5.21it/s]Tokenizer:   1%|▏         | 98/7405 [00:18<26:21,  4.62it/s]Tokenizer:   1%|▏         | 99/7405 [00:18<23:38,  5.15it/s]Tokenizer:   1%|▏         | 101/7405 [00:18<20:06,  6.05it/s]Tokenizer:   1%|▏         | 102/7405 [00:18<20:34,  5.92it/s]Tokenizer:   1%|▏         | 103/7405 [00:19<24:01,  5.06it/s]Tokenizer:   1%|▏         | 104/7405 [00:19<23:50,  5.10it/s]Tokenizer:   1%|▏         | 105/7405 [00:19<22:08,  5.50it/s]Tokenizer:   1%|▏         | 106/7405 [00:19<21:08,  5.75it/s]Tokenizer:   1%|▏         | 107/7405 [00:19<21:47,  5.58it/s]Tokenizer:   1%|▏         | 108/7405 [00:20<20:49,  5.84it/s]Tokenizer:   1%|▏         | 109/7405 [00:20<22:44,  5.35it/s]Tokenizer:   1%|▏         | 111/7405 [00:20<20:28,  5.94it/s]Tokenizer:   2%|▏         | 112/7405 [00:20<20:06,  6.04it/s]Tokenizer:   2%|▏         | 113/7405 [00:20<19:47,  6.14it/s]Tokenizer:   2%|▏         | 114/7405 [00:20<18:42,  6.50it/s]Tokenizer:   2%|▏         | 115/7405 [00:21<18:29,  6.57it/s]Tokenizer:   2%|▏         | 116/7405 [00:21<19:19,  6.28it/s]Tokenizer:   2%|▏         | 117/7405 [00:21<19:45,  6.15it/s]Tokenizer:   2%|▏         | 118/7405 [00:21<23:02,  5.27it/s]Tokenizer:   2%|▏         | 119/7405 [00:21<21:00,  5.78it/s]Tokenizer:   2%|▏         | 120/7405 [00:22<21:24,  5.67it/s]Tokenizer:   2%|▏         | 121/7405 [00:22<21:12,  5.73it/s]Tokenizer:   2%|▏         | 122/7405 [00:22<18:51,  6.44it/s]Tokenizer:   2%|▏         | 123/7405 [00:22<18:18,  6.63it/s]Tokenizer:   2%|▏         | 124/7405 [00:22<18:07,  6.69it/s]Tokenizer:   2%|▏         | 125/7405 [00:22<17:53,  6.78it/s]Tokenizer:   2%|▏         | 126/7405 [00:22<17:35,  6.89it/s]Tokenizer:   2%|▏         | 127/7405 [00:23<19:34,  6.19it/s]Tokenizer:   2%|▏         | 128/7405 [00:23<18:14,  6.65it/s]Tokenizer:   2%|▏         | 129/7405 [00:23<19:32,  6.21it/s]Tokenizer:   2%|▏         | 130/7405 [00:23<20:56,  5.79it/s]Tokenizer:   2%|▏         | 131/7405 [00:23<18:47,  6.45it/s]Tokenizer:   2%|▏         | 132/7405 [00:23<19:01,  6.37it/s]Tokenizer:   2%|▏         | 133/7405 [00:24<19:52,  6.10it/s]Tokenizer:   2%|▏         | 134/7405 [00:24<26:54,  4.50it/s]Tokenizer:   2%|▏         | 135/7405 [00:24<24:37,  4.92it/s]Tokenizer:   2%|▏         | 136/7405 [00:24<25:21,  4.78it/s]Tokenizer:   2%|▏         | 137/7405 [00:25<25:46,  4.70it/s]Tokenizer:   2%|▏         | 138/7405 [00:25<25:25,  4.76it/s]Tokenizer:   2%|▏         | 139/7405 [00:25<26:56,  4.49it/s]Tokenizer:   2%|▏         | 140/7405 [00:25<23:59,  5.05it/s]Tokenizer:   2%|▏         | 141/7405 [00:25<24:09,  5.01it/s]Tokenizer:   2%|▏         | 142/7405 [00:25<23:42,  5.11it/s]Tokenizer:   2%|▏         | 143/7405 [00:26<25:17,  4.78it/s]Tokenizer:   2%|▏         | 144/7405 [00:26<22:47,  5.31it/s]Tokenizer:   2%|▏         | 145/7405 [00:26<20:23,  5.93it/s]Tokenizer:   2%|▏         | 146/7405 [00:26<20:41,  5.85it/s]Tokenizer:   2%|▏         | 147/7405 [00:26<22:14,  5.44it/s]Tokenizer:   2%|▏         | 148/7405 [00:27<22:44,  5.32it/s]Tokenizer:   2%|▏         | 149/7405 [00:27<21:19,  5.67it/s]Tokenizer:   2%|▏         | 150/7405 [00:27<22:02,  5.49it/s]Tokenizer:   2%|▏         | 151/7405 [00:27<19:35,  6.17it/s]Tokenizer:   2%|▏         | 152/7405 [00:27<23:42,  5.10it/s]Tokenizer:   2%|▏         | 153/7405 [00:27<21:53,  5.52it/s]Tokenizer:   2%|▏         | 154/7405 [00:28<21:17,  5.68it/s]Tokenizer:   2%|▏         | 155/7405 [00:28<20:15,  5.96it/s]Tokenizer:   2%|▏         | 156/7405 [00:28<23:08,  5.22it/s]Tokenizer:   2%|▏         | 157/7405 [00:28<22:30,  5.37it/s]Tokenizer:   2%|▏         | 158/7405 [00:28<23:24,  5.16it/s]Tokenizer:   2%|▏         | 159/7405 [00:29<25:26,  4.75it/s]Tokenizer:   2%|▏         | 160/7405 [00:29<25:24,  4.75it/s]Tokenizer:   2%|▏         | 161/7405 [00:29<22:15,  5.42it/s]Tokenizer:   2%|▏         | 162/7405 [00:29<22:18,  5.41it/s]Tokenizer:   2%|▏         | 163/7405 [00:29<22:41,  5.32it/s]Tokenizer:   2%|▏         | 164/7405 [00:30<21:49,  5.53it/s]Tokenizer:   2%|▏         | 165/7405 [00:30<24:06,  5.00it/s]Tokenizer:   2%|▏         | 166/7405 [00:30<25:32,  4.72it/s]Tokenizer:   2%|▏         | 167/7405 [00:30<24:44,  4.88it/s]Tokenizer:   2%|▏         | 168/7405 [00:30<22:19,  5.40it/s]Tokenizer:   2%|▏         | 169/7405 [00:31<21:14,  5.68it/s]Tokenizer:   2%|▏         | 170/7405 [00:31<20:41,  5.83it/s]Tokenizer:   2%|▏         | 171/7405 [00:31<20:40,  5.83it/s]Tokenizer:   2%|▏         | 172/7405 [00:31<20:33,  5.86it/s]Tokenizer:   2%|▏         | 173/7405 [00:31<20:37,  5.84it/s]Tokenizer:   2%|▏         | 174/7405 [00:31<20:40,  5.83it/s]Tokenizer:   2%|▏         | 175/7405 [00:32<21:11,  5.69it/s]Tokenizer:   2%|▏         | 176/7405 [00:32<20:26,  5.89it/s]Tokenizer:   2%|▏         | 177/7405 [00:32<19:37,  6.14it/s]Tokenizer:   2%|▏         | 178/7405 [00:32<20:10,  5.97it/s]Tokenizer:   2%|▏         | 180/7405 [00:32<17:11,  7.00it/s]Tokenizer:   2%|▏         | 181/7405 [00:33<20:39,  5.83it/s]Tokenizer:   2%|▏         | 182/7405 [00:33<21:19,  5.64it/s]Tokenizer:   2%|▏         | 183/7405 [00:33<22:52,  5.26it/s]Tokenizer:   2%|▏         | 184/7405 [00:33<23:42,  5.08it/s]Tokenizer:   2%|▏         | 185/7405 [00:33<23:59,  5.01it/s]Tokenizer:   3%|▎         | 186/7405 [00:34<23:01,  5.23it/s]Tokenizer:   3%|▎         | 187/7405 [00:34<21:59,  5.47it/s]Tokenizer:   3%|▎         | 188/7405 [00:34<20:18,  5.92it/s]Tokenizer:   3%|▎         | 189/7405 [00:34<18:07,  6.64it/s]Tokenizer:   3%|▎         | 190/7405 [00:34<19:17,  6.23it/s]Tokenizer:   3%|▎         | 191/7405 [00:34<20:45,  5.79it/s]Tokenizer:   3%|▎         | 192/7405 [00:34<19:57,  6.02it/s]Tokenizer:   3%|▎         | 193/7405 [00:35<24:01,  5.00it/s]Tokenizer:   3%|▎         | 194/7405 [00:35<21:44,  5.53it/s]Tokenizer:   3%|▎         | 195/7405 [00:35<21:16,  5.65it/s]Tokenizer:   3%|▎         | 196/7405 [00:35<21:07,  5.69it/s]Tokenizer:   3%|▎         | 197/7405 [00:35<20:13,  5.94it/s]Tokenizer:   3%|▎         | 198/7405 [00:35<18:05,  6.64it/s]Tokenizer:   3%|▎         | 199/7405 [00:36<21:52,  5.49it/s]Tokenizer:   3%|▎         | 200/7405 [00:36<26:50,  4.47it/s]Tokenizer:   3%|▎         | 201/7405 [00:36<25:28,  4.71it/s]Tokenizer:   3%|▎         | 202/7405 [00:36<22:57,  5.23it/s]Tokenizer:   3%|▎         | 204/7405 [00:37<19:39,  6.11it/s]Tokenizer:   3%|▎         | 205/7405 [00:37<21:02,  5.70it/s]Tokenizer:   3%|▎         | 206/7405 [00:37<19:52,  6.04it/s]Tokenizer:   3%|▎         | 207/7405 [00:37<20:55,  5.73it/s]Tokenizer:   3%|▎         | 208/7405 [00:37<19:51,  6.04it/s]Tokenizer:   3%|▎         | 210/7405 [00:38<19:19,  6.20it/s]Tokenizer:   3%|▎         | 211/7405 [00:38<20:21,  5.89it/s]Tokenizer:   3%|▎         | 212/7405 [00:38<21:37,  5.54it/s]Tokenizer:   3%|▎         | 213/7405 [00:38<20:55,  5.73it/s]Tokenizer:   3%|▎         | 214/7405 [00:38<20:59,  5.71it/s]Tokenizer:   3%|▎         | 215/7405 [00:39<20:14,  5.92it/s]Tokenizer:   3%|▎         | 216/7405 [00:39<19:51,  6.03it/s]Tokenizer:   3%|▎         | 217/7405 [00:39<19:34,  6.12it/s]Tokenizer:   3%|▎         | 218/7405 [00:39<18:02,  6.64it/s]Tokenizer:   3%|▎         | 219/7405 [00:39<19:36,  6.11it/s]Tokenizer:   3%|▎         | 220/7405 [00:39<19:34,  6.12it/s]Tokenizer:   3%|▎         | 221/7405 [00:39<19:15,  6.22it/s]Tokenizer:   3%|▎         | 222/7405 [00:40<22:43,  5.27it/s]Tokenizer:   3%|▎         | 224/7405 [00:40<17:51,  6.70it/s]Tokenizer:   3%|▎         | 225/7405 [00:40<17:20,  6.90it/s]Tokenizer:   3%|▎         | 226/7405 [00:40<21:17,  5.62it/s]Tokenizer:   3%|▎         | 227/7405 [00:41<21:42,  5.51it/s]Tokenizer:   3%|▎         | 228/7405 [00:41<20:57,  5.71it/s]Tokenizer:   3%|▎         | 229/7405 [00:41<24:47,  4.83it/s]Tokenizer:   3%|▎         | 230/7405 [00:41<26:20,  4.54it/s]Tokenizer:   3%|▎         | 231/7405 [00:41<24:24,  4.90it/s]Tokenizer:   3%|▎         | 232/7405 [00:42<21:40,  5.52it/s]Tokenizer:   3%|▎         | 233/7405 [00:42<21:08,  5.65it/s]Tokenizer:   3%|▎         | 234/7405 [00:42<20:14,  5.91it/s]Tokenizer:   3%|▎         | 235/7405 [00:42<22:44,  5.25it/s]Tokenizer:   3%|▎         | 236/7405 [00:42<21:20,  5.60it/s]Tokenizer:   3%|▎         | 237/7405 [00:42<20:55,  5.71it/s]Tokenizer:   3%|▎         | 238/7405 [00:43<22:07,  5.40it/s]Tokenizer:   3%|▎         | 239/7405 [00:43<21:17,  5.61it/s]Tokenizer:   3%|▎         | 241/7405 [00:43<19:32,  6.11it/s]Tokenizer:   3%|▎         | 242/7405 [00:43<19:13,  6.21it/s]Tokenizer:   3%|▎         | 243/7405 [00:43<19:10,  6.23it/s]Tokenizer:   3%|▎         | 244/7405 [00:44<20:02,  5.96it/s]Tokenizer:   3%|▎         | 245/7405 [00:44<22:11,  5.38it/s]Tokenizer:   3%|▎         | 246/7405 [00:44<21:24,  5.58it/s]Tokenizer:   3%|▎         | 247/7405 [00:44<21:13,  5.62it/s]Tokenizer:   3%|▎         | 248/7405 [00:44<22:25,  5.32it/s]Tokenizer:   3%|▎         | 249/7405 [00:45<22:51,  5.22it/s]Tokenizer:   3%|▎         | 250/7405 [00:45<23:28,  5.08it/s]Tokenizer:   3%|▎         | 251/7405 [00:45<22:25,  5.32it/s]Tokenizer:   3%|▎         | 252/7405 [00:45<33:18,  3.58it/s]Tokenizer:   3%|▎         | 253/7405 [00:46<28:21,  4.20it/s]Tokenizer:   3%|▎         | 254/7405 [00:46<24:42,  4.82it/s]Tokenizer:   3%|▎         | 255/7405 [00:46<27:27,  4.34it/s]Tokenizer:   3%|▎         | 256/7405 [00:46<26:18,  4.53it/s]Tokenizer:   3%|▎         | 258/7405 [00:46<20:46,  5.73it/s]Tokenizer:   3%|▎         | 259/7405 [00:47<21:23,  5.57it/s]Tokenizer:   4%|▎         | 260/7405 [00:47<19:36,  6.07it/s]Tokenizer:   4%|▎         | 261/7405 [00:47<19:28,  6.11it/s]Tokenizer:   4%|▎         | 262/7405 [00:47<17:50,  6.67it/s]Tokenizer:   4%|▎         | 263/7405 [00:47<17:04,  6.97it/s]Tokenizer:   4%|▎         | 264/7405 [00:47<18:53,  6.30it/s]Tokenizer:   4%|▎         | 265/7405 [00:48<22:32,  5.28it/s]Tokenizer:   4%|▎         | 266/7405 [00:48<21:40,  5.49it/s]Tokenizer:   4%|▎         | 267/7405 [00:48<22:38,  5.26it/s]Tokenizer:   4%|▎         | 268/7405 [00:48<22:49,  5.21it/s]Tokenizer:   4%|▎         | 269/7405 [00:48<20:04,  5.92it/s]Tokenizer:   4%|▎         | 270/7405 [00:48<18:39,  6.37it/s]Tokenizer:   4%|▎         | 271/7405 [00:49<18:12,  6.53it/s]Tokenizer:   4%|▎         | 272/7405 [00:49<21:18,  5.58it/s]Tokenizer:   4%|▎         | 273/7405 [00:49<20:01,  5.94it/s]Tokenizer:   4%|▎         | 274/7405 [00:49<20:43,  5.74it/s]Tokenizer:   4%|▎         | 275/7405 [00:49<21:19,  5.57it/s]Tokenizer:   4%|▎         | 276/7405 [00:49<20:57,  5.67it/s]Tokenizer:   4%|▍         | 278/7405 [00:50<15:51,  7.49it/s]Tokenizer:   4%|▍         | 279/7405 [00:50<16:54,  7.02it/s]Tokenizer:   4%|▍         | 280/7405 [00:50<18:15,  6.51it/s]Tokenizer:   4%|▍         | 281/7405 [00:50<20:00,  5.94it/s]Tokenizer:   4%|▍         | 282/7405 [00:50<19:45,  6.01it/s]Tokenizer:   4%|▍         | 283/7405 [00:51<20:36,  5.76it/s]Tokenizer:   4%|▍         | 284/7405 [00:51<21:07,  5.62it/s]Tokenizer:   4%|▍         | 285/7405 [00:51<19:22,  6.12it/s]Tokenizer:   4%|▍         | 287/7405 [00:51<16:20,  7.26it/s]Tokenizer:   4%|▍         | 288/7405 [00:51<19:00,  6.24it/s]Tokenizer:   4%|▍         | 289/7405 [00:51<18:05,  6.56it/s]Tokenizer:   4%|▍         | 290/7405 [00:52<19:00,  6.24it/s]Tokenizer:   4%|▍         | 291/7405 [00:52<19:22,  6.12it/s]Tokenizer:   4%|▍         | 292/7405 [00:52<21:05,  5.62it/s]Tokenizer:   4%|▍         | 293/7405 [00:52<20:40,  5.74it/s]Tokenizer:   4%|▍         | 294/7405 [00:52<18:27,  6.42it/s]Tokenizer:   4%|▍         | 295/7405 [00:53<19:40,  6.02it/s]Tokenizer:   4%|▍         | 296/7405 [00:53<18:56,  6.25it/s]Tokenizer:   4%|▍         | 297/7405 [00:53<18:37,  6.36it/s]Tokenizer:   4%|▍         | 298/7405 [00:53<20:23,  5.81it/s]Tokenizer:   4%|▍         | 299/7405 [00:53<24:15,  4.88it/s]Tokenizer:   4%|▍         | 300/7405 [00:54<24:59,  4.74it/s]Tokenizer:   4%|▍         | 301/7405 [00:54<22:13,  5.33it/s]Tokenizer:   4%|▍         | 302/7405 [00:54<20:17,  5.83it/s]Tokenizer:   4%|▍         | 303/7405 [00:54<18:37,  6.36it/s]Tokenizer:   4%|▍         | 304/7405 [00:54<17:10,  6.89it/s]Tokenizer:   4%|▍         | 305/7405 [00:54<17:23,  6.80it/s]Tokenizer:   4%|▍         | 306/7405 [00:54<18:17,  6.47it/s]Tokenizer:   4%|▍         | 307/7405 [00:54<18:08,  6.52it/s]Tokenizer:   4%|▍         | 308/7405 [00:55<16:54,  6.99it/s]Tokenizer:   4%|▍         | 309/7405 [00:55<17:25,  6.78it/s]Tokenizer:   4%|▍         | 310/7405 [00:55<17:27,  6.77it/s]Tokenizer:   4%|▍         | 311/7405 [00:55<17:20,  6.82it/s]Tokenizer:   4%|▍         | 312/7405 [00:55<19:36,  6.03it/s]Tokenizer:   4%|▍         | 313/7405 [00:55<17:44,  6.66it/s]Tokenizer:   4%|▍         | 314/7405 [00:56<16:41,  7.08it/s]Tokenizer:   4%|▍         | 315/7405 [00:56<24:01,  4.92it/s]Tokenizer:   4%|▍         | 316/7405 [00:56<21:22,  5.53it/s]Tokenizer:   4%|▍         | 317/7405 [00:56<19:57,  5.92it/s]Tokenizer:   4%|▍         | 318/7405 [00:56<21:56,  5.38it/s]Tokenizer:   4%|▍         | 319/7405 [00:57<20:56,  5.64it/s]Tokenizer:   4%|▍         | 320/7405 [00:57<21:35,  5.47it/s]Tokenizer:   4%|▍         | 321/7405 [00:57<21:08,  5.59it/s]Tokenizer:   4%|▍         | 322/7405 [00:57<19:58,  5.91it/s]Tokenizer:   4%|▍         | 323/7405 [00:57<20:38,  5.72it/s]Tokenizer:   4%|▍         | 324/7405 [00:57<22:22,  5.27it/s]Tokenizer:   4%|▍         | 325/7405 [00:58<21:25,  5.51it/s]Tokenizer:   4%|▍         | 326/7405 [00:58<20:00,  5.89it/s]Tokenizer:   4%|▍         | 327/7405 [00:58<21:10,  5.57it/s]Tokenizer:   4%|▍         | 328/7405 [00:58<22:33,  5.23it/s]Tokenizer:   4%|▍         | 329/7405 [00:58<21:34,  5.46it/s]Tokenizer:   4%|▍         | 330/7405 [00:58<20:43,  5.69it/s]Tokenizer:   4%|▍         | 331/7405 [00:59<20:16,  5.81it/s]Tokenizer:   4%|▍         | 332/7405 [00:59<18:10,  6.49it/s]Tokenizer:   4%|▍         | 333/7405 [00:59<19:07,  6.16it/s]Tokenizer:   5%|▍         | 334/7405 [00:59<19:10,  6.15it/s]Tokenizer:   5%|▍         | 335/7405 [00:59<18:07,  6.50it/s]Tokenizer:   5%|▍         | 336/7405 [00:59<20:32,  5.73it/s]Tokenizer:   5%|▍         | 337/7405 [01:00<20:43,  5.68it/s]Tokenizer:   5%|▍         | 338/7405 [01:00<20:39,  5.70it/s]Tokenizer:   5%|▍         | 340/7405 [01:00<17:15,  6.82it/s]Tokenizer:   5%|▍         | 341/7405 [01:00<17:58,  6.55it/s]Tokenizer:   5%|▍         | 342/7405 [01:00<17:15,  6.82it/s]Tokenizer:   5%|▍         | 343/7405 [01:01<18:29,  6.36it/s]Tokenizer:   5%|▍         | 344/7405 [01:01<19:29,  6.04it/s]Tokenizer:   5%|▍         | 345/7405 [01:01<18:23,  6.40it/s]Tokenizer:   5%|▍         | 346/7405 [01:01<20:00,  5.88it/s]Tokenizer:   5%|▍         | 347/7405 [01:01<20:22,  5.77it/s]Tokenizer:   5%|▍         | 349/7405 [01:01<17:17,  6.80it/s]Tokenizer:   5%|▍         | 350/7405 [01:02<16:33,  7.10it/s]Tokenizer:   5%|▍         | 351/7405 [01:02<17:50,  6.59it/s]Tokenizer:   5%|▍         | 352/7405 [01:02<16:40,  7.05it/s]Tokenizer:   5%|▍         | 353/7405 [01:02<18:14,  6.44it/s]Tokenizer:   5%|▍         | 354/7405 [01:02<19:53,  5.91it/s]Tokenizer:   5%|▍         | 355/7405 [01:02<18:42,  6.28it/s]Tokenizer:   5%|▍         | 356/7405 [01:03<19:58,  5.88it/s]Tokenizer:   5%|▍         | 357/7405 [01:03<23:58,  4.90it/s]Tokenizer:   5%|▍         | 358/7405 [01:03<23:21,  5.03it/s]Tokenizer:   5%|▍         | 359/7405 [01:03<23:59,  4.90it/s]Tokenizer:   5%|▍         | 360/7405 [01:03<22:17,  5.27it/s]Tokenizer:   5%|▍         | 361/7405 [01:04<21:34,  5.44it/s]Tokenizer:   5%|▍         | 362/7405 [01:04<23:14,  5.05it/s]Tokenizer:   5%|▍         | 363/7405 [01:04<22:49,  5.14it/s]Tokenizer:   5%|▍         | 364/7405 [01:04<23:24,  5.01it/s]Tokenizer:   5%|▍         | 365/7405 [01:04<24:16,  4.83it/s]Tokenizer:   5%|▍         | 366/7405 [01:05<21:02,  5.57it/s]Tokenizer:   5%|▍         | 367/7405 [01:05<20:26,  5.74it/s]Tokenizer:   5%|▍         | 368/7405 [01:05<20:39,  5.68it/s]Tokenizer:   5%|▍         | 369/7405 [01:05<20:38,  5.68it/s]Tokenizer:   5%|▍         | 370/7405 [01:05<20:36,  5.69it/s]Tokenizer:   5%|▌         | 371/7405 [01:05<19:13,  6.10it/s]Tokenizer:   5%|▌         | 372/7405 [01:06<19:25,  6.03it/s]Tokenizer:   5%|▌         | 373/7405 [01:06<19:51,  5.90it/s]Tokenizer:   5%|▌         | 374/7405 [01:06<17:37,  6.65it/s]Tokenizer:   5%|▌         | 375/7405 [01:06<19:32,  6.00it/s]Tokenizer:   5%|▌         | 376/7405 [01:06<21:55,  5.34it/s]Tokenizer:   5%|▌         | 377/7405 [01:06<20:05,  5.83it/s]Tokenizer:   5%|▌         | 378/7405 [01:07<21:42,  5.40it/s]Tokenizer:   5%|▌         | 379/7405 [01:07<21:50,  5.36it/s]Tokenizer:   5%|▌         | 380/7405 [01:07<20:39,  5.67it/s]Tokenizer:   5%|▌         | 381/7405 [01:07<19:05,  6.13it/s]Tokenizer:   5%|▌         | 382/7405 [01:07<18:37,  6.28it/s]Tokenizer:   5%|▌         | 383/7405 [01:07<18:47,  6.23it/s]Tokenizer:   5%|▌         | 384/7405 [01:08<20:30,  5.70it/s]Tokenizer:   5%|▌         | 385/7405 [01:08<19:20,  6.05it/s]Tokenizer:   5%|▌         | 386/7405 [01:08<21:16,  5.50it/s]Tokenizer:   5%|▌         | 387/7405 [01:08<23:07,  5.06it/s]Tokenizer:   5%|▌         | 388/7405 [01:08<21:30,  5.44it/s]Tokenizer:   5%|▌         | 389/7405 [01:09<20:18,  5.76it/s]Tokenizer:   5%|▌         | 390/7405 [01:09<21:15,  5.50it/s]Tokenizer:   5%|▌         | 391/7405 [01:09<20:33,  5.68it/s]Tokenizer:   5%|▌         | 392/7405 [01:09<21:22,  5.47it/s]Tokenizer:   5%|▌         | 393/7405 [01:09<19:02,  6.14it/s]Tokenizer:   5%|▌         | 394/7405 [01:09<17:16,  6.76it/s]Tokenizer:   5%|▌         | 395/7405 [01:10<23:44,  4.92it/s]Tokenizer:   5%|▌         | 396/7405 [01:10<24:53,  4.69it/s]Tokenizer:   5%|▌         | 397/7405 [01:10<21:26,  5.45it/s]Tokenizer:   5%|▌         | 398/7405 [01:10<20:39,  5.65it/s]Tokenizer:   5%|▌         | 399/7405 [01:10<20:33,  5.68it/s]Tokenizer:   5%|▌         | 400/7405 [01:11<19:38,  5.94it/s]Tokenizer:   5%|▌         | 401/7405 [01:11<18:41,  6.24it/s]Tokenizer:   5%|▌         | 402/7405 [01:11<18:19,  6.37it/s]Tokenizer:   5%|▌         | 403/7405 [01:11<17:52,  6.53it/s]Tokenizer:   5%|▌         | 404/7405 [01:11<19:25,  6.01it/s]Tokenizer:   5%|▌         | 405/7405 [01:11<20:46,  5.62it/s]Tokenizer:   5%|▌         | 406/7405 [01:12<19:40,  5.93it/s]Tokenizer:   5%|▌         | 407/7405 [01:12<21:19,  5.47it/s]Tokenizer:   6%|▌         | 408/7405 [01:12<20:38,  5.65it/s]Tokenizer:   6%|▌         | 409/7405 [01:12<20:39,  5.65it/s]Tokenizer:   6%|▌         | 410/7405 [01:12<19:35,  5.95it/s]Tokenizer:   6%|▌         | 411/7405 [01:12<19:17,  6.04it/s]Tokenizer:   6%|▌         | 412/7405 [01:12<17:48,  6.55it/s]Tokenizer:   6%|▌         | 414/7405 [01:13<14:24,  8.08it/s]Tokenizer:   6%|▌         | 415/7405 [01:13<17:36,  6.62it/s]Tokenizer:   6%|▌         | 416/7405 [01:13<17:33,  6.64it/s]Tokenizer:   6%|▌         | 417/7405 [01:13<20:09,  5.78it/s]Tokenizer:   6%|▌         | 418/7405 [01:13<20:39,  5.64it/s]Tokenizer:   6%|▌         | 419/7405 [01:14<21:09,  5.50it/s]Tokenizer:   6%|▌         | 420/7405 [01:14<18:44,  6.21it/s]Tokenizer:   6%|▌         | 421/7405 [01:14<18:36,  6.26it/s]Tokenizer:   6%|▌         | 422/7405 [01:14<19:37,  5.93it/s]Tokenizer:   6%|▌         | 423/7405 [01:14<19:41,  5.91it/s]Tokenizer:   6%|▌         | 424/7405 [01:15<22:17,  5.22it/s]Tokenizer:   6%|▌         | 425/7405 [01:15<21:24,  5.43it/s]Tokenizer:   6%|▌         | 426/7405 [01:15<21:21,  5.45it/s]Tokenizer:   6%|▌         | 427/7405 [01:15<20:10,  5.77it/s]Tokenizer:   6%|▌         | 428/7405 [01:15<19:20,  6.01it/s]Tokenizer:   6%|▌         | 429/7405 [01:15<21:24,  5.43it/s]Tokenizer:   6%|▌         | 430/7405 [01:16<22:08,  5.25it/s]Tokenizer:   6%|▌         | 431/7405 [01:16<21:06,  5.51it/s]Tokenizer:   6%|▌         | 432/7405 [01:16<24:26,  4.76it/s]Tokenizer:   6%|▌         | 433/7405 [01:16<21:42,  5.35it/s]Tokenizer:   6%|▌         | 434/7405 [01:16<20:02,  5.80it/s]Tokenizer:   6%|▌         | 435/7405 [01:16<18:52,  6.15it/s]Tokenizer:   6%|▌         | 436/7405 [01:17<21:15,  5.46it/s]Tokenizer:   6%|▌         | 437/7405 [01:17<20:31,  5.66it/s]Tokenizer:   6%|▌         | 438/7405 [01:17<18:45,  6.19it/s]Tokenizer:   6%|▌         | 439/7405 [01:17<19:13,  6.04it/s]Tokenizer:   6%|▌         | 440/7405 [01:17<18:49,  6.16it/s]Tokenizer:   6%|▌         | 441/7405 [01:17<17:56,  6.47it/s]Tokenizer:   6%|▌         | 442/7405 [01:18<19:25,  5.97it/s]Tokenizer:   6%|▌         | 443/7405 [01:18<18:15,  6.35it/s]Tokenizer:   6%|▌         | 444/7405 [01:18<18:39,  6.22it/s]Tokenizer:   6%|▌         | 445/7405 [01:18<20:01,  5.79it/s]Tokenizer:   6%|▌         | 446/7405 [01:18<19:48,  5.85it/s]Tokenizer:   6%|▌         | 447/7405 [01:18<18:41,  6.20it/s]Tokenizer:   6%|▌         | 448/7405 [01:19<17:23,  6.67it/s]Tokenizer:   6%|▌         | 449/7405 [01:19<17:50,  6.50it/s]Tokenizer:   6%|▌         | 450/7405 [01:19<20:11,  5.74it/s]Tokenizer:   6%|▌         | 451/7405 [01:19<23:57,  4.84it/s]Tokenizer:   6%|▌         | 452/7405 [01:19<24:15,  4.78it/s]Tokenizer:   6%|▌         | 453/7405 [01:20<22:03,  5.25it/s]Tokenizer:   6%|▌         | 454/7405 [01:20<22:11,  5.22it/s]Tokenizer:   6%|▌         | 455/7405 [01:20<21:31,  5.38it/s]Tokenizer:   6%|▌         | 456/7405 [01:20<20:41,  5.60it/s]Tokenizer:   6%|▌         | 457/7405 [01:20<21:39,  5.35it/s]Tokenizer:   6%|▌         | 458/7405 [01:21<21:44,  5.33it/s]Tokenizer:   6%|▌         | 459/7405 [01:21<21:07,  5.48it/s]Tokenizer:   6%|▌         | 460/7405 [01:21<20:50,  5.55it/s]Tokenizer:   6%|▌         | 461/7405 [01:21<19:14,  6.01it/s]Tokenizer:   6%|▌         | 462/7405 [01:21<20:44,  5.58it/s]Tokenizer:   6%|▋         | 463/7405 [01:21<19:50,  5.83it/s]Tokenizer:   6%|▋         | 464/7405 [01:22<18:56,  6.11it/s]Tokenizer:   6%|▋         | 465/7405 [01:22<20:05,  5.76it/s]Tokenizer:   6%|▋         | 466/7405 [01:22<19:22,  5.97it/s]Tokenizer:   6%|▋         | 468/7405 [01:22<16:15,  7.11it/s]Tokenizer:   6%|▋         | 469/7405 [01:22<16:01,  7.21it/s]Tokenizer:   6%|▋         | 470/7405 [01:22<15:09,  7.63it/s]Tokenizer:   6%|▋         | 471/7405 [01:22<14:38,  7.89it/s]Tokenizer:   6%|▋         | 472/7405 [01:23<16:52,  6.85it/s]Tokenizer:   6%|▋         | 473/7405 [01:23<16:39,  6.94it/s]Tokenizer:   6%|▋         | 474/7405 [01:23<16:26,  7.03it/s]Tokenizer:   6%|▋         | 475/7405 [01:23<16:00,  7.21it/s]Tokenizer:   6%|▋         | 476/7405 [01:23<15:52,  7.27it/s]Tokenizer:   6%|▋         | 477/7405 [01:23<20:12,  5.72it/s]Tokenizer:   6%|▋         | 478/7405 [01:24<19:35,  5.89it/s]Tokenizer:   6%|▋         | 479/7405 [01:24<21:32,  5.36it/s]Tokenizer:   6%|▋         | 480/7405 [01:24<20:39,  5.59it/s]Tokenizer:   6%|▋         | 481/7405 [01:24<20:28,  5.64it/s]Tokenizer:   7%|▋         | 482/7405 [01:24<20:57,  5.51it/s]Tokenizer:   7%|▋         | 483/7405 [01:25<20:41,  5.58it/s]Tokenizer:   7%|▋         | 484/7405 [01:25<22:43,  5.07it/s]Tokenizer:   7%|▋         | 485/7405 [01:25<22:21,  5.16it/s]Tokenizer:   7%|▋         | 486/7405 [01:25<19:23,  5.95it/s]Tokenizer:   7%|▋         | 487/7405 [01:25<19:24,  5.94it/s]Tokenizer:   7%|▋         | 488/7405 [01:25<20:16,  5.68it/s]Tokenizer:   7%|▋         | 489/7405 [01:26<20:10,  5.72it/s]Tokenizer:   7%|▋         | 490/7405 [01:26<20:08,  5.72it/s]Tokenizer:   7%|▋         | 491/7405 [01:26<20:18,  5.67it/s]Tokenizer:   7%|▋         | 492/7405 [01:26<18:50,  6.11it/s]nohup: ignoring input
Some weights of the model checkpoint at /data2/wangbingchao/database/bert_pretrained/bert-base-uncased were not used when initializing SentenceChoice: ['bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'cls.predictions.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.8.output.dense.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias']
- This IS expected if you are initializing SentenceChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SentenceChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of SentenceChoice were not initialized from the model checkpoint at /data2/wangbingchao/database/bert_pretrained/bert-base-uncased and are newly initialized: ['c_lstm.1.weight_ih_l0', 'c_lstm.1.bias_hh_l1_reverse', 'c_lstm.0.bias_hh_l0', 'c_lstm.0.weight_ih_l0', 'c_lstm.0.weight_ih_l0_reverse', 'c_lstm.1.bias_hh_l0', 'c_lstm.0.bias_ih_l1', 'c_lstm.0.weight_hh_l0', 'c_lstm.1.weight_ih_l1', 'c_lstm.0.bias_hh_l1', 'c_lstm.1.weight_ih_l1_reverse', 'q_lstm.weight_hh_l1', 'liner.weight', 'q_lstm.bias_hh_l1_reverse', 'c_lstm.1.weight_hh_l1_reverse', 'q_lstm.bias_hh_l2_reverse', 'c_lstm.1.weight_hh_l0', 'c_lstm.1.bias_ih_l1_reverse', 'c_lstm.0.bias_hh_l2_reverse', 'c_lstm.1.bias_hh_l2', 'c_lstm.2.weight_hh_l2_reverse', 'c_lstm.0.bias_hh_l1_reverse', 'q_lstm.weight_hh_l0_reverse', 'q_lstm.bias_ih_l2_reverse', 'c_lstm.0.bias_ih_l2_reverse', 'attention.k_proj.bias', 'c_lstm.0.weight_ih_l1', 'c_lstm.1.bias_hh_l0_reverse', 'c_lstm.1.weight_hh_l1', 'c_lstm.2.weight_ih_l1', 'attention.q_proj.weight', 'c_lstm.2.weight_ih_l2_reverse', 'c_lstm.0.weight_ih_l1_reverse', 'q_lstm.weight_ih_l0', 'liner.bias', 'c_lstm.0.bias_ih_l2', 'q_lstm.bias_ih_l0_reverse', 'c_lstm.2.bias_hh_l1', 'c_lstm.0.weight_ih_l2_reverse', 'q_lstm.weight_ih_l1', 'c_lstm.1.weight_hh_l2', 'c_lstm.2.bias_ih_l0_reverse', 'c_lstm.0.bias_hh_l0_reverse', 'q_lstm.bias_ih_l1_reverse', 'q_lstm.weight_hh_l2', 'c_lstm.1.bias_ih_l1', 'c_lstm.2.bias_hh_l2', 'q_lstm.weight_hh_l2_reverse', 'c_lstm.2.weight_ih_l2', 'c_lstm.2.bias_ih_l1', 'q_lstm.weight_ih_l1_reverse', 'c_lstm.2.bias_hh_l0', 'c_lstm.1.bias_hh_l2_reverse', 'c_lstm.0.weight_hh_l0_reverse', 'c_lstm.2.bias_hh_l2_reverse', 'c_lstm.2.weight_hh_l1_reverse', 'q_lstm.weight_ih_l0_reverse', 'c_lstm.2.bias_hh_l1_reverse', 'q_lstm.bias_ih_l2', 'q_lstm.weight_hh_l1_reverse', 'c_lstm.1.bias_ih_l2', 'q_lstm.weight_ih_l2_reverse', 'bert.weight', 'attention.k_proj.weight', 'q_lstm.bias_hh_l2', 'q_lstm.weight_hh_l0', 'c_lstm.1.bias_ih_l0', 'c_lstm.2.weight_hh_l0', 'c_lstm.1.bias_ih_l0_reverse', 'attention.out_proj.bias', 'c_lstm.2.bias_ih_l0', 'c_lstm.2.bias_ih_l1_reverse', 'q_lstm.weight_ih_l2', 'c_lstm.2.bias_hh_l0_reverse', 'c_lstm.0.bias_ih_l1_reverse', 'c_lstm.2.weight_ih_l1_reverse', 'attention.q_proj.bias', 'c_lstm.0.weight_hh_l1_reverse', 'attention.v_proj.bias', 'c_lstm.2.weight_ih_l0_reverse', 'c_lstm.1.weight_ih_l0_reverse', 'attention.out_proj.weight', 'c_lstm.0.weight_hh_l2_reverse', 'c_lstm.1.weight_ih_l2', 'c_lstm.0.weight_ih_l2', 'q_lstm.bias_hh_l0_reverse', 'c_lstm.0.weight_hh_l1', 'c_lstm.2.weight_hh_l2', 'c_lstm.2.bias_ih_l2_reverse', 'q_lstm.bias_hh_l0', 'c_lstm.1.bias_ih_l2_reverse', 'c_lstm.2.weight_ih_l0', 'c_lstm.2.weight_hh_l0_reverse', 'attention.v_proj.weight', 'c_lstm.1.weight_hh_l0_reverse', 'c_lstm.1.weight_ih_l2_reverse', 'c_lstm.2.weight_hh_l1', 'c_lstm.1.weight_hh_l2_reverse', 'c_lstm.0.weight_hh_l2', 'c_lstm.1.bias_hh_l1', 'c_lstm.0.bias_ih_l0_reverse', 'q_lstm.bias_hh_l1', 'c_lstm.2.bias_ih_l2', 'q_lstm.bias_ih_l0', 'c_lstm.0.bias_ih_l0', 'q_lstm.bias_ih_l1', 'c_lstm.0.bias_hh_l2']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
创建文件夹...
[22-05-05 12:55:08] --Trainer-- INFO: Load pretrained model from file /data2/wangbingchao/database/bert_pretrained/bert-base-uncased ...
[22-05-05 12:55:08] --Trainer-- INFO: Load pretrained model config finished!!!
[22-05-05 12:55:08] --Trainer-- INFO: Define model...
[22-05-05 12:55:11] --Trainer-- INFO: use cuda to train
[22-05-05 12:55:22] --Trainer-- INFO: Use Multi-GPUs[0, 1, 2]
[22-05-05 12:55:22] --DropDataloader-- INFO: Loading dataset from cached file /data2/maqi/LongTextDatasets/LongTextModels/cache/cache_train_hotpot_dev_fullwiki_v1.json_bert-base-uncased
[22-05-05 12:55:23] --Trainer-- INFO: Define model finished!!!
[22-05-05 12:55:23] --Trainer-- INFO: model config output dir: /data2/maqi/LongTextDatasets/LongTextModels/output/exp_5-5/config.txt
 0%|                                                                                                     |0/309[00:00<?]Epoch: 1/4:  0%|                                                                                         |0/309[00:00<?]/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/modules/rnn.py:662: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1616554827596/work/aten/src/ATen/native/cudnn/RNN.cpp:915.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Epoch: 1/4:  0%|                                                                             |0/309[00:23<?, loss=0.693]Epoch: 1/4:  0%|                           |0/309[00:23<?, Iter=0, Loss=0.693147, Step=0, UsedTime=0:00:23, lr=0.000000]Epoch: 1/4:  0%|                     |1/309[00:23<2:00:26, Iter=0, Loss=0.693147, Step=0, UsedTime=0:00:23, lr=0.000000]Epoch: 1/4:  1%|▏                      |2/309[00:26<58:50, Iter=0, Loss=0.693147, Step=0, UsedTime=0:00:23, lr=0.000000]Epoch: 1/4:  1%|▍                                                                        |2/309[00:30<58:50, loss=0.693]Epoch: 1/4:  1%|▏                      |2/309[00:30<58:50, Iter=1, Loss=0.693147, Step=1, UsedTime=0:00:30, lr=0.000001]Epoch: 1/4:  1%|▏                      |3/309[00:30<40:37, Iter=1, Loss=0.693147, Step=1, UsedTime=0:00:30, lr=0.000001]Epoch: 1/4:  1%|▋                                                                        |3/309[00:37<40:37, loss=0.693]Epoch: 1/4:  1%|▏                      |3/309[00:37<40:37, Iter=2, Loss=0.693147, Step=2, UsedTime=0:00:37, lr=0.000002]Epoch: 1/4:  1%|▎                      |4/309[00:37<38:17, Iter=2, Loss=0.693147, Step=2, UsedTime=0:00:37, lr=0.000002]Epoch: 1/4:  2%|▎                      |5/309[00:40<30:02, Iter=2, Loss=0.693147, Step=2, UsedTime=0:00:37, lr=0.000002]Epoch: 1/4:  2%|█▏                                                                       |5/309[00:44<30:02, loss=0.693]Epoch: 1/4:  2%|▎                      |5/309[00:44<30:02, Iter=3, Loss=0.693147, Step=3, UsedTime=0:00:44, lr=0.000003]Epoch: 1/4:  2%|▍                      |6/309[00:44<26:13, Iter=3, Loss=0.693147, Step=3, UsedTime=0:00:44, lr=0.000003]Epoch: 1/4:  2%|█▍                                                                       |6/309[00:50<26:13, loss=0.693]Epoch: 1/4:  2%|▍                      |6/309[00:50<26:13, Iter=4, Loss=0.693147, Step=4, UsedTime=0:00:50, lr=0.000004]Epoch: 1/4:  2%|▌                      |7/309[00:50<28:49, Iter=4, Loss=0.693147, Step=4, UsedTime=0:00:50, lr=0.000004]Epoch: 1/4:  2%|█▋                                                                       |7/309[00:57<28:49, loss=0.693]Epoch: 1/4:  2%|▌                      |7/309[00:57<28:49, Iter=5, Loss=0.693147, Step=5, UsedTime=0:00:57, lr=0.000005]Epoch: 1/4:  3%|▌                      |8/309[00:57<30:39, Iter=5, Loss=0.693147, Step=5, UsedTime=0:00:57, lr=0.000005]Epoch: 1/4:  3%|▋                      |9/309[01:00<25:56, Iter=5, Loss=0.693147, Step=5, UsedTime=0:00:57, lr=0.000005]Epoch: 1/4:  3%|██▏                                                                      |9/309[01:04<25:56, loss=0.693]Epoch: 1/4:  3%|▋                      |9/309[01:04<25:56, Iter=6, Loss=0.693147, Step=6, UsedTime=0:01:04, lr=0.000006]Epoch: 1/4:  3%|▋                     |10/309[01:04<23:35, Iter=6, Loss=0.693147, Step=6, UsedTime=0:01:04, lr=0.000006]Epoch: 1/4:  3%|██▎                                                                     |10/309[01:11<23:35, loss=0.693]Epoch: 1/4:  3%|▋                     |10/309[01:11<23:35, Iter=7, Loss=0.693147, Step=7, UsedTime=0:01:11, lr=0.000007]Epoch: 1/4:  4%|▊                     |11/309[01:11<26:38, Iter=7, Loss=0.693147, Step=7, UsedTime=0:01:11, lr=0.000007]Epoch: 1/4:  4%|██▌                                                                     |11/309[01:18<26:38, loss=0.693]Epoch: 1/4:  4%|▊                     |11/309[01:18<26:38, Iter=8, Loss=0.693147, Step=8, UsedTime=0:01:18, lr=0.000008]Epoch: 1/4:  4%|▊                     |12/309[01:18<28:48, Iter=8, Loss=0.693147, Step=8, UsedTime=0:01:18, lr=0.000008]Epoch: 1/4:  4%|██▊                                                                     |12/309[01:25<28:48, loss=0.693]Epoch: 1/4:  4%|▊                     |12/309[01:25<28:48, Iter=9, Loss=0.693147, Step=9, UsedTime=0:01:25, lr=0.000009]Epoch: 1/4:  4%|▉                     |13/309[01:25<30:13, Iter=9, Loss=0.693147, Step=9, UsedTime=0:01:25, lr=0.000009]Epoch: 1/4:  5%|▉                     |14/309[01:28<25:41, Iter=9, Loss=0.693147, Step=9, UsedTime=0:01:25, lr=0.000009]Epoch: 1/4:  5%|███▎                                                                    |14/309[01:32<25:41, loss=0.693]Epoch: 1/4:  5%|▉                   |14/309[01:32<25:41, Iter=10, Loss=0.693147, Step=10, UsedTime=0:01:32, lr=0.000010]Epoch: 1/4:  5%|▉                   |15/309[01:32<23:21, Iter=10, Loss=0.693147, Step=10, UsedTime=0:01:32, lr=0.000010]Epoch: 1/4:  5%|███▍                                                                    |15/309[01:38<23:21, loss=0.693]Epoch: 1/4:  5%|▉                   |15/309[01:38<23:21, Iter=11, Loss=0.693147, Step=11, UsedTime=0:01:38, lr=0.000011]Epoch: 1/4:  5%|█                   |16/309[01:38<26:23, Iter=11, Loss=0.693147, Step=11, UsedTime=0:01:38, lr=0.000011]Epoch: 1/4:  5%|███▋                                                                    |16/309[01:45<26:23, loss=0.693]Epoch: 1/4:  5%|█                   |16/309[01:45<26:23, Iter=12, Loss=0.693147, Step=12, UsedTime=0:01:45, lr=0.000012]Epoch: 1/4:  6%|█                   |17/309[01:45<28:28, Iter=12, Loss=0.693147, Step=12, UsedTime=0:01:45, lr=0.000012]Epoch: 1/4:  6%|███▉                                                                    |17/309[01:52<28:28, loss=0.693]Epoch: 1/4:  6%|█                   |17/309[01:52<28:28, Iter=13, Loss=0.693147, Step=13, UsedTime=0:01:52, lr=0.000013]Epoch: 1/4:  6%|█▏                  |18/309[01:52<29:49, Iter=13, Loss=0.693147, Step=13, UsedTime=0:01:52, lr=0.000013]Epoch: 1/4:  6%|████▏                                                                   |18/309[01:59<29:49, loss=0.693]Epoch: 1/4:  6%|█▏                  |18/309[01:59<29:49, Iter=14, Loss=0.693147, Step=14, UsedTime=0:01:59, lr=0.000014]Epoch: 1/4:  6%|█▏                  |19/309[01:59<30:53, Iter=14, Loss=0.693147, Step=14, UsedTime=0:01:59, lr=0.000014]Epoch: 1/4:  6%|█▎                  |20/309[02:02<26:04, Iter=14, Loss=0.693147, Step=14, UsedTime=0:01:59, lr=0.000014]Epoch: 1/4:  6%|████▋                                                                   |20/309[02:06<26:04, loss=0.693]Epoch: 1/4:  6%|█▎                  |20/309[02:06<26:04, Iter=15, Loss=0.693147, Step=15, UsedTime=0:02:06, lr=0.000015]Epoch: 1/4:  7%|█▎                  |21/309[02:06<23:33, Iter=15, Loss=0.693147, Step=15, UsedTime=0:02:06, lr=0.000015]Epoch: 1/4:  7%|████▉                                                                   |21/309[02:13<23:33, loss=0.693]Epoch: 1/4:  7%|█▎                  |21/309[02:13<23:33, Iter=16, Loss=0.693147, Step=16, UsedTime=0:02:13, lr=0.000016]Epoch: 1/4:  7%|█▍                  |22/309[02:13<26:17, Iter=16, Loss=0.693147, Step=16, UsedTime=0:02:13, lr=0.000016]Epoch: 1/4:  7%|█████▏                                                                  |22/309[02:20<26:17, loss=0.693]Epoch: 1/4:  7%|█▍                  |22/309[02:20<26:17, Iter=17, Loss=0.693147, Step=17, UsedTime=0:02:20, lr=0.000017]Epoch: 1/4:  7%|█▍                  |23/309[02:20<28:04, Iter=17, Loss=0.693147, Step=17, UsedTime=0:02:20, lr=0.000017]Epoch: 1/4:  7%|█████▎                                                                  |23/309[02:27<28:04, loss=0.693]Epoch: 1/4:  7%|█▍                  |23/309[02:27<28:04, Iter=18, Loss=0.693147, Step=18, UsedTime=0:02:27, lr=0.000018]Epoch: 1/4:  8%|█▌                  |24/309[02:27<29:28, Iter=18, Loss=0.693147, Step=18, UsedTime=0:02:27, lr=0.000018]Epoch: 1/4:  8%|█████▌                                                                  |24/309[02:33<29:28, loss=0.693]Epoch: 1/4:  8%|█▌                  |24/309[02:33<29:28, Iter=19, Loss=0.693147, Step=19, UsedTime=0:02:33, lr=0.000019]Epoch: 1/4:  8%|█▌                  |25/309[02:33<30:21, Iter=19, Loss=0.693147, Step=19, UsedTime=0:02:33, lr=0.000019]Epoch: 1/4:  8%|█████▊                                                                  |25/309[02:40<30:21, loss=0.693]Epoch: 1/4:  8%|█▌                  |25/309[02:40<30:21, Iter=20, Loss=0.693147, Step=20, UsedTime=0:02:40, lr=0.000020]Epoch: 1/4:  8%|█▋                  |26/309[02:40<30:53, Iter=20, Loss=0.693147, Step=20, UsedTime=0:02:40, lr=0.000020]Epoch: 1/4:  9%|█▋                  |27/309[02:44<26:00, Iter=20, Loss=0.693147, Step=20, UsedTime=0:02:40, lr=0.000020]Epoch: 1/4:  9%|██████▎                                                                 |27/309[02:47<26:00, loss=0.693]Epoch: 1/4:  9%|█▋                  |27/309[02:47<26:00, Iter=21, Loss=0.693147, Step=21, UsedTime=0:02:47, lr=0.000021]Epoch: 1/4:  9%|█▊                  |28/309[02:47<23:22, Iter=21, Loss=0.693147, Step=21, UsedTime=0:02:47, lr=0.000021]Epoch: 1/4:  9%|██████▌                                                                 |28/309[02:54<23:22, loss=0.693]Epoch: 1/4:  9%|█▊                  |28/309[02:54<23:22, Iter=22, Loss=0.693147, Step=22, UsedTime=0:02:54, lr=0.000022]Epoch: 1/4:  9%|█▉                  |29/309[02:54<25:54, Iter=22, Loss=0.693147, Step=22, UsedTime=0:02:54, lr=0.000022]Epoch: 1/4:  9%|██████▊                                                                 |29/309[03:01<25:54, loss=0.693]Epoch: 1/4:  9%|█▉                  |29/309[03:01<25:54, Iter=23, Loss=0.693147, Step=23, UsedTime=0:03:01, lr=0.000023]Epoch: 1/4: 10%|█▉                  |30/309[03:01<27:41, Iter=23, Loss=0.693147, Step=23, UsedTime=0:03:01, lr=0.000023]Epoch: 1/4: 10%|██████▉                                                                 |30/309[03:08<27:41, loss=0.693]Epoch: 1/4: 10%|█▉                  |30/309[03:08<27:41, Iter=24, Loss=0.693147, Step=24, UsedTime=0:03:08, lr=0.000024]Epoch: 1/4: 10%|██                  |31/309[03:08<28:58, Iter=24, Loss=0.693147, Step=24, UsedTime=0:03:08, lr=0.000024]Epoch: 1/4: 10%|███████▏                                                                |31/309[03:15<28:58, loss=0.693]Epoch: 1/4: 10%|██                  |31/309[03:15<28:58, Iter=25, Loss=0.693147, Step=25, UsedTime=0:03:15, lr=0.000025]Epoch: 1/4: 10%|██                  |32/309[03:15<29:43, Iter=25, Loss=0.693147, Step=25, UsedTime=0:03:15, lr=0.000025]Epoch: 1/4: 10%|███████▍                                                                |32/309[03:22<29:43, loss=0.693]Epoch: 1/4: 10%|██                  |32/309[03:22<29:43, Iter=26, Loss=0.693147, Step=26, UsedTime=0:03:22, lr=0.000026]Epoch: 1/4: 11%|██▏                 |33/309[03:22<30:20, Iter=26, Loss=0.693147, Step=26, UsedTime=0:03:22, lr=0.000026]Epoch: 1/4: 11%|███████▋                                                                |33/309[03:29<30:20, loss=0.693]Epoch: 1/4: 11%|██▏                 |33/309[03:29<30:20, Iter=27, Loss=0.693147, Step=27, UsedTime=0:03:29, lr=0.000027]Epoch: 1/4: 11%|██▏                 |34/309[03:29<30:35, Iter=27, Loss=0.693147, Step=27, UsedTime=0:03:29, lr=0.000027]Epoch: 1/4: 11%|██▎                 |35/309[03:32<25:38, Iter=27, Loss=0.693147, Step=27, UsedTime=0:03:29, lr=0.000027]Epoch: 1/4: 11%|████████▏                                                               |35/309[03:35<25:38, loss=0.693]Epoch: 1/4: 11%|██▎                 |35/309[03:35<25:38, Iter=28, Loss=0.693147, Step=28, UsedTime=0:03:35, lr=0.000028]Epoch: 1/4: 12%|██▎                 |36/309[03:35<22:56, Iter=28, Loss=0.693147, Step=28, UsedTime=0:03:35, lr=0.000028]Epoch: 1/4: 12%|████████▍                                                               |36/309[03:42<22:56, loss=0.693]Epoch: 1/4: 12%|██▎                 |36/309[03:42<22:56, Iter=29, Loss=0.693147, Step=29, UsedTime=0:03:42, lr=0.000029]Epoch: 1/4: 12%|██▍                 |37/309[03:42<25:24, Iter=29, Loss=0.693147, Step=29, UsedTime=0:03:42, lr=0.000029]Epoch: 1/4: 12%|████████▌                                                               |37/309[03:49<25:24, loss=0.693]Epoch: 1/4: 12%|██▍                 |37/309[03:49<25:24, Iter=30, Loss=0.693147, Step=30, UsedTime=0:03:49, lr=0.000030]Epoch: 1/4: 12%|██▍                 |38/309[03:49<27:06, Iter=30, Loss=0.693147, Step=30, UsedTime=0:03:49, lr=0.000030]Epoch: 1/4: 12%|████████▊                                                               |38/309[03:56<27:06, loss=0.693]Epoch: 1/4: 12%|██▍                 |38/309[03:56<27:06, Iter=31, Loss=0.693147, Step=31, UsedTime=0:03:56, lr=0.000031]Epoch: 1/4: 13%|██▌                 |39/309[03:56<28:09, Iter=31, Loss=0.693147, Step=31, UsedTime=0:03:56, lr=0.000031]Epoch: 1/4: 13%|█████████                                                               |39/309[04:03<28:09, loss=0.693]Epoch: 1/4: 13%|██▌                 |39/309[04:03<28:09, Iter=32, Loss=0.693147, Step=32, UsedTime=0:04:03, lr=0.000032]Epoch: 1/4: 13%|██▌                 |40/309[04:03<28:53, Iter=32, Loss=0.693147, Step=32, UsedTime=0:04:03, lr=0.000032]Epoch: 1/4: 13%|█████████▎                                                              |40/309[04:10<28:53, loss=0.693]Epoch: 1/4: 13%|██▌                 |40/309[04:10<28:53, Iter=33, Loss=0.693147, Step=33, UsedTime=0:04:10, lr=0.000033]Epoch: 1/4: 13%|██▋                 |41/309[04:10<29:25, Iter=33, Loss=0.693147, Step=33, UsedTime=0:04:10, lr=0.000033]Epoch: 1/4: 13%|█████████▌                                                              |41/309[04:17<29:25, loss=0.693]Epoch: 1/4: 13%|██▋                 |41/309[04:17<29:25, Iter=34, Loss=0.693147, Step=34, UsedTime=0:04:17, lr=0.000034]Epoch: 1/4: 14%|██▋                 |42/309[04:17<29:45, Iter=34, Loss=0.693147, Step=34, UsedTime=0:04:17, lr=0.000034]Epoch: 1/4: 14%|█████████▊                                                              |42/309[04:24<29:45, loss=0.693]Epoch: 1/4: 14%|██▋                 |42/309[04:24<29:45, Iter=35, Loss=0.693147, Step=35, UsedTime=0:04:24, lr=0.000035]Epoch: 1/4: 14%|██▊                 |43/309[04:24<29:55, Iter=35, Loss=0.693147, Step=35, UsedTime=0:04:24, lr=0.000035]Epoch: 1/4: 14%|██▊                 |44/309[04:27<25:07, Iter=35, Loss=0.693147, Step=35, UsedTime=0:04:24, lr=0.000035]Epoch: 1/4: 14%|██████████▎                                                             |44/309[04:31<25:07, loss=0.693]Epoch: 1/4: 14%|██▊                 |44/309[04:31<25:07, Iter=36, Loss=0.693147, Step=36, UsedTime=0:04:31, lr=0.000036]Epoch: 1/4: 15%|██▉                 |45/309[04:31<22:30, Iter=36, Loss=0.693147, Step=36, UsedTime=0:04:31, lr=0.000036]Epoch: 1/4: 15%|██████████▍                                                             |45/309[04:38<22:30, loss=0.693]Epoch: 1/4: 15%|██▉                 |45/309[04:38<22:30, Iter=37, Loss=0.693147, Step=37, UsedTime=0:04:38, lr=0.000037]Epoch: 1/4: 15%|██▉                 |46/309[04:38<24:44, Iter=37, Loss=0.693147, Step=37, UsedTime=0:04:38, lr=0.000037]Epoch: 1/4: 15%|██████████▋                                                             |46/309[04:45<24:44, loss=0.693]Epoch: 1/4: 15%|██▉                 |46/309[04:45<24:44, Iter=38, Loss=0.693147, Step=38, UsedTime=0:04:45, lr=0.000038]Epoch: 1/4: 15%|███                 |47/309[04:45<26:21, Iter=38, Loss=0.693147, Step=38, UsedTime=0:04:45, lr=0.000038]Epoch: 1/4: 15%|██████████▉                                                             |47/309[04:52<26:21, loss=0.693]Epoch: 1/4: 15%|███                 |47/309[04:52<26:21, Iter=39, Loss=0.693147, Step=39, UsedTime=0:04:52, lr=0.000039]Epoch: 1/4: 16%|███                 |48/309[04:52<27:25, Iter=39, Loss=0.693147, Step=39, UsedTime=0:04:52, lr=0.000039]Epoch: 1/4: 16%|███████████▏                                                            |48/309[04:58<27:25, loss=0.693]Epoch: 1/4: 16%|███                 |48/309[04:58<27:25, Iter=40, Loss=0.693147, Step=40, UsedTime=0:04:58, lr=0.000040]Epoch: 1/4: 16%|███▏                |49/309[04:58<28:03, Iter=40, Loss=0.693147, Step=40, UsedTime=0:04:58, lr=0.000040]Epoch: 1/4: 16%|███████████▍                                                            |49/309[05:05<28:03, loss=0.693]Epoch: 1/4: 16%|███▏                |49/309[05:05<28:03, Iter=41, Loss=0.693147, Step=41, UsedTime=0:05:05, lr=0.000041]Epoch: 1/4: 16%|███▏                |50/309[05:05<28:32, Iter=41, Loss=0.693147, Step=41, UsedTime=0:05:05, lr=0.000041]Epoch: 1/4: 16%|███████████▋                                                            |50/309[05:12<28:32, loss=0.693]Epoch: 1/4: 16%|███▏                |50/309[05:12<28:32, Iter=42, Loss=0.693147, Step=42, UsedTime=0:05:12, lr=0.000042]Epoch: 1/4: 17%|███▎                |51/309[05:12<28:55, Iter=42, Loss=0.693147, Step=42, UsedTime=0:05:12, lr=0.000042]Epoch: 1/4: 17%|███████████▉                                                            |51/309[05:19<28:55, loss=0.693]Epoch: 1/4: 17%|███▎                |51/309[05:19<28:55, Iter=43, Loss=0.693147, Step=43, UsedTime=0:05:19, lr=0.000043]Epoch: 1/4: 17%|███▎                |52/309[05:19<28:58, Iter=43, Loss=0.693147, Step=43, UsedTime=0:05:19, lr=0.000043]Epoch: 1/4: 17%|████████████                                                            |52/309[05:26<28:58, loss=0.693]Epoch: 1/4: 17%|███▎                |52/309[05:26<28:58, Iter=44, Loss=0.693147, Step=44, UsedTime=0:05:26, lr=0.000044]Epoch: 1/4: 17%|███▍                |53/309[05:26<29:02, Iter=44, Loss=0.693147, Step=44, UsedTime=0:05:26, lr=0.000044]nohup: ignoring input
Some weights of the model checkpoint at /data2/wangbingchao/database/bert_pretrained/bert-base-uncased were not used when initializing SentenceChoice: ['bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'cls.predictions.decoder.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'cls.predictions.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.key.weight']
- This IS expected if you are initializing SentenceChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SentenceChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of SentenceChoice were not initialized from the model checkpoint at /data2/wangbingchao/database/bert_pretrained/bert-base-uncased and are newly initialized: ['q_lstm.bias_ih_l2', 'c_lstm.1.weight_ih_l0', 'q_lstm.weight_hh_l2_reverse', 'q_lstm.bias_hh_l2', 'c_lstm.0.weight_ih_l1', 'c_lstm.0.weight_ih_l0', 'c_lstm.2.bias_hh_l1_reverse', 'c_lstm.2.bias_hh_l0', 'c_lstm.2.weight_ih_l0_reverse', 'c_lstm.1.bias_ih_l2', 'c_lstm.0.weight_ih_l1_reverse', 'c_lstm.2.bias_hh_l2_reverse', 'q_lstm.bias_hh_l1_reverse', 'q_lstm.bias_hh_l2_reverse', 'attention.q_proj.bias', 'bert.weight', 'c_lstm.0.weight_hh_l1', 'c_lstm.0.bias_hh_l1_reverse', 'c_lstm.2.bias_hh_l1', 'c_lstm.2.bias_hh_l2', 'c_lstm.2.bias_ih_l1', 'c_lstm.1.weight_ih_l1_reverse', 'c_lstm.1.weight_hh_l2', 'c_lstm.2.bias_ih_l2_reverse', 'c_lstm.2.weight_ih_l1', 'attention.q_proj.weight', 'c_lstm.0.bias_hh_l0_reverse', 'q_lstm.bias_ih_l1', 'c_lstm.0.bias_hh_l2', 'c_lstm.1.weight_hh_l0', 'c_lstm.1.bias_hh_l1', 'q_lstm.bias_ih_l2_reverse', 'c_lstm.0.bias_hh_l2_reverse', 'c_lstm.2.weight_hh_l2_reverse', 'attention.k_proj.bias', 'c_lstm.2.bias_ih_l0', 'q_lstm.weight_hh_l1', 'c_lstm.2.weight_ih_l2_reverse', 'c_lstm.1.bias_ih_l1', 'c_lstm.2.weight_hh_l2', 'c_lstm.1.bias_ih_l0_reverse', 'attention.k_proj.weight', 'c_lstm.1.weight_ih_l0_reverse', 'c_lstm.0.bias_ih_l1_reverse', 'q_lstm.weight_ih_l0_reverse', 'q_lstm.weight_ih_l2_reverse', 'c_lstm.0.weight_hh_l0', 'c_lstm.2.weight_ih_l0', 'q_lstm.bias_ih_l0', 'c_lstm.2.weight_ih_l1_reverse', 'c_lstm.0.bias_ih_l0', 'q_lstm.bias_ih_l0_reverse', 'c_lstm.1.bias_hh_l2_reverse', 'q_lstm.bias_hh_l0_reverse', 'c_lstm.1.bias_ih_l0', 'c_lstm.0.weight_hh_l2_reverse', 'c_lstm.1.bias_hh_l1_reverse', 'c_lstm.1.bias_hh_l0_reverse', 'c_lstm.2.bias_ih_l0_reverse', 'c_lstm.2.bias_ih_l2', 'q_lstm.weight_ih_l1', 'c_lstm.1.weight_hh_l1', 'c_lstm.2.weight_ih_l2', 'c_lstm.0.weight_ih_l0_reverse', 'c_lstm.2.weight_hh_l0_reverse', 'q_lstm.bias_hh_l1', 'q_lstm.weight_ih_l1_reverse', 'c_lstm.2.bias_ih_l1_reverse', 'c_lstm.1.weight_ih_l2_reverse', 'c_lstm.1.bias_ih_l2_reverse', 'c_lstm.0.bias_hh_l0', 'c_lstm.0.weight_hh_l0_reverse', 'c_lstm.0.bias_hh_l1', 'c_lstm.0.weight_hh_l1_reverse', 'c_lstm.0.bias_ih_l2', 'attention.out_proj.weight', 'c_lstm.1.weight_hh_l0_reverse', 'c_lstm.2.weight_hh_l1', 'attention.v_proj.bias', 'q_lstm.weight_hh_l2', 'c_lstm.2.bias_hh_l0_reverse', 'c_lstm.1.weight_ih_l1', 'attention.v_proj.weight', 'c_lstm.2.weight_hh_l1_reverse', 'q_lstm.bias_ih_l1_reverse', 'c_lstm.2.weight_hh_l0', 'c_lstm.0.weight_hh_l2', 'liner.weight', 'liner.bias', 'c_lstm.1.bias_hh_l2', 'attention.out_proj.bias', 'q_lstm.weight_hh_l0_reverse', 'c_lstm.0.bias_ih_l1', 'c_lstm.0.weight_ih_l2_reverse', 'c_lstm.1.weight_ih_l2', 'c_lstm.1.bias_ih_l1_reverse', 'c_lstm.0.bias_ih_l0_reverse', 'c_lstm.1.bias_hh_l0', 'q_lstm.weight_ih_l2', 'q_lstm.weight_ih_l0', 'c_lstm.0.bias_ih_l2_reverse', 'q_lstm.weight_hh_l1_reverse', 'q_lstm.bias_hh_l0', 'c_lstm.1.weight_hh_l1_reverse', 'c_lstm.1.weight_hh_l2_reverse', 'q_lstm.weight_hh_l0', 'c_lstm.0.weight_ih_l2']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
--- Logging error ---
Traceback (most recent call last):
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/logging/__init__.py", line 1025, in emit
    msg = self.format(record)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/logging/__init__.py", line 869, in format
    return fmt.format(record)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/logging/__init__.py", line 608, in format
    record.message = record.getMessage()
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/logging/__init__.py", line 369, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/runpy.py", line 183, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/runpy.py", line 109, in _get_module_details
    __import__(pkg_name)
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/data2/maqi/LongTextDatasets/LongTextModels/main/main.py", line 12, in <module>
    train_begin_time=train_begin)
  File "/data2/maqi/LongTextDatasets/LongTextModels/model/trainer.py", line 267, in run_train_epoch
    self.log.info('gradient_accumulation_steps: ',self.hparams.gradient_accumulation_steps//(len(self.hparams.gpu_ids)*self.hparams.per_gpu_batch_size))
Message: 'gradient_accumulation_steps: '
Arguments: (0,)
--- Logging error ---
Traceback (most recent call last):
  File "/data2/maqi/LongTextDatasets/LongTextModels/tools/logger.py", line 23, in emit
    msg = self.format(record)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/logging/__init__.py", line 869, in format
    return fmt.format(record)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/logging/__init__.py", line 608, in format
    record.message = record.getMessage()
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/logging/__init__.py", line 369, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/runpy.py", line 183, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/runpy.py", line 109, in _get_module_details
    __import__(pkg_name)
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/data2/maqi/LongTextDatasets/LongTextModels/main/main.py", line 12, in <module>
    train_begin_time=train_begin)
  File "/data2/maqi/LongTextDatasets/LongTextModels/model/trainer.py", line 267, in run_train_epoch
    self.log.info('gradient_accumulation_steps: ',self.hparams.gradient_accumulation_steps//(len(self.hparams.gpu_ids)*self.hparams.per_gpu_batch_size))
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/logging/__init__.py", line 1378, in info
    self._log(INFO, msg, args, **kwargs)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/data2/maqi/LongTextDatasets/LongTextModels/tools/logger.py", line 29, in emit
    self.handleError(record)
Message: 'gradient_accumulation_steps: '
Arguments: (0,)
创建文件夹...
[22-05-05 13:00:59] --Trainer-- INFO: Load pretrained model from file /data2/wangbingchao/database/bert_pretrained/bert-base-uncased ...
[22-05-05 13:00:59] --Trainer-- INFO: Load pretrained model config finished!!!
[22-05-05 13:00:59] --Trainer-- INFO: Define model...
[22-05-05 13:01:01] --Trainer-- INFO: use cuda to train
[22-05-05 13:01:13] --Trainer-- INFO: Use Multi-GPUs[0, 1, 2]
[22-05-05 13:01:13] --DropDataloader-- INFO: Loading dataset from cached file /data2/maqi/LongTextDatasets/LongTextModels/cache/cache_train_hotpot_dev_fullwiki_v1.json_bert-base-uncased
[22-05-05 13:01:14] --Trainer-- INFO: Define model finished!!!
[22-05-05 13:01:14] --Trainer-- INFO: model config output dir: /data2/maqi/LongTextDatasets/LongTextModels/output/exp_5-5/config.txt
 0%|                                                                                                     |0/309[00:00<?]Epoch: 1/4:  0%|                                                                                         |0/309[00:00<?]/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/modules/rnn.py:662: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1616554827596/work/aten/src/ATen/native/cudnn/RNN.cpp:915.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Epoch: 1/4:  0%|                                                                             |0/309[00:24<?, loss=0.693]Epoch: 1/4:  0%|                           |0/309[00:24<?, Iter=0, Loss=0.693147, Step=0, UsedTime=0:00:24, lr=0.000000]Epoch: 1/4:  0%|                     |1/309[00:24<2:04:04, Iter=0, Loss=0.693147, Step=0, UsedTime=0:00:24, lr=0.000000]Epoch: 1/4:  1%|▏                    |2/309[00:27<1:00:19, Iter=0, Loss=0.693147, Step=0, UsedTime=0:00:24, lr=0.000000]Epoch: 1/4:  1%|▍                                                                      |2/309[00:31<1:00:19, loss=0.693]Epoch: 1/4:  1%|▏                    |2/309[00:31<1:00:19, Iter=1, Loss=0.693147, Step=1, UsedTime=0:00:31, lr=0.000001]Epoch: 1/4:  1%|▏                      |3/309[00:31<41:21, Iter=1, Loss=0.693147, Step=1, UsedTime=0:00:31, lr=0.000001]Epoch: 1/4:  1%|▋                                                                        |3/309[00:37<41:21, loss=0.693]Epoch: 1/4:  1%|▏                      |3/309[00:37<41:21, Iter=2, Loss=0.693147, Step=2, UsedTime=0:00:37, lr=0.000002]Epoch: 1/4:  1%|▎                      |4/309[00:37<38:37, Iter=2, Loss=0.693147, Step=2, UsedTime=0:00:37, lr=0.000002]Epoch: 1/4:  2%|▎                      |5/309[00:40<30:21, Iter=2, Loss=0.693147, Step=2, UsedTime=0:00:37, lr=0.000002]Epoch: 1/4:  2%|█▏                                                                       |5/309[00:44<30:21, loss=0.693]Epoch: 1/4:  2%|▎                      |5/309[00:44<30:21, Iter=3, Loss=0.693147, Step=3, UsedTime=0:00:44, lr=0.000003]Epoch: 1/4:  2%|▍                      |6/309[00:44<26:21, Iter=3, Loss=0.693147, Step=3, UsedTime=0:00:44, lr=0.000003]Epoch: 1/4:  2%|█▍                                                                       |6/309[00:51<26:21, loss=0.693]Epoch: 1/4:  2%|▍                      |6/309[00:51<26:21, Iter=4, Loss=0.693147, Step=4, UsedTime=0:00:51, lr=0.000004]Epoch: 1/4:  2%|▌                      |7/309[00:51<28:47, Iter=4, Loss=0.693147, Step=4, UsedTime=0:00:51, lr=0.000004]Epoch: 1/4:  2%|█▋                                                                       |7/309[00:58<28:47, loss=0.693]Epoch: 1/4:  2%|▌                      |7/309[00:58<28:47, Iter=5, Loss=0.693147, Step=5, UsedTime=0:00:58, lr=0.000005]Epoch: 1/4:  3%|▌                      |8/309[00:58<30:30, Iter=5, Loss=0.693147, Step=5, UsedTime=0:00:58, lr=0.000005]Epoch: 1/4:  3%|▋                      |9/309[01:01<25:52, Iter=5, Loss=0.693147, Step=5, UsedTime=0:00:58, lr=0.000005]Epoch: 1/4:  3%|██▏                                                                      |9/309[01:05<25:52, loss=0.693]Epoch: 1/4:  3%|▋                      |9/309[01:05<25:52, Iter=6, Loss=0.693147, Step=6, UsedTime=0:01:05, lr=0.000006]Epoch: 1/4:  3%|▋                     |10/309[01:05<23:32, Iter=6, Loss=0.693147, Step=6, UsedTime=0:01:05, lr=0.000006]Epoch: 1/4:  3%|██▎                                                                     |10/309[01:11<23:32, loss=0.693]Epoch: 1/4:  3%|▋                     |10/309[01:11<23:32, Iter=7, Loss=0.693147, Step=7, UsedTime=0:01:11, lr=0.000007]Epoch: 1/4:  4%|▊                     |11/309[01:11<26:34, Iter=7, Loss=0.693147, Step=7, UsedTime=0:01:11, lr=0.000007]Epoch: 1/4:  4%|██▌                                                                     |11/309[01:18<26:34, loss=0.693]Epoch: 1/4:  4%|▊                     |11/309[01:18<26:34, Iter=8, Loss=0.693147, Step=8, UsedTime=0:01:18, lr=0.000008]Epoch: 1/4:  4%|▊                     |12/309[01:18<28:49, Iter=8, Loss=0.693147, Step=8, UsedTime=0:01:18, lr=0.000008]Epoch: 1/4:  4%|██▊                                                                     |12/309[01:25<28:49, loss=0.693]Epoch: 1/4:  4%|▊                     |12/309[01:25<28:49, Iter=9, Loss=0.693147, Step=9, UsedTime=0:01:25, lr=0.000009]Epoch: 1/4:  4%|▉                     |13/309[01:25<30:16, Iter=9, Loss=0.693147, Step=9, UsedTime=0:01:25, lr=0.000009]Epoch: 1/4:  5%|▉                     |14/309[01:28<25:42, Iter=9, Loss=0.693147, Step=9, UsedTime=0:01:25, lr=0.000009]Epoch: 1/4:  5%|███▎                                                                    |14/309[01:32<25:42, loss=0.693]Epoch: 1/4:  5%|▉                   |14/309[01:32<25:42, Iter=10, Loss=0.693147, Step=10, UsedTime=0:01:32, lr=0.000010]Epoch: 1/4:  5%|▉                   |15/309[01:32<23:18, Iter=10, Loss=0.693147, Step=10, UsedTime=0:01:32, lr=0.000010]Epoch: 1/4:  5%|███▍                                                                    |15/309[01:39<23:18, loss=0.693]Epoch: 1/4:  5%|▉                   |15/309[01:39<23:18, Iter=11, Loss=0.693147, Step=11, UsedTime=0:01:39, lr=0.000011]Epoch: 1/4:  5%|█                   |16/309[01:39<26:23, Iter=11, Loss=0.693147, Step=11, UsedTime=0:01:39, lr=0.000011]Epoch: 1/4:  5%|███▋                                                                    |16/309[01:46<26:23, loss=0.693]Epoch: 1/4:  5%|█                   |16/309[01:46<26:23, Iter=12, Loss=0.693147, Step=12, UsedTime=0:01:46, lr=0.000012]Epoch: 1/4:  6%|█                   |17/309[01:46<28:26, Iter=12, Loss=0.693147, Step=12, UsedTime=0:01:46, lr=0.000012]nohup: ignoring input
Some weights of the model checkpoint at /data2/wangbingchao/database/bert_pretrained/bert-base-uncased were not used when initializing SentenceChoice: ['bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'cls.predictions.decoder.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.4.output.dense.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.value.weight']
- This IS expected if you are initializing SentenceChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SentenceChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of SentenceChoice were not initialized from the model checkpoint at /data2/wangbingchao/database/bert_pretrained/bert-base-uncased and are newly initialized: ['c_lstm.1.bias_hh_l1', 'c_lstm.0.weight_hh_l0', 'c_lstm.1.weight_hh_l0_reverse', 'q_lstm.bias_hh_l1', 'c_lstm.2.bias_hh_l2_reverse', 'c_lstm.2.bias_hh_l1', 'c_lstm.0.bias_hh_l2_reverse', 'q_lstm.bias_ih_l2', 'q_lstm.bias_ih_l2_reverse', 'q_lstm.bias_hh_l1_reverse', 'c_lstm.0.bias_hh_l2', 'attention.q_proj.weight', 'q_lstm.weight_ih_l2', 'c_lstm.1.weight_ih_l2_reverse', 'q_lstm.weight_ih_l1_reverse', 'c_lstm.2.weight_ih_l1_reverse', 'c_lstm.0.weight_ih_l1_reverse', 'c_lstm.2.weight_hh_l1', 'c_lstm.2.bias_hh_l0_reverse', 'attention.v_proj.bias', 'c_lstm.1.weight_hh_l2', 'q_lstm.weight_hh_l2', 'c_lstm.0.weight_ih_l2_reverse', 'c_lstm.1.weight_ih_l2', 'c_lstm.0.weight_hh_l1_reverse', 'c_lstm.2.weight_ih_l0', 'q_lstm.weight_hh_l2_reverse', 'c_lstm.1.bias_ih_l2', 'c_lstm.0.bias_hh_l1_reverse', 'c_lstm.1.bias_hh_l2_reverse', 'c_lstm.0.bias_ih_l2', 'q_lstm.bias_ih_l0_reverse', 'c_lstm.0.bias_ih_l1', 'c_lstm.2.bias_ih_l1', 'c_lstm.2.bias_ih_l2_reverse', 'c_lstm.0.bias_ih_l0', 'bert.weight', 'c_lstm.0.bias_hh_l0', 'c_lstm.0.weight_ih_l1', 'c_lstm.1.bias_hh_l1_reverse', 'q_lstm.bias_ih_l1_reverse', 'c_lstm.0.bias_ih_l2_reverse', 'c_lstm.1.bias_ih_l2_reverse', 'attention.k_proj.bias', 'q_lstm.weight_hh_l1_reverse', 'c_lstm.2.weight_ih_l0_reverse', 'c_lstm.1.weight_ih_l1_reverse', 'c_lstm.0.bias_ih_l1_reverse', 'c_lstm.2.weight_hh_l2', 'liner.bias', 'q_lstm.bias_hh_l0_reverse', 'c_lstm.0.bias_ih_l0_reverse', 'c_lstm.1.weight_hh_l2_reverse', 'c_lstm.1.weight_hh_l0', 'c_lstm.2.bias_hh_l0', 'c_lstm.2.bias_ih_l0_reverse', 'c_lstm.0.weight_ih_l2', 'q_lstm.bias_hh_l2', 'c_lstm.2.bias_hh_l2', 'liner.weight', 'c_lstm.0.bias_hh_l0_reverse', 'c_lstm.2.weight_hh_l0', 'c_lstm.2.bias_ih_l0', 'c_lstm.2.bias_ih_l1_reverse', 'c_lstm.1.weight_ih_l0_reverse', 'c_lstm.0.bias_hh_l1', 'c_lstm.1.bias_ih_l1_reverse', 'c_lstm.2.weight_ih_l1', 'q_lstm.weight_ih_l1', 'q_lstm.weight_ih_l0_reverse', 'attention.k_proj.weight', 'c_lstm.1.bias_ih_l0', 'c_lstm.0.weight_hh_l2_reverse', 'c_lstm.2.weight_ih_l2', 'q_lstm.bias_ih_l0', 'q_lstm.weight_hh_l1', 'c_lstm.2.weight_ih_l2_reverse', 'attention.q_proj.bias', 'c_lstm.0.weight_ih_l0', 'q_lstm.weight_ih_l2_reverse', 'q_lstm.bias_hh_l2_reverse', 'c_lstm.0.weight_hh_l2', 'c_lstm.0.weight_ih_l0_reverse', 'q_lstm.weight_ih_l0', 'attention.out_proj.weight', 'c_lstm.2.bias_hh_l1_reverse', 'q_lstm.bias_ih_l1', 'c_lstm.1.bias_hh_l0', 'c_lstm.1.weight_ih_l0', 'c_lstm.2.bias_ih_l2', 'c_lstm.1.bias_ih_l0_reverse', 'q_lstm.weight_hh_l0_reverse', 'attention.out_proj.bias', 'c_lstm.0.weight_hh_l1', 'attention.v_proj.weight', 'c_lstm.0.weight_hh_l0_reverse', 'c_lstm.1.bias_ih_l1', 'q_lstm.bias_hh_l0', 'q_lstm.weight_hh_l0', 'c_lstm.1.weight_ih_l1', 'c_lstm.2.weight_hh_l1_reverse', 'c_lstm.1.weight_hh_l1_reverse', 'c_lstm.1.bias_hh_l0_reverse', 'c_lstm.1.bias_hh_l2', 'c_lstm.1.weight_hh_l1', 'c_lstm.2.weight_hh_l2_reverse', 'c_lstm.2.weight_hh_l0_reverse']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/runpy.py", line 183, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/runpy.py", line 109, in _get_module_details
    __import__(pkg_name)
  File "/data2/maqi/LongTextDatasets/LongTextModels/main/main.py", line 12, in <module>
    train_begin_time=train_begin)
  File "/data2/maqi/LongTextDatasets/LongTextModels/model/trainer.py", line 267, in run_train_epoch
    self.log.info('gradient_accumulation_steps: '+self.hparams.gradient_accumulation_steps//(len(self.hparams.gpu_ids)*self.hparams.per_gpu_batch_size))
TypeError: can only concatenate str (not "int") to str
创建文件夹...
[22-05-05 13:03:32] --Trainer-- INFO: Load pretrained model from file /data2/wangbingchao/database/bert_pretrained/bert-base-uncased ...
[22-05-05 13:03:32] --Trainer-- INFO: Load pretrained model config finished!!!
[22-05-05 13:03:32] --Trainer-- INFO: Define model...
[22-05-05 13:03:34] --Trainer-- INFO: use cuda to train
[22-05-05 13:03:50] --Trainer-- INFO: Use Multi-GPUs[0, 1, 2]
[22-05-05 13:03:50] --DropDataloader-- INFO: Loading dataset from cached file /data2/maqi/LongTextDatasets/LongTextModels/cache/cache_train_hotpot_dev_fullwiki_v1.json_bert-base-uncased
[22-05-05 13:03:51] --Trainer-- INFO: Define model finished!!!
[22-05-05 13:03:51] --Trainer-- INFO: model config output dir: /data2/maqi/LongTextDatasets/LongTextModels/output/exp_5-5/config.txt
nohup: ignoring input
Some weights of the model checkpoint at /data2/wangbingchao/database/bert_pretrained/bert-base-uncased were not used when initializing SentenceChoice: ['bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'cls.seq_relationship.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'cls.predictions.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing SentenceChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SentenceChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of SentenceChoice were not initialized from the model checkpoint at /data2/wangbingchao/database/bert_pretrained/bert-base-uncased and are newly initialized: ['c_lstm.2.bias_hh_l1', 'q_lstm.weight_hh_l1_reverse', 'c_lstm.2.bias_ih_l0', 'attention.v_proj.weight', 'q_lstm.weight_hh_l0', 'c_lstm.0.weight_ih_l1', 'c_lstm.2.bias_ih_l1_reverse', 'c_lstm.1.weight_ih_l2', 'q_lstm.bias_hh_l1_reverse', 'q_lstm.weight_ih_l0', 'c_lstm.0.bias_ih_l0', 'c_lstm.2.bias_ih_l1', 'attention.q_proj.weight', 'c_lstm.1.weight_hh_l0_reverse', 'liner.dense.weight', 'c_lstm.2.weight_ih_l0', 'c_lstm.0.bias_ih_l1_reverse', 'q_lstm.weight_hh_l0_reverse', 'q_lstm.weight_ih_l1', 'q_lstm.bias_ih_l1_reverse', 'c_lstm.1.bias_hh_l1_reverse', 'q_lstm.weight_hh_l2_reverse', 'c_lstm.1.bias_hh_l2', 'q_lstm.bias_ih_l1', 'attention.out_proj.bias', 'c_lstm.0.weight_ih_l0', 'q_lstm.bias_hh_l0_reverse', 'c_lstm.0.bias_hh_l1_reverse', 'c_lstm.1.bias_ih_l1', 'c_lstm.1.weight_ih_l1_reverse', 'c_lstm.1.bias_hh_l1', 'c_lstm.0.weight_ih_l2_reverse', 'q_lstm.bias_ih_l0', 'c_lstm.1.weight_ih_l1', 'c_lstm.1.bias_ih_l1_reverse', 'c_lstm.1.weight_hh_l0', 'c_lstm.2.weight_ih_l1_reverse', 'q_lstm.bias_ih_l0_reverse', 'q_lstm.weight_hh_l1', 'q_lstm.weight_hh_l2', 'c_lstm.2.bias_hh_l0_reverse', 'c_lstm.1.bias_ih_l2_reverse', 'c_lstm.2.weight_hh_l2_reverse', 'attention.v_proj.bias', 'attention.out_proj.weight', 'attention.k_proj.bias', 'c_lstm.1.bias_hh_l2_reverse', 'c_lstm.0.weight_hh_l1_reverse', 'c_lstm.1.bias_ih_l2', 'c_lstm.2.bias_hh_l0', 'c_lstm.0.bias_ih_l2_reverse', 'c_lstm.1.weight_ih_l0', 'c_lstm.0.bias_ih_l2', 'c_lstm.2.bias_ih_l2', 'bert.weight', 'q_lstm.bias_ih_l2', 'c_lstm.1.weight_hh_l1', 'c_lstm.2.weight_ih_l0_reverse', 'attention.q_proj.bias', 'c_lstm.1.bias_ih_l0', 'c_lstm.1.weight_hh_l2_reverse', 'c_lstm.2.weight_hh_l0', 'q_lstm.bias_hh_l0', 'c_lstm.0.weight_hh_l0_reverse', 'c_lstm.2.weight_ih_l1', 'c_lstm.1.weight_hh_l1_reverse', 'c_lstm.0.bias_hh_l0', 'c_lstm.2.weight_hh_l1', 'c_lstm.0.weight_ih_l1_reverse', 'c_lstm.0.bias_ih_l1', 'c_lstm.0.weight_ih_l0_reverse', 'liner.affine.weight', 'c_lstm.1.weight_ih_l0_reverse', 'c_lstm.2.bias_hh_l2_reverse', 'c_lstm.2.weight_ih_l2_reverse', 'q_lstm.weight_ih_l1_reverse', 'c_lstm.0.bias_ih_l0_reverse', 'c_lstm.0.weight_ih_l2', 'q_lstm.weight_ih_l2', 'liner.affine.bias', 'c_lstm.0.bias_hh_l0_reverse', 'c_lstm.2.weight_ih_l2', 'q_lstm.bias_hh_l2', 'c_lstm.2.bias_ih_l2_reverse', 'q_lstm.bias_hh_l2_reverse', 'c_lstm.0.weight_hh_l1', 'c_lstm.2.bias_ih_l0_reverse', 'c_lstm.0.bias_hh_l2', 'c_lstm.0.weight_hh_l2_reverse', 'c_lstm.2.weight_hh_l2', 'c_lstm.1.weight_ih_l2_reverse', 'c_lstm.1.weight_hh_l2', 'c_lstm.0.bias_hh_l1', 'q_lstm.bias_hh_l1', 'c_lstm.0.weight_hh_l2', 'c_lstm.2.weight_hh_l0_reverse', 'liner.LayerNorm.gamma', 'q_lstm.weight_ih_l2_reverse', 'c_lstm.1.bias_ih_l0_reverse', 'attention.k_proj.weight', 'c_lstm.1.bias_hh_l0', 'c_lstm.2.weight_hh_l1_reverse', 'c_lstm.0.weight_hh_l0', 'liner.LayerNorm.beta', 'c_lstm.2.bias_hh_l1_reverse', 'q_lstm.bias_ih_l2_reverse', 'liner.dense.bias', 'c_lstm.2.bias_hh_l2', 'c_lstm.1.bias_hh_l0_reverse', 'c_lstm.0.bias_hh_l2_reverse', 'q_lstm.weight_ih_l0_reverse']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
创建文件夹...
[22-05-05 15:39:38] --Trainer-- INFO: Load pretrained model from file /data2/wangbingchao/database/bert_pretrained/bert-base-uncased ...
[22-05-05 15:39:38] --Trainer-- INFO: Load pretrained model config finished!!!
[22-05-05 15:39:38] --Trainer-- INFO: Define model...
[22-05-05 15:39:40] --Trainer-- INFO: use cuda to train
[22-05-05 15:39:44] --Trainer-- INFO: Use Multi-GPUs[0, 1, 2]
[22-05-05 15:39:44] --DropDataloader-- INFO: Loading dataset from cached file /data2/maqi/LongTextDatasets/LongTextModels/cache/cache_train_hotpot_dev_fullwiki_v1.json_bert-base-uncased
[22-05-05 15:39:45] --Trainer-- INFO: Define model finished!!!
[22-05-05 15:39:45] --Trainer-- INFO: model config output dir: /data2/maqi/LongTextDatasets/LongTextModels/output/exp_5_5/config.txt
 0%|                                                                                                     |0/309[00:00<?]Epoch: 1/4:  0%|                                                                                         |0/309[00:00<?]/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/modules/rnn.py:662: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1616554827596/work/aten/src/ATen/native/cudnn/RNN.cpp:915.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Epoch: 1/4:  0%|                                                                             |0/309[00:11<?, loss=0.693]Epoch: 1/4:  0%|                           |0/309[00:11<?, Iter=0, Loss=0.693147, Step=0, UsedTime=0:00:11, lr=0.000000]Epoch: 1/4:  0%|                       |1/309[00:11<58:33, Iter=0, Loss=0.693147, Step=0, UsedTime=0:00:11, lr=0.000000]/data2/maqi/LongTextDatasets/LongTextModels/model/optimization.py:132: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1616554827596/work/torch/csrc/utils/python_arg_parser.cpp:1005.)
  next_m.mul_(beta1).add_(1 - beta1, grad)
Epoch: 1/4:  1%|▏                      |2/309[00:14<34:39, Iter=0, Loss=0.693147, Step=0, UsedTime=0:00:11, lr=0.000000]Epoch: 1/4:  1%|▍                                                                        |2/309[00:18<34:39, loss=0.693]Epoch: 1/4:  1%|▏                      |2/309[00:18<34:39, Iter=1, Loss=0.693147, Step=1, UsedTime=0:00:18, lr=0.000809]Epoch: 1/4:  1%|▏                      |3/309[00:18<27:36, Iter=1, Loss=0.693147, Step=1, UsedTime=0:00:18, lr=0.000809]Epoch: 1/4:  1%|▋                                                                        |3/309[00:26<27:36, loss=0.693]Epoch: 1/4:  1%|▏                      |3/309[00:26<27:36, Iter=2, Loss=0.693147, Step=2, UsedTime=0:00:26, lr=0.001618]Epoch: 1/4:  1%|▎                      |4/309[00:26<31:24, Iter=2, Loss=0.693147, Step=2, UsedTime=0:00:26, lr=0.001618]Epoch: 1/4:  2%|▎                      |5/309[00:29<26:35, Iter=2, Loss=0.693147, Step=2, UsedTime=0:00:26, lr=0.001618]Epoch: 1/4:  2%|█▏                                                                       |5/309[00:33<26:35, loss=0.693]Epoch: 1/4:  2%|▎                      |5/309[00:33<26:35, Iter=3, Loss=0.693147, Step=3, UsedTime=0:00:33, lr=0.002427]Epoch: 1/4:  2%|▍                      |6/309[00:33<23:53, Iter=3, Loss=0.693147, Step=3, UsedTime=0:00:33, lr=0.002427]Epoch: 1/4:  2%|█▍                                                                       |6/309[00:40<23:53, loss=0.693]Epoch: 1/4:  2%|▍                      |6/309[00:40<23:53, Iter=4, Loss=0.693147, Step=4, UsedTime=0:00:40, lr=0.003236]Epoch: 1/4:  2%|▌                      |7/309[00:40<28:04, Iter=4, Loss=0.693147, Step=4, UsedTime=0:00:40, lr=0.003236]Epoch: 1/4:  2%|█▋                                                                       |7/309[00:48<28:04, loss=0.693]Epoch: 1/4:  2%|▌                      |7/309[00:48<28:04, Iter=5, Loss=0.693147, Step=5, UsedTime=0:00:48, lr=0.004045]Epoch: 1/4:  3%|▌                      |8/309[00:48<30:53, Iter=5, Loss=0.693147, Step=5, UsedTime=0:00:48, lr=0.004045]Epoch: 1/4:  3%|▋                      |9/309[00:51<26:42, Iter=5, Loss=0.693147, Step=5, UsedTime=0:00:48, lr=0.004045]Epoch: 1/4:  3%|██▏                                                                      |9/309[00:55<26:42, loss=0.693]Epoch: 1/4:  3%|▋                      |9/309[00:55<26:42, Iter=6, Loss=0.693147, Step=6, UsedTime=0:00:55, lr=0.004854]Epoch: 1/4:  3%|▋                     |10/309[00:55<24:13, Iter=6, Loss=0.693147, Step=6, UsedTime=0:00:55, lr=0.004854]Epoch: 1/4:  3%|██▎                                                                     |10/309[01:02<24:13, loss=0.693]Epoch: 1/4:  3%|▋                     |10/309[01:02<24:13, Iter=7, Loss=0.693147, Step=7, UsedTime=0:01:02, lr=0.005663]Epoch: 1/4:  4%|▊                     |11/309[01:02<27:56, Iter=7, Loss=0.693147, Step=7, UsedTime=0:01:02, lr=0.005663]Epoch: 1/4:  4%|██▌                                                                     |11/309[01:10<27:56, loss=0.693]Epoch: 1/4:  4%|▊                     |11/309[01:10<27:56, Iter=8, Loss=0.693146, Step=8, UsedTime=0:01:10, lr=0.006472]Epoch: 1/4:  4%|▊                     |12/309[01:10<30:25, Iter=8, Loss=0.693146, Step=8, UsedTime=0:01:10, lr=0.006472]Epoch: 1/4:  4%|██▊                                                                     |12/309[01:17<30:25, loss=0.693]Epoch: 1/4:  4%|▊                     |12/309[01:17<30:25, Iter=9, Loss=0.693146, Step=9, UsedTime=0:01:17, lr=0.007282]Epoch: 1/4:  4%|▉                     |13/309[01:17<32:07, Iter=9, Loss=0.693146, Step=9, UsedTime=0:01:17, lr=0.007282]Epoch: 1/4:  5%|▉                     |14/309[01:21<27:39, Iter=9, Loss=0.693146, Step=9, UsedTime=0:01:17, lr=0.007282]Epoch: 1/4:  5%|███▎                                                                    |14/309[01:24<27:39, loss=0.693]Epoch: 1/4:  5%|▉                   |14/309[01:24<27:39, Iter=10, Loss=0.693144, Step=10, UsedTime=0:01:24, lr=0.008091]Epoch: 1/4:  5%|▉                   |15/309[01:24<24:49, Iter=10, Loss=0.693144, Step=10, UsedTime=0:01:24, lr=0.008091]Epoch: 1/4:  5%|███▍                                                                    |15/309[01:32<24:49, loss=0.693]Epoch: 1/4:  5%|▉                   |15/309[01:32<24:49, Iter=11, Loss=0.693141, Step=11, UsedTime=0:01:32, lr=0.008900]Epoch: 1/4:  5%|█                   |16/309[01:32<28:09, Iter=11, Loss=0.693141, Step=11, UsedTime=0:01:32, lr=0.008900]Epoch: 1/4:  5%|███▋                                                                    |16/309[01:39<28:09, loss=0.693]Epoch: 1/4:  5%|█                   |16/309[01:39<28:09, Iter=12, Loss=0.693134, Step=12, UsedTime=0:01:39, lr=0.009709]Epoch: 1/4:  6%|█                   |17/309[01:39<30:25, Iter=12, Loss=0.693134, Step=12, UsedTime=0:01:39, lr=0.009709]Epoch: 1/4:  6%|███▉                                                                    |17/309[01:47<30:25, loss=0.693]Epoch: 1/4:  6%|█                   |17/309[01:47<30:25, Iter=13, Loss=0.693110, Step=13, UsedTime=0:01:47, lr=0.010518]Epoch: 1/4:  6%|█▏                  |18/309[01:47<31:59, Iter=13, Loss=0.693110, Step=13, UsedTime=0:01:47, lr=0.010518]Epoch: 1/4:  6%|████▏                                                                   |18/309[01:54<31:59, loss=0.693]Epoch: 1/4:  6%|█▏                  |18/309[01:54<31:59, Iter=14, Loss=0.693134, Step=14, UsedTime=0:01:54, lr=0.011327]Epoch: 1/4:  6%|█▏                  |19/309[01:54<32:57, Iter=14, Loss=0.693134, Step=14, UsedTime=0:01:54, lr=0.011327]Epoch: 1/4:  6%|█▎                  |20/309[01:57<28:10, Iter=14, Loss=0.693134, Step=14, UsedTime=0:01:54, lr=0.011327]Epoch: 1/4:  6%|████▋                                                                   |20/309[02:01<28:10, loss=0.693]Epoch: 1/4:  6%|█▎                  |20/309[02:01<28:10, Iter=15, Loss=0.693102, Step=15, UsedTime=0:02:01, lr=0.012136]Epoch: 1/4:  7%|█▎                  |21/309[02:01<25:03, Iter=15, Loss=0.693102, Step=15, UsedTime=0:02:01, lr=0.012136]Epoch: 1/4:  7%|████▉                                                                   |21/309[02:09<25:03, loss=0.693]Epoch: 1/4:  7%|█▎                  |21/309[02:09<25:03, Iter=16, Loss=0.693047, Step=16, UsedTime=0:02:09, lr=0.012945]Epoch: 1/4:  7%|█▍                  |22/309[02:09<28:06, Iter=16, Loss=0.693047, Step=16, UsedTime=0:02:09, lr=0.012945]Epoch: 1/4:  7%|█████▏                                                                  |22/309[02:16<28:06, loss=0.693]Epoch: 1/4:  7%|█▍                  |22/309[02:16<28:06, Iter=17, Loss=0.692508, Step=17, UsedTime=0:02:16, lr=0.013754]Epoch: 1/4:  7%|█▍                  |23/309[02:16<30:07, Iter=17, Loss=0.692508, Step=17, UsedTime=0:02:16, lr=0.013754]Epoch: 1/4:  7%|█████▎                                                                  |23/309[02:23<30:07, loss=0.687]Epoch: 1/4:  7%|█▍                  |23/309[02:23<30:07, Iter=18, Loss=0.687227, Step=18, UsedTime=0:02:23, lr=0.014563]Epoch: 1/4:  8%|█▌                  |24/309[02:23<31:33, Iter=18, Loss=0.687227, Step=18, UsedTime=0:02:23, lr=0.014563]Epoch: 1/4:  8%|█████▌                                                                  |24/309[02:31<31:33, loss=0.351]Epoch: 1/4:  8%|█▌                  |24/309[02:31<31:33, Iter=19, Loss=0.350809, Step=19, UsedTime=0:02:31, lr=0.015372]Epoch: 1/4:  8%|█▌                  |25/309[02:31<32:24, Iter=19, Loss=0.350809, Step=19, UsedTime=0:02:31, lr=0.015372]Epoch: 1/4:  8%|█████▊                                                                  |25/309[02:38<32:24, loss=0.552]Epoch: 1/4:  8%|█▌                  |25/309[02:38<32:24, Iter=20, Loss=0.551578, Step=20, UsedTime=0:02:38, lr=0.016181]Epoch: 1/4:  8%|█▋                  |26/309[02:38<33:01, Iter=20, Loss=0.551578, Step=20, UsedTime=0:02:38, lr=0.016181]Epoch: 1/4:  9%|█▋                  |27/309[02:42<28:07, Iter=20, Loss=0.551578, Step=20, UsedTime=0:02:38, lr=0.016181]Epoch: 1/4:  9%|██████▎                                                                 |27/309[02:45<28:07, loss=0.293]Epoch: 1/4:  9%|█▋                  |27/309[02:45<28:07, Iter=21, Loss=0.293093, Step=21, UsedTime=0:02:45, lr=0.016990]Epoch: 1/4:  9%|█▊                  |28/309[02:45<24:57, Iter=21, Loss=0.293093, Step=21, UsedTime=0:02:45, lr=0.016990]Epoch: 1/4:  9%|██████▌                                                                  |28/309[02:53<24:57, loss=8.75]Epoch: 1/4:  9%|█▊                  |28/309[02:53<24:57, Iter=22, Loss=8.754097, Step=22, UsedTime=0:02:53, lr=0.017799]Epoch: 1/4:  9%|█▉                  |29/309[02:53<27:53, Iter=22, Loss=8.754097, Step=22, UsedTime=0:02:53, lr=0.017799]Epoch: 1/4:  9%|██████▊                                                                 |29/309[03:00<27:53, loss=0.716]Epoch: 1/4:  9%|█▉                  |29/309[03:00<27:53, Iter=23, Loss=0.716035, Step=23, UsedTime=0:03:00, lr=0.018608]Epoch: 1/4: 10%|█▉                  |30/309[03:00<29:52, Iter=23, Loss=0.716035, Step=23, UsedTime=0:03:00, lr=0.018608]Epoch: 1/4: 10%|██████▉                                                                 |30/309[03:08<29:52, loss=0.712]Epoch: 1/4: 10%|█▉                  |30/309[03:08<29:52, Iter=24, Loss=0.711998, Step=24, UsedTime=0:03:08, lr=0.019417]Epoch: 1/4: 10%|██                  |31/309[03:08<31:16, Iter=24, Loss=0.711998, Step=24, UsedTime=0:03:08, lr=0.019417]Epoch: 1/4: 10%|███████▍                                                                  |31/309[03:15<31:16, loss=nan]Epoch: 1/4: 10%|██▌                      |31/309[03:15<31:16, Iter=25, Loss=nan, Step=25, UsedTime=0:03:15, lr=0.020227]Epoch: 1/4: 10%|██▌                      |32/309[03:15<31:57, Iter=25, Loss=nan, Step=25, UsedTime=0:03:15, lr=0.020227]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 10%|███████▋                                                                  |32/309[03:23<31:57, loss=nan]Epoch: 1/4: 10%|██▌                      |32/309[03:23<31:57, Iter=26, Loss=nan, Step=26, UsedTime=0:03:23, lr=0.021036]Epoch: 1/4: 11%|██▋                      |33/309[03:23<32:21, Iter=26, Loss=nan, Step=26, UsedTime=0:03:23, lr=0.021036]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 11%|███████▉                                                                  |33/309[03:30<32:21, loss=nan]Epoch: 1/4: 11%|██▋                      |33/309[03:30<32:21, Iter=27, Loss=nan, Step=27, UsedTime=0:03:30, lr=0.021845]Epoch: 1/4: 11%|██▊                      |34/309[03:30<32:36, Iter=27, Loss=nan, Step=27, UsedTime=0:03:30, lr=0.021845]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 11%|██▊                      |35/309[03:33<27:35, Iter=27, Loss=nan, Step=27, UsedTime=0:03:30, lr=0.021845]Epoch: 1/4: 11%|████████▍                                                                 |35/309[03:37<27:35, loss=nan]Epoch: 1/4: 11%|██▊                      |35/309[03:37<27:35, Iter=28, Loss=nan, Step=28, UsedTime=0:03:37, lr=0.022654]Epoch: 1/4: 12%|██▉                      |36/309[03:37<24:22, Iter=28, Loss=nan, Step=28, UsedTime=0:03:37, lr=0.022654]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 12%|████████▌                                                                 |36/309[03:44<24:22, loss=nan]Epoch: 1/4: 12%|██▉                      |36/309[03:44<24:22, Iter=29, Loss=nan, Step=29, UsedTime=0:03:44, lr=0.023463]Epoch: 1/4: 12%|██▉                      |37/309[03:44<26:57, Iter=29, Loss=nan, Step=29, UsedTime=0:03:44, lr=0.023463]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 12%|████████▊                                                                 |37/309[03:52<26:57, loss=nan]Epoch: 1/4: 12%|██▉                      |37/309[03:52<26:57, Iter=30, Loss=nan, Step=30, UsedTime=0:03:52, lr=0.024272]Epoch: 1/4: 12%|███                      |38/309[03:52<28:42, Iter=30, Loss=nan, Step=30, UsedTime=0:03:52, lr=0.024272]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 12%|█████████                                                                 |38/309[03:59<28:42, loss=nan]Epoch: 1/4: 12%|███                      |38/309[03:59<28:42, Iter=31, Loss=nan, Step=31, UsedTime=0:03:59, lr=0.025081]Epoch: 1/4: 13%|███▏                     |39/309[03:59<29:55, Iter=31, Loss=nan, Step=31, UsedTime=0:03:59, lr=0.025081]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 13%|█████████▎                                                                |39/309[04:06<29:55, loss=nan]Epoch: 1/4: 13%|███▏                     |39/309[04:06<29:55, Iter=32, Loss=nan, Step=32, UsedTime=0:04:06, lr=0.025890]Epoch: 1/4: 13%|███▏                     |40/309[04:06<30:40, Iter=32, Loss=nan, Step=32, UsedTime=0:04:06, lr=0.025890]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 13%|█████████▌                                                                |40/309[04:14<30:40, loss=nan]Epoch: 1/4: 13%|███▏                     |40/309[04:14<30:40, Iter=33, Loss=nan, Step=33, UsedTime=0:04:14, lr=0.026699]Epoch: 1/4: 13%|███▎                     |41/309[04:14<31:12, Iter=33, Loss=nan, Step=33, UsedTime=0:04:14, lr=0.026699]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 13%|█████████▊                                                                |41/309[04:21<31:12, loss=nan]Epoch: 1/4: 13%|███▎                     |41/309[04:21<31:12, Iter=34, Loss=nan, Step=34, UsedTime=0:04:21, lr=0.027508]Epoch: 1/4: 14%|███▍                     |42/309[04:21<31:28, Iter=34, Loss=nan, Step=34, UsedTime=0:04:21, lr=0.027508]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 14%|██████████                                                                |42/309[04:28<31:28, loss=nan]Epoch: 1/4: 14%|███▍                     |42/309[04:28<31:28, Iter=35, Loss=nan, Step=35, UsedTime=0:04:28, lr=0.028317]Epoch: 1/4: 14%|███▍                     |43/309[04:28<31:45, Iter=35, Loss=nan, Step=35, UsedTime=0:04:28, lr=0.028317]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 14%|███▌                     |44/309[04:32<26:48, Iter=35, Loss=nan, Step=35, UsedTime=0:04:28, lr=0.028317]Epoch: 1/4: 14%|██████████▌                                                               |44/309[04:36<26:48, loss=nan]Epoch: 1/4: 14%|███▌                     |44/309[04:36<26:48, Iter=36, Loss=nan, Step=36, UsedTime=0:04:36, lr=0.029126]Epoch: 1/4: 15%|███▋                     |45/309[04:36<23:39, Iter=36, Loss=nan, Step=36, UsedTime=0:04:36, lr=0.029126]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 15%|██████████▊                                                               |45/309[04:43<23:39, loss=nan]Epoch: 1/4: 15%|███▋                     |45/309[04:43<23:39, Iter=37, Loss=nan, Step=37, UsedTime=0:04:43, lr=0.029935]Epoch: 1/4: 15%|███▋                     |46/309[04:43<26:08, Iter=37, Loss=nan, Step=37, UsedTime=0:04:43, lr=0.029935]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 15%|███████████                                                               |46/309[04:50<26:08, loss=nan]Epoch: 1/4: 15%|███▋                     |46/309[04:50<26:08, Iter=38, Loss=nan, Step=38, UsedTime=0:04:50, lr=0.030744]Epoch: 1/4: 15%|███▊                     |47/309[04:50<27:47, Iter=38, Loss=nan, Step=38, UsedTime=0:04:50, lr=0.030744]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 15%|███████████▎                                                              |47/309[04:58<27:47, loss=nan]Epoch: 1/4: 15%|███▊                     |47/309[04:58<27:47, Iter=39, Loss=nan, Step=39, UsedTime=0:04:58, lr=0.031553]Epoch: 1/4: 16%|███▉                     |48/309[04:58<28:53, Iter=39, Loss=nan, Step=39, UsedTime=0:04:58, lr=0.031553]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 16%|███████████▍                                                              |48/309[05:05<28:53, loss=nan]Epoch: 1/4: 16%|███▉                     |48/309[05:05<28:53, Iter=40, Loss=nan, Step=40, UsedTime=0:05:05, lr=0.032362]Epoch: 1/4: 16%|███▉                     |49/309[05:05<29:40, Iter=40, Loss=nan, Step=40, UsedTime=0:05:05, lr=0.032362]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 16%|███████████▋                                                              |49/309[05:12<29:40, loss=nan]Epoch: 1/4: 16%|███▉                     |49/309[05:12<29:40, Iter=41, Loss=nan, Step=41, UsedTime=0:05:12, lr=0.033172]Epoch: 1/4: 16%|████                     |50/309[05:12<30:10, Iter=41, Loss=nan, Step=41, UsedTime=0:05:12, lr=0.033172]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 16%|███████████▉                                                              |50/309[05:20<30:10, loss=nan]Epoch: 1/4: 16%|████                     |50/309[05:20<30:10, Iter=42, Loss=nan, Step=42, UsedTime=0:05:20, lr=0.033981]Epoch: 1/4: 17%|████▏                    |51/309[05:20<30:33, Iter=42, Loss=nan, Step=42, UsedTime=0:05:20, lr=0.033981]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 17%|████████████▏                                                             |51/309[05:27<30:33, loss=nan]Epoch: 1/4: 17%|████▏                    |51/309[05:27<30:33, Iter=43, Loss=nan, Step=43, UsedTime=0:05:27, lr=0.034790]Epoch: 1/4: 17%|████▏                    |52/309[05:27<30:45, Iter=43, Loss=nan, Step=43, UsedTime=0:05:27, lr=0.034790]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 17%|████████████▍                                                             |52/309[05:34<30:45, loss=nan]Epoch: 1/4: 17%|████▏                    |52/309[05:34<30:45, Iter=44, Loss=nan, Step=44, UsedTime=0:05:34, lr=0.035599]Epoch: 1/4: 17%|████▎                    |53/309[05:34<30:46, Iter=44, Loss=nan, Step=44, UsedTime=0:05:34, lr=0.035599]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 17%|████▎                    |54/309[05:38<25:57, Iter=44, Loss=nan, Step=44, UsedTime=0:05:34, lr=0.035599]Epoch: 1/4: 17%|████████████▉                                                             |54/309[05:42<25:57, loss=nan]Epoch: 1/4: 17%|████▎                    |54/309[05:42<25:57, Iter=45, Loss=nan, Step=45, UsedTime=0:05:42, lr=0.036408]Epoch: 1/4: 18%|████▍                    |55/309[05:42<22:55, Iter=45, Loss=nan, Step=45, UsedTime=0:05:42, lr=0.036408]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 18%|█████████████▏                                                            |55/309[05:49<22:55, loss=nan]Epoch: 1/4: 18%|████▍                    |55/309[05:49<22:55, Iter=46, Loss=nan, Step=46, UsedTime=0:05:49, lr=0.037217]Epoch: 1/4: 18%|████▌                    |56/309[05:49<25:14, Iter=46, Loss=nan, Step=46, UsedTime=0:05:49, lr=0.037217]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 18%|█████████████▍                                                            |56/309[05:56<25:14, loss=nan]Epoch: 1/4: 18%|████▌                    |56/309[05:56<25:14, Iter=47, Loss=nan, Step=47, UsedTime=0:05:56, lr=0.038026]Epoch: 1/4: 18%|████▌                    |57/309[05:56<26:48, Iter=47, Loss=nan, Step=47, UsedTime=0:05:56, lr=0.038026]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 18%|█████████████▋                                                            |57/309[06:04<26:48, loss=nan]Epoch: 1/4: 18%|████▌                    |57/309[06:04<26:48, Iter=48, Loss=nan, Step=48, UsedTime=0:06:04, lr=0.038835]Epoch: 1/4: 19%|████▋                    |58/309[06:04<27:51, Iter=48, Loss=nan, Step=48, UsedTime=0:06:04, lr=0.038835]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 19%|█████████████▉                                                            |58/309[06:11<27:51, loss=nan]Epoch: 1/4: 19%|████▋                    |58/309[06:11<27:51, Iter=49, Loss=nan, Step=49, UsedTime=0:06:11, lr=0.039644]Epoch: 1/4: 19%|████▊                    |59/309[06:11<28:36, Iter=49, Loss=nan, Step=49, UsedTime=0:06:11, lr=0.039644]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 19%|██████████████▏                                                           |59/309[06:18<28:36, loss=nan]Epoch: 1/4: 19%|████▊                    |59/309[06:18<28:36, Iter=50, Loss=nan, Step=50, UsedTime=0:06:18, lr=0.040453]Epoch: 1/4: 19%|████▊                    |60/309[06:18<29:03, Iter=50, Loss=nan, Step=50, UsedTime=0:06:18, lr=0.040453]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 19%|██████████████▎                                                           |60/309[06:26<29:03, loss=nan]Epoch: 1/4: 19%|████▊                    |60/309[06:26<29:03, Iter=51, Loss=nan, Step=51, UsedTime=0:06:26, lr=0.041262]Epoch: 1/4: 20%|████▉                    |61/309[06:26<29:19, Iter=51, Loss=nan, Step=51, UsedTime=0:06:26, lr=0.041262]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 20%|██████████████▌                                                           |61/309[06:33<29:19, loss=nan]Epoch: 1/4: 20%|████▉                    |61/309[06:33<29:19, Iter=52, Loss=nan, Step=52, UsedTime=0:06:33, lr=0.042071]Epoch: 1/4: 20%|█████                    |62/309[06:33<29:33, Iter=52, Loss=nan, Step=52, UsedTime=0:06:33, lr=0.042071]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 20%|██████████████▊                                                           |62/309[06:40<29:33, loss=nan]Epoch: 1/4: 20%|█████                    |62/309[06:40<29:33, Iter=53, Loss=nan, Step=53, UsedTime=0:06:40, lr=0.042880]Epoch: 1/4: 20%|█████                    |63/309[06:40<29:36, Iter=53, Loss=nan, Step=53, UsedTime=0:06:40, lr=0.042880]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 20%|███████████████                                                           |63/309[06:48<29:36, loss=nan]Epoch: 1/4: 20%|█████                    |63/309[06:48<29:36, Iter=54, Loss=nan, Step=54, UsedTime=0:06:48, lr=0.043689]Epoch: 1/4: 21%|█████▏                   |64/309[06:48<29:40, Iter=54, Loss=nan, Step=54, UsedTime=0:06:48, lr=0.043689]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 21%|█████▎                   |65/309[06:51<25:01, Iter=54, Loss=nan, Step=54, UsedTime=0:06:48, lr=0.043689]Epoch: 1/4: 21%|███████████████▌                                                          |65/309[06:55<25:01, loss=nan]Epoch: 1/4: 21%|█████▎                   |65/309[06:55<25:01, Iter=55, Loss=nan, Step=55, UsedTime=0:06:55, lr=0.044498]Epoch: 1/4: 21%|█████▎                   |66/309[06:55<22:01, Iter=55, Loss=nan, Step=55, UsedTime=0:06:55, lr=0.044498]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 21%|███████████████▊                                                          |66/309[07:02<22:01, loss=nan]Epoch: 1/4: 21%|█████▎                   |66/309[07:02<22:01, Iter=56, Loss=nan, Step=56, UsedTime=0:07:02, lr=0.045307]Epoch: 1/4: 22%|█████▍                   |67/309[07:02<24:14, Iter=56, Loss=nan, Step=56, UsedTime=0:07:02, lr=0.045307]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 22%|████████████████                                                          |67/309[07:10<24:14, loss=nan]Epoch: 1/4: 22%|█████▍                   |67/309[07:10<24:14, Iter=57, Loss=nan, Step=57, UsedTime=0:07:10, lr=0.046117]Epoch: 1/4: 22%|█████▌                   |68/309[07:10<25:44, Iter=57, Loss=nan, Step=57, UsedTime=0:07:10, lr=0.046117]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 22%|████████████████▎                                                         |68/309[07:17<25:44, loss=nan]Epoch: 1/4: 22%|█████▌                   |68/309[07:17<25:44, Iter=58, Loss=nan, Step=58, UsedTime=0:07:17, lr=0.046926]Epoch: 1/4: 22%|█████▌                   |69/309[07:17<26:43, Iter=58, Loss=nan, Step=58, UsedTime=0:07:17, lr=0.046926]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 22%|████████████████▌                                                         |69/309[07:24<26:43, loss=nan]Epoch: 1/4: 22%|█████▌                   |69/309[07:24<26:43, Iter=59, Loss=nan, Step=59, UsedTime=0:07:24, lr=0.047735]Epoch: 1/4: 23%|█████▋                   |70/309[07:24<27:25, Iter=59, Loss=nan, Step=59, UsedTime=0:07:24, lr=0.047735]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 23%|████████████████▊                                                         |70/309[07:32<27:25, loss=nan]Epoch: 1/4: 23%|█████▋                   |70/309[07:32<27:25, Iter=60, Loss=nan, Step=60, UsedTime=0:07:32, lr=0.048544]Epoch: 1/4: 23%|█████▋                   |71/309[07:32<27:47, Iter=60, Loss=nan, Step=60, UsedTime=0:07:32, lr=0.048544]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 23%|█████████████████                                                         |71/309[07:39<27:47, loss=nan]Epoch: 1/4: 23%|█████▋                   |71/309[07:39<27:47, Iter=61, Loss=nan, Step=61, UsedTime=0:07:39, lr=0.049353]Epoch: 1/4: 23%|█████▊                   |72/309[07:39<27:59, Iter=61, Loss=nan, Step=61, UsedTime=0:07:39, lr=0.049353]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 23%|█████████████████▏                                                        |72/309[07:46<27:59, loss=nan]Epoch: 1/4: 23%|█████▊                   |72/309[07:46<27:59, Iter=62, Loss=nan, Step=62, UsedTime=0:07:46, lr=0.050162]Epoch: 1/4: 24%|█████▉                   |73/309[07:46<28:12, Iter=62, Loss=nan, Step=62, UsedTime=0:07:46, lr=0.050162]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 24%|█████████████████▍                                                        |73/309[07:54<28:12, loss=nan]Epoch: 1/4: 24%|█████▉                   |73/309[07:54<28:12, Iter=63, Loss=nan, Step=63, UsedTime=0:07:54, lr=0.050971]Epoch: 1/4: 24%|█████▉                   |74/309[07:54<28:16, Iter=63, Loss=nan, Step=63, UsedTime=0:07:54, lr=0.050971]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 24%|█████████████████▋                                                        |74/309[08:01<28:16, loss=nan]Epoch: 1/4: 24%|█████▉                   |74/309[08:01<28:16, Iter=64, Loss=nan, Step=64, UsedTime=0:08:01, lr=0.051780]Epoch: 1/4: 24%|██████                   |75/309[08:01<28:13, Iter=64, Loss=nan, Step=64, UsedTime=0:08:01, lr=0.051780]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 24%|█████████████████▉                                                        |75/309[08:08<28:13, loss=nan]Epoch: 1/4: 24%|██████                   |75/309[08:08<28:13, Iter=65, Loss=nan, Step=65, UsedTime=0:08:08, lr=0.052589]Epoch: 1/4: 25%|██████▏                  |76/309[08:08<28:18, Iter=65, Loss=nan, Step=65, UsedTime=0:08:08, lr=0.052589]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 25%|██████▏                  |77/309[08:12<23:52, Iter=65, Loss=nan, Step=65, UsedTime=0:08:08, lr=0.052589]Epoch: 1/4: 25%|██████████████████▍                                                       |77/309[08:16<23:52, loss=nan]Epoch: 1/4: 25%|██████▏                  |77/309[08:16<23:52, Iter=66, Loss=nan, Step=66, UsedTime=0:08:16, lr=0.053398]Epoch: 1/4: 25%|██████▎                  |78/309[08:16<21:05, Iter=66, Loss=nan, Step=66, UsedTime=0:08:16, lr=0.053398]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 25%|██████████████████▋                                                       |78/309[08:23<21:05, loss=nan]Epoch: 1/4: 25%|██████▎                  |78/309[08:23<21:05, Iter=67, Loss=nan, Step=67, UsedTime=0:08:23, lr=0.054207]Epoch: 1/4: 26%|██████▍                  |79/309[08:23<23:08, Iter=67, Loss=nan, Step=67, UsedTime=0:08:23, lr=0.054207]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 26%|██████████████████▉                                                       |79/309[08:30<23:08, loss=nan]Epoch: 1/4: 26%|██████▍                  |79/309[08:30<23:08, Iter=68, Loss=nan, Step=68, UsedTime=0:08:30, lr=0.055016]Epoch: 1/4: 26%|██████▍                  |80/309[08:30<24:30, Iter=68, Loss=nan, Step=68, UsedTime=0:08:30, lr=0.055016]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 26%|███████████████████▏                                                      |80/309[08:38<24:30, loss=nan]Epoch: 1/4: 26%|██████▍                  |80/309[08:38<24:30, Iter=69, Loss=nan, Step=69, UsedTime=0:08:38, lr=0.055825]Epoch: 1/4: 26%|██████▌                  |81/309[08:38<25:23, Iter=69, Loss=nan, Step=69, UsedTime=0:08:38, lr=0.055825]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 26%|███████████████████▍                                                      |81/309[08:45<25:23, loss=nan]Epoch: 1/4: 26%|██████▌                  |81/309[08:45<25:23, Iter=70, Loss=nan, Step=70, UsedTime=0:08:45, lr=0.056634]Epoch: 1/4: 27%|██████▋                  |82/309[08:45<26:01, Iter=70, Loss=nan, Step=70, UsedTime=0:08:45, lr=0.056634]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 27%|███████████████████▋                                                      |82/309[08:52<26:01, loss=nan]Epoch: 1/4: 27%|██████▋                  |82/309[08:52<26:01, Iter=71, Loss=nan, Step=71, UsedTime=0:08:52, lr=0.057443]Epoch: 1/4: 27%|██████▋                  |83/309[08:52<26:18, Iter=71, Loss=nan, Step=71, UsedTime=0:08:52, lr=0.057443]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 27%|███████████████████▉                                                      |83/309[09:00<26:18, loss=nan]Epoch: 1/4: 27%|██████▋                  |83/309[09:00<26:18, Iter=72, Loss=nan, Step=72, UsedTime=0:09:00, lr=0.058252]Epoch: 1/4: 27%|██████▊                  |84/309[09:00<26:36, Iter=72, Loss=nan, Step=72, UsedTime=0:09:00, lr=0.058252]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 27%|████████████████████                                                      |84/309[09:07<26:36, loss=nan]Epoch: 1/4: 27%|██████▊                  |84/309[09:07<26:36, Iter=73, Loss=nan, Step=73, UsedTime=0:09:07, lr=0.059061]Epoch: 1/4: 28%|██████▉                  |85/309[09:07<26:44, Iter=73, Loss=nan, Step=73, UsedTime=0:09:07, lr=0.059061]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 28%|████████████████████▎                                                     |85/309[09:14<26:44, loss=nan]Epoch: 1/4: 28%|██████▉                  |85/309[09:14<26:44, Iter=74, Loss=nan, Step=74, UsedTime=0:09:14, lr=0.059871]Epoch: 1/4: 28%|██████▉                  |86/309[09:14<26:44, Iter=74, Loss=nan, Step=74, UsedTime=0:09:14, lr=0.059871]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 28%|████████████████████▌                                                     |86/309[09:21<26:44, loss=nan]Epoch: 1/4: 28%|██████▉                  |86/309[09:21<26:44, Iter=75, Loss=nan, Step=75, UsedTime=0:09:21, lr=0.060680]Epoch: 1/4: 28%|███████                  |87/309[09:21<26:46, Iter=75, Loss=nan, Step=75, UsedTime=0:09:21, lr=0.060680]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 28%|████████████████████▊                                                     |87/309[09:29<26:46, loss=nan]Epoch: 1/4: 28%|███████                  |87/309[09:29<26:46, Iter=76, Loss=nan, Step=76, UsedTime=0:09:29, lr=0.061489]Epoch: 1/4: 28%|███████                  |88/309[09:29<26:46, Iter=76, Loss=nan, Step=76, UsedTime=0:09:29, lr=0.061489]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 28%|█████████████████████                                                     |88/309[09:36<26:46, loss=nan]Epoch: 1/4: 28%|███████                  |88/309[09:36<26:46, Iter=77, Loss=nan, Step=77, UsedTime=0:09:36, lr=0.062298]Epoch: 1/4: 29%|███████▏                 |89/309[09:36<26:42, Iter=77, Loss=nan, Step=77, UsedTime=0:09:36, lr=0.062298]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 29%|███████▎                 |90/309[09:40<22:28, Iter=77, Loss=nan, Step=77, UsedTime=0:09:36, lr=0.062298]Epoch: 1/4: 29%|█████████████████████▌                                                    |90/309[09:43<22:28, loss=nan]Epoch: 1/4: 29%|███████▎                 |90/309[09:43<22:28, Iter=78, Loss=nan, Step=78, UsedTime=0:09:43, lr=0.063107]Epoch: 1/4: 29%|███████▎                 |91/309[09:43<19:44, Iter=78, Loss=nan, Step=78, UsedTime=0:09:43, lr=0.063107]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 29%|█████████████████████▊                                                    |91/309[09:51<19:44, loss=nan]Epoch: 1/4: 29%|███████▎                 |91/309[09:51<19:44, Iter=79, Loss=nan, Step=79, UsedTime=0:09:51, lr=0.063916]Epoch: 1/4: 30%|███████▍                 |92/309[09:51<21:45, Iter=79, Loss=nan, Step=79, UsedTime=0:09:51, lr=0.063916]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 30%|██████████████████████                                                    |92/309[09:58<21:45, loss=nan]Epoch: 1/4: 30%|███████▍                 |92/309[09:58<21:45, Iter=80, Loss=nan, Step=80, UsedTime=0:09:58, lr=0.064725]Epoch: 1/4: 30%|███████▌                 |93/309[09:58<22:58, Iter=80, Loss=nan, Step=80, UsedTime=0:09:58, lr=0.064725]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 30%|██████████████████████▎                                                   |93/309[10:05<22:58, loss=nan]Epoch: 1/4: 30%|███████▌                 |93/309[10:05<22:58, Iter=81, Loss=nan, Step=81, UsedTime=0:10:05, lr=0.065534]Epoch: 1/4: 30%|███████▌                 |94/309[10:05<23:54, Iter=81, Loss=nan, Step=81, UsedTime=0:10:05, lr=0.065534]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 30%|██████████████████████▌                                                   |94/309[10:13<23:54, loss=nan]Epoch: 1/4: 30%|███████▌                 |94/309[10:13<23:54, Iter=82, Loss=nan, Step=82, UsedTime=0:10:13, lr=0.066343]Epoch: 1/4: 31%|███████▋                 |95/309[10:13<24:26, Iter=82, Loss=nan, Step=82, UsedTime=0:10:13, lr=0.066343]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 31%|██████████████████████▊                                                   |95/309[10:20<24:26, loss=nan]Epoch: 1/4: 31%|███████▋                 |95/309[10:20<24:26, Iter=83, Loss=nan, Step=83, UsedTime=0:10:20, lr=0.067152]Epoch: 1/4: 31%|███████▊                 |96/309[10:20<24:49, Iter=83, Loss=nan, Step=83, UsedTime=0:10:20, lr=0.067152]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 31%|██████████████████████▉                                                   |96/309[10:27<24:49, loss=nan]Epoch: 1/4: 31%|███████▊                 |96/309[10:27<24:49, Iter=84, Loss=nan, Step=84, UsedTime=0:10:27, lr=0.067961]Epoch: 1/4: 31%|███████▊                 |97/309[10:27<25:02, Iter=84, Loss=nan, Step=84, UsedTime=0:10:27, lr=0.067961]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 31%|███████████████████████▏                                                  |97/309[10:35<25:02, loss=nan]Epoch: 1/4: 31%|███████▊                 |97/309[10:35<25:02, Iter=85, Loss=nan, Step=85, UsedTime=0:10:35, lr=0.068770]Epoch: 1/4: 32%|███████▉                 |98/309[10:35<25:11, Iter=85, Loss=nan, Step=85, UsedTime=0:10:35, lr=0.068770]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 32%|███████████████████████▍                                                  |98/309[10:42<25:11, loss=nan]Epoch: 1/4: 32%|███████▉                 |98/309[10:42<25:11, Iter=86, Loss=nan, Step=86, UsedTime=0:10:42, lr=0.069579]Epoch: 1/4: 32%|████████                 |99/309[10:42<25:11, Iter=86, Loss=nan, Step=86, UsedTime=0:10:42, lr=0.069579]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 32%|███████████████████████▋                                                  |99/309[10:49<25:11, loss=nan]Epoch: 1/4: 32%|████████                 |99/309[10:49<25:11, Iter=87, Loss=nan, Step=87, UsedTime=0:10:49, lr=0.070388]Epoch: 1/4: 32%|███████▊                |100/309[10:49<25:14, Iter=87, Loss=nan, Step=87, UsedTime=0:10:49, lr=0.070388]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 32%|███████████████████████▌                                                 |100/309[10:56<25:14, loss=nan]Epoch: 1/4: 32%|███████▊                |100/309[10:57<25:14, Iter=88, Loss=nan, Step=88, UsedTime=0:10:57, lr=0.071197]Epoch: 1/4: 33%|███████▊                |101/309[10:57<25:10, Iter=88, Loss=nan, Step=88, UsedTime=0:10:57, lr=0.071197]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 33%|███████████████████████▊                                                 |101/309[11:04<25:10, loss=nan]Epoch: 1/4: 33%|███████▊                |101/309[11:04<25:10, Iter=89, Loss=nan, Step=89, UsedTime=0:11:04, lr=0.072006]Epoch: 1/4: 33%|███████▉                |102/309[11:04<25:04, Iter=89, Loss=nan, Step=89, UsedTime=0:11:04, lr=0.072006]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 33%|████████████████████████                                                 |102/309[11:11<25:04, loss=nan]Epoch: 1/4: 33%|███████▉                |102/309[11:11<25:04, Iter=90, Loss=nan, Step=90, UsedTime=0:11:11, lr=0.072816]Epoch: 1/4: 33%|████████                |103/309[11:11<24:58, Iter=90, Loss=nan, Step=90, UsedTime=0:11:11, lr=0.072816]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 34%|████████                |104/309[11:15<21:02, Iter=90, Loss=nan, Step=90, UsedTime=0:11:11, lr=0.072816]Epoch: 1/4: 34%|████████████████████████▌                                                |104/309[11:18<21:02, loss=nan]Epoch: 1/4: 34%|████████                |104/309[11:18<21:02, Iter=91, Loss=nan, Step=91, UsedTime=0:11:18, lr=0.073625]Epoch: 1/4: 34%|████████▏               |105/309[11:18<18:32, Iter=91, Loss=nan, Step=91, UsedTime=0:11:18, lr=0.073625]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 34%|████████████████████████▊                                                |105/309[11:26<18:32, loss=nan]Epoch: 1/4: 34%|████████▏               |105/309[11:26<18:32, Iter=92, Loss=nan, Step=92, UsedTime=0:11:26, lr=0.074434]Epoch: 1/4: 34%|████████▏               |106/309[11:26<20:24, Iter=92, Loss=nan, Step=92, UsedTime=0:11:26, lr=0.074434]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 34%|█████████████████████████                                                |106/309[11:33<20:24, loss=nan]Epoch: 1/4: 34%|████████▏               |106/309[11:33<20:24, Iter=93, Loss=nan, Step=93, UsedTime=0:11:33, lr=0.075243]Epoch: 1/4: 35%|████████▎               |107/309[11:33<21:35, Iter=93, Loss=nan, Step=93, UsedTime=0:11:33, lr=0.075243]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 35%|█████████████████████████▎                                               |107/309[11:40<21:35, loss=nan]Epoch: 1/4: 35%|████████▎               |107/309[11:40<21:35, Iter=94, Loss=nan, Step=94, UsedTime=0:11:40, lr=0.076052]Epoch: 1/4: 35%|████████▍               |108/309[11:40<22:24, Iter=94, Loss=nan, Step=94, UsedTime=0:11:40, lr=0.076052]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 35%|█████████████████████████▌                                               |108/309[11:48<22:24, loss=nan]Epoch: 1/4: 35%|████████▍               |108/309[11:48<22:24, Iter=95, Loss=nan, Step=95, UsedTime=0:11:48, lr=0.076861]Epoch: 1/4: 35%|████████▍               |109/309[11:48<22:54, Iter=95, Loss=nan, Step=95, UsedTime=0:11:48, lr=0.076861]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 35%|█████████████████████████▊                                               |109/309[11:55<22:54, loss=nan]Epoch: 1/4: 35%|████████▍               |109/309[11:55<22:54, Iter=96, Loss=nan, Step=96, UsedTime=0:11:55, lr=0.077670]Epoch: 1/4: 36%|████████▌               |110/309[11:55<23:11, Iter=96, Loss=nan, Step=96, UsedTime=0:11:55, lr=0.077670]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 36%|█████████████████████████▉                                               |110/309[12:02<23:11, loss=nan]Epoch: 1/4: 36%|████████▌               |110/309[12:02<23:11, Iter=97, Loss=nan, Step=97, UsedTime=0:12:02, lr=0.078479]Epoch: 1/4: 36%|████████▌               |111/309[12:02<23:22, Iter=97, Loss=nan, Step=97, UsedTime=0:12:02, lr=0.078479]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 36%|██████████████████████████▏                                              |111/309[12:10<23:22, loss=nan]Epoch: 1/4: 36%|████████▌               |111/309[12:10<23:22, Iter=98, Loss=nan, Step=98, UsedTime=0:12:10, lr=0.079288]Epoch: 1/4: 36%|████████▋               |112/309[12:10<23:23, Iter=98, Loss=nan, Step=98, UsedTime=0:12:10, lr=0.079288]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 36%|██████████████████████████▍                                              |112/309[12:17<23:23, loss=nan]Epoch: 1/4: 36%|████████▋               |112/309[12:17<23:23, Iter=99, Loss=nan, Step=99, UsedTime=0:12:17, lr=0.080097]Epoch: 1/4: 37%|████████▊               |113/309[12:17<23:31, Iter=99, Loss=nan, Step=99, UsedTime=0:12:17, lr=0.080097]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 37%|██████████████████████████▋                                              |113/309[12:24<23:31, loss=nan]Epoch: 1/4: 37%|████████              |113/309[12:24<23:31, Iter=100, Loss=nan, Step=100, UsedTime=0:12:24, lr=0.080906]Epoch: 1/4: 37%|████████              |114/309[12:24<23:28, Iter=100, Loss=nan, Step=100, UsedTime=0:12:24, lr=0.080906]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 37%|██████████████████████████▉                                              |114/309[12:31<23:28, loss=nan]Epoch: 1/4: 37%|████████              |114/309[12:31<23:28, Iter=101, Loss=nan, Step=101, UsedTime=0:12:31, lr=0.081715]Epoch: 1/4: 37%|████████▏             |115/309[12:31<23:25, Iter=101, Loss=nan, Step=101, UsedTime=0:12:31, lr=0.081715]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 37%|███████████████████████████▏                                             |115/309[12:39<23:25, loss=nan]Epoch: 1/4: 37%|████████▏             |115/309[12:39<23:25, Iter=102, Loss=nan, Step=102, UsedTime=0:12:39, lr=0.082524]Epoch: 1/4: 38%|████████▎             |116/309[12:39<23:26, Iter=102, Loss=nan, Step=102, UsedTime=0:12:39, lr=0.082524]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 38%|███████████████████████████▍                                             |116/309[12:46<23:26, loss=nan]Epoch: 1/4: 38%|████████▎             |116/309[12:46<23:26, Iter=103, Loss=nan, Step=103, UsedTime=0:12:46, lr=0.083333]Epoch: 1/4: 38%|████████▎             |117/309[12:46<23:23, Iter=103, Loss=nan, Step=103, UsedTime=0:12:46, lr=0.083333]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 38%|███████████████████████████▋                                             |117/309[12:54<23:23, loss=nan]Epoch: 1/4: 38%|████████▎             |117/309[12:54<23:23, Iter=104, Loss=nan, Step=104, UsedTime=0:12:54, lr=0.084142]Epoch: 1/4: 38%|████████▍             |118/309[12:54<23:16, Iter=104, Loss=nan, Step=104, UsedTime=0:12:54, lr=0.084142]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 39%|████████▍             |119/309[12:57<19:34, Iter=104, Loss=nan, Step=104, UsedTime=0:12:54, lr=0.084142]Epoch: 1/4: 39%|████████████████████████████                                             |119/309[13:01<19:34, loss=nan]Epoch: 1/4: 39%|████████▍             |119/309[13:01<19:34, Iter=105, Loss=nan, Step=105, UsedTime=0:13:01, lr=0.084951]Epoch: 1/4: 39%|████████▌             |120/309[13:01<17:12, Iter=105, Loss=nan, Step=105, UsedTime=0:13:01, lr=0.084951]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 39%|████████████████████████████▎                                            |120/309[13:08<17:12, loss=nan]Epoch: 1/4: 39%|████████▌             |120/309[13:08<17:12, Iter=106, Loss=nan, Step=106, UsedTime=0:13:08, lr=0.085761]Epoch: 1/4: 39%|████████▌             |121/309[13:08<18:56, Iter=106, Loss=nan, Step=106, UsedTime=0:13:08, lr=0.085761]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 39%|████████████████████████████▌                                            |121/309[13:16<18:56, loss=nan]Epoch: 1/4: 39%|████████▌             |121/309[13:16<18:56, Iter=107, Loss=nan, Step=107, UsedTime=0:13:16, lr=0.086570]Epoch: 1/4: 39%|████████▋             |122/309[13:16<20:08, Iter=107, Loss=nan, Step=107, UsedTime=0:13:16, lr=0.086570]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 39%|████████████████████████████▊                                            |122/309[13:23<20:08, loss=nan]Epoch: 1/4: 39%|████████▋             |122/309[13:23<20:08, Iter=108, Loss=nan, Step=108, UsedTime=0:13:23, lr=0.087379]Epoch: 1/4: 40%|████████▊             |123/309[13:23<20:49, Iter=108, Loss=nan, Step=108, UsedTime=0:13:23, lr=0.087379]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 40%|█████████████████████████████                                            |123/309[13:30<20:49, loss=nan]Epoch: 1/4: 40%|████████▊             |123/309[13:30<20:49, Iter=109, Loss=nan, Step=109, UsedTime=0:13:30, lr=0.088188]Epoch: 1/4: 40%|████████▊             |124/309[13:30<21:19, Iter=109, Loss=nan, Step=109, UsedTime=0:13:30, lr=0.088188]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 40%|█████████████████████████████▎                                           |124/309[13:38<21:19, loss=nan]Epoch: 1/4: 40%|████████▊             |124/309[13:38<21:19, Iter=110, Loss=nan, Step=110, UsedTime=0:13:38, lr=0.088997]Epoch: 1/4: 40%|████████▉             |125/309[13:38<21:37, Iter=110, Loss=nan, Step=110, UsedTime=0:13:38, lr=0.088997]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 40%|█████████████████████████████▌                                           |125/309[13:45<21:37, loss=nan]Epoch: 1/4: 40%|████████▉             |125/309[13:45<21:37, Iter=111, Loss=nan, Step=111, UsedTime=0:13:45, lr=0.089806]Epoch: 1/4: 41%|████████▉             |126/309[13:45<21:47, Iter=111, Loss=nan, Step=111, UsedTime=0:13:45, lr=0.089806]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 41%|█████████████████████████████▊                                           |126/309[13:52<21:47, loss=nan]Epoch: 1/4: 41%|████████▉             |126/309[13:52<21:47, Iter=112, Loss=nan, Step=112, UsedTime=0:13:52, lr=0.090615]Epoch: 1/4: 41%|█████████             |127/309[13:52<21:48, Iter=112, Loss=nan, Step=112, UsedTime=0:13:52, lr=0.090615]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 41%|██████████████████████████████                                           |127/309[14:00<21:48, loss=nan]Epoch: 1/4: 41%|█████████             |127/309[14:00<21:48, Iter=113, Loss=nan, Step=113, UsedTime=0:14:00, lr=0.091424]Epoch: 1/4: 41%|█████████             |128/309[14:00<21:49, Iter=113, Loss=nan, Step=113, UsedTime=0:14:00, lr=0.091424]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 41%|██████████████████████████████▏                                          |128/309[14:07<21:49, loss=nan]Epoch: 1/4: 41%|█████████             |128/309[14:07<21:49, Iter=114, Loss=nan, Step=114, UsedTime=0:14:07, lr=0.092233]Epoch: 1/4: 42%|█████████▏            |129/309[14:07<21:47, Iter=114, Loss=nan, Step=114, UsedTime=0:14:07, lr=0.092233]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 42%|██████████████████████████████▍                                          |129/309[14:14<21:47, loss=nan]Epoch: 1/4: 42%|█████████▏            |129/309[14:14<21:47, Iter=115, Loss=nan, Step=115, UsedTime=0:14:14, lr=0.093042]Epoch: 1/4: 42%|█████████▎            |130/309[14:14<21:44, Iter=115, Loss=nan, Step=115, UsedTime=0:14:14, lr=0.093042]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 42%|██████████████████████████████▋                                          |130/309[14:22<21:44, loss=nan]Epoch: 1/4: 42%|█████████▎            |130/309[14:22<21:44, Iter=116, Loss=nan, Step=116, UsedTime=0:14:22, lr=0.093851]Epoch: 1/4: 42%|█████████▎            |131/309[14:22<21:41, Iter=116, Loss=nan, Step=116, UsedTime=0:14:22, lr=0.093851]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 42%|██████████████████████████████▉                                          |131/309[14:29<21:41, loss=nan]Epoch: 1/4: 42%|█████████▎            |131/309[14:29<21:41, Iter=117, Loss=nan, Step=117, UsedTime=0:14:29, lr=0.094660]Epoch: 1/4: 43%|█████████▍            |132/309[14:29<21:36, Iter=117, Loss=nan, Step=117, UsedTime=0:14:29, lr=0.094660]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 43%|███████████████████████████████▏                                         |132/309[14:37<21:36, loss=nan]Epoch: 1/4: 43%|█████████▍            |132/309[14:37<21:36, Iter=118, Loss=nan, Step=118, UsedTime=0:14:37, lr=0.095469]Epoch: 1/4: 43%|█████████▍            |133/309[14:37<21:29, Iter=118, Loss=nan, Step=118, UsedTime=0:14:37, lr=0.095469]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 43%|███████████████████████████████▍                                         |133/309[14:44<21:29, loss=nan]Epoch: 1/4: 43%|█████████▍            |133/309[14:44<21:29, Iter=119, Loss=nan, Step=119, UsedTime=0:14:44, lr=0.096278]Epoch: 1/4: 43%|█████████▌            |134/309[14:44<21:20, Iter=119, Loss=nan, Step=119, UsedTime=0:14:44, lr=0.096278]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 44%|█████████▌            |135/309[14:47<17:54, Iter=119, Loss=nan, Step=119, UsedTime=0:14:44, lr=0.096278]Epoch: 1/4: 44%|███████████████████████████████▉                                         |135/309[14:51<17:54, loss=nan]Epoch: 1/4: 44%|█████████▌            |135/309[14:51<17:54, Iter=120, Loss=nan, Step=120, UsedTime=0:14:51, lr=0.097087]Epoch: 1/4: 44%|█████████▋            |136/309[14:51<15:44, Iter=120, Loss=nan, Step=120, UsedTime=0:14:51, lr=0.097087]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 44%|████████████████████████████████▏                                        |136/309[14:58<15:44, loss=nan]Epoch: 1/4: 44%|█████████▋            |136/309[14:58<15:44, Iter=121, Loss=nan, Step=121, UsedTime=0:14:58, lr=0.097896]Epoch: 1/4: 44%|█████████▊            |137/309[14:58<17:16, Iter=121, Loss=nan, Step=121, UsedTime=0:14:58, lr=0.097896]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 44%|████████████████████████████████▎                                        |137/309[15:06<17:16, loss=nan]Epoch: 1/4: 44%|█████████▊            |137/309[15:06<17:16, Iter=122, Loss=nan, Step=122, UsedTime=0:15:06, lr=0.098706]Epoch: 1/4: 45%|█████████▊            |138/309[15:06<18:18, Iter=122, Loss=nan, Step=122, UsedTime=0:15:06, lr=0.098706]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 45%|████████████████████████████████▌                                        |138/309[15:13<18:18, loss=nan]Epoch: 1/4: 45%|█████████▊            |138/309[15:13<18:18, Iter=123, Loss=nan, Step=123, UsedTime=0:15:13, lr=0.099515]Epoch: 1/4: 45%|█████████▉            |139/309[15:13<19:01, Iter=123, Loss=nan, Step=123, UsedTime=0:15:13, lr=0.099515]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 45%|████████████████████████████████▊                                        |139/309[15:21<19:01, loss=nan]Epoch: 1/4: 45%|█████████▉            |139/309[15:21<19:01, Iter=124, Loss=nan, Step=124, UsedTime=0:15:21, lr=0.099964]Epoch: 1/4: 45%|█████████▉            |140/309[15:21<19:26, Iter=124, Loss=nan, Step=124, UsedTime=0:15:21, lr=0.099964]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 45%|█████████████████████████████████                                        |140/309[15:28<19:26, loss=nan]Epoch: 1/4: 45%|█████████▉            |140/309[15:28<19:26, Iter=125, Loss=nan, Step=125, UsedTime=0:15:28, lr=0.099874]Epoch: 1/4: 46%|██████████            |141/309[15:28<19:42, Iter=125, Loss=nan, Step=125, UsedTime=0:15:28, lr=0.099874]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 46%|█████████████████████████████████▎                                       |141/309[15:35<19:42, loss=nan]Epoch: 1/4: 46%|██████████            |141/309[15:35<19:42, Iter=126, Loss=nan, Step=126, UsedTime=0:15:35, lr=0.099784]Epoch: 1/4: 46%|██████████            |142/309[15:35<19:48, Iter=126, Loss=nan, Step=126, UsedTime=0:15:35, lr=0.099784]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 46%|█████████████████████████████████▌                                       |142/309[15:43<19:48, loss=nan]Epoch: 1/4: 46%|██████████            |142/309[15:43<19:48, Iter=127, Loss=nan, Step=127, UsedTime=0:15:43, lr=0.099694]Epoch: 1/4: 46%|██████████▏           |143/309[15:43<19:53, Iter=127, Loss=nan, Step=127, UsedTime=0:15:43, lr=0.099694]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 46%|█████████████████████████████████▊                                       |143/309[15:50<19:53, loss=nan]Epoch: 1/4: 46%|██████████▏           |143/309[15:50<19:53, Iter=128, Loss=nan, Step=128, UsedTime=0:15:50, lr=0.099604]Epoch: 1/4: 47%|██████████▎           |144/309[15:50<19:53, Iter=128, Loss=nan, Step=128, UsedTime=0:15:50, lr=0.099604]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 47%|██████████████████████████████████                                       |144/309[15:57<19:53, loss=nan]Epoch: 1/4: 47%|██████████▎           |144/309[15:57<19:53, Iter=129, Loss=nan, Step=129, UsedTime=0:15:57, lr=0.099515]Epoch: 1/4: 47%|██████████▎           |145/309[15:57<19:54, Iter=129, Loss=nan, Step=129, UsedTime=0:15:57, lr=0.099515]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 47%|██████████████████████████████████▎                                      |145/309[16:05<19:54, loss=nan]Epoch: 1/4: 47%|██████████▎           |145/309[16:05<19:54, Iter=130, Loss=nan, Step=130, UsedTime=0:16:05, lr=0.099425]Epoch: 1/4: 47%|██████████▍           |146/309[16:05<19:50, Iter=130, Loss=nan, Step=130, UsedTime=0:16:05, lr=0.099425]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 47%|██████████████████████████████████▍                                      |146/309[16:12<19:50, loss=nan]Epoch: 1/4: 47%|██████████▍           |146/309[16:12<19:50, Iter=131, Loss=nan, Step=131, UsedTime=0:16:12, lr=0.099335]Epoch: 1/4: 48%|██████████▍           |147/309[16:12<19:44, Iter=131, Loss=nan, Step=131, UsedTime=0:16:12, lr=0.099335]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 48%|██████████████████████████████████▋                                      |147/309[16:19<19:44, loss=nan]Epoch: 1/4: 48%|██████████▍           |147/309[16:19<19:44, Iter=132, Loss=nan, Step=132, UsedTime=0:16:19, lr=0.099245]Epoch: 1/4: 48%|██████████▌           |148/309[16:19<19:42, Iter=132, Loss=nan, Step=132, UsedTime=0:16:19, lr=0.099245]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 48%|██████████████████████████████████▉                                      |148/309[16:27<19:42, loss=nan]Epoch: 1/4: 48%|██████████▌           |148/309[16:27<19:42, Iter=133, Loss=nan, Step=133, UsedTime=0:16:27, lr=0.099155]Epoch: 1/4: 48%|██████████▌           |149/309[16:27<19:34, Iter=133, Loss=nan, Step=133, UsedTime=0:16:27, lr=0.099155]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 48%|███████████████████████████████████▏                                     |149/309[16:34<19:34, loss=nan]Epoch: 1/4: 48%|██████████▌           |149/309[16:34<19:34, Iter=134, Loss=nan, Step=134, UsedTime=0:16:34, lr=0.099065]Epoch: 1/4: 49%|██████████▋           |150/309[16:34<19:23, Iter=134, Loss=nan, Step=134, UsedTime=0:16:34, lr=0.099065]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 49%|███████████████████████████████████▍                                     |150/309[16:41<19:23, loss=nan]Epoch: 1/4: 49%|██████████▋           |150/309[16:41<19:23, Iter=135, Loss=nan, Step=135, UsedTime=0:16:41, lr=0.098975]Epoch: 1/4: 49%|██████████▊           |151/309[16:41<19:17, Iter=135, Loss=nan, Step=135, UsedTime=0:16:41, lr=0.098975]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 49%|██████████▊           |152/309[16:45<16:11, Iter=135, Loss=nan, Step=135, UsedTime=0:16:41, lr=0.098975]Epoch: 1/4: 49%|███████████████████████████████████▉                                     |152/309[16:49<16:11, loss=nan]Epoch: 1/4: 49%|██████████▊           |152/309[16:49<16:11, Iter=136, Loss=nan, Step=136, UsedTime=0:16:49, lr=0.098885]Epoch: 1/4: 50%|██████████▉           |153/309[16:49<14:13, Iter=136, Loss=nan, Step=136, UsedTime=0:16:49, lr=0.098885]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 50%|████████████████████████████████████▏                                    |153/309[16:56<14:13, loss=nan]Epoch: 1/4: 50%|██████████▉           |153/309[16:56<14:13, Iter=137, Loss=nan, Step=137, UsedTime=0:16:56, lr=0.098795]Epoch: 1/4: 50%|██████████▉           |154/309[16:56<15:34, Iter=137, Loss=nan, Step=137, UsedTime=0:16:56, lr=0.098795]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 50%|████████████████████████████████████▍                                    |154/309[17:03<15:34, loss=nan]Epoch: 1/4: 50%|██████████▉           |154/309[17:03<15:34, Iter=138, Loss=nan, Step=138, UsedTime=0:17:03, lr=0.098706]Epoch: 1/4: 50%|███████████           |155/309[17:03<16:29, Iter=138, Loss=nan, Step=138, UsedTime=0:17:03, lr=0.098706]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 50%|████████████████████████████████████▌                                    |155/309[17:11<16:29, loss=nan]Epoch: 1/4: 50%|███████████           |155/309[17:11<16:29, Iter=139, Loss=nan, Step=139, UsedTime=0:17:11, lr=0.098616]Epoch: 1/4: 50%|███████████           |156/309[17:11<17:03, Iter=139, Loss=nan, Step=139, UsedTime=0:17:11, lr=0.098616]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 50%|████████████████████████████████████▊                                    |156/309[17:18<17:03, loss=nan]Epoch: 1/4: 50%|███████████           |156/309[17:18<17:03, Iter=140, Loss=nan, Step=140, UsedTime=0:17:18, lr=0.098526]Epoch: 1/4: 51%|███████████▏          |157/309[17:18<17:24, Iter=140, Loss=nan, Step=140, UsedTime=0:17:18, lr=0.098526]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 51%|█████████████████████████████████████                                    |157/309[17:25<17:24, loss=nan]Epoch: 1/4: 51%|███████████▏          |157/309[17:25<17:24, Iter=141, Loss=nan, Step=141, UsedTime=0:17:25, lr=0.098436]Epoch: 1/4: 51%|███████████▏          |158/309[17:25<17:41, Iter=141, Loss=nan, Step=141, UsedTime=0:17:25, lr=0.098436]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 51%|█████████████████████████████████████▎                                   |158/309[17:33<17:41, loss=nan]Epoch: 1/4: 51%|███████████▏          |158/309[17:33<17:41, Iter=142, Loss=nan, Step=142, UsedTime=0:17:33, lr=0.098346]Epoch: 1/4: 51%|███████████▎          |159/309[17:33<17:47, Iter=142, Loss=nan, Step=142, UsedTime=0:17:33, lr=0.098346]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 51%|█████████████████████████████████████▌                                   |159/309[17:40<17:47, loss=nan]Epoch: 1/4: 51%|███████████▎          |159/309[17:40<17:47, Iter=143, Loss=nan, Step=143, UsedTime=0:17:40, lr=0.098256]Epoch: 1/4: 52%|███████████▍          |160/309[17:40<17:49, Iter=143, Loss=nan, Step=143, UsedTime=0:17:40, lr=0.098256]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 52%|█████████████████████████████████████▊                                   |160/309[17:47<17:49, loss=nan]Epoch: 1/4: 52%|███████████▍          |160/309[17:47<17:49, Iter=144, Loss=nan, Step=144, UsedTime=0:17:47, lr=0.098166]Epoch: 1/4: 52%|███████████▍          |161/309[17:47<17:47, Iter=144, Loss=nan, Step=144, UsedTime=0:17:47, lr=0.098166]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 52%|██████████████████████████████████████                                   |161/309[17:55<17:47, loss=nan]Epoch: 1/4: 52%|███████████▍          |161/309[17:55<17:47, Iter=145, Loss=nan, Step=145, UsedTime=0:17:55, lr=0.098076]Epoch: 1/4: 52%|███████████▌          |162/309[17:55<17:43, Iter=145, Loss=nan, Step=145, UsedTime=0:17:55, lr=0.098076]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 52%|██████████████████████████████████████▎                                  |162/309[18:02<17:43, loss=nan]Epoch: 1/4: 52%|███████████▌          |162/309[18:02<17:43, Iter=146, Loss=nan, Step=146, UsedTime=0:18:02, lr=0.097986]Epoch: 1/4: 53%|███████████▌          |163/309[18:02<17:38, Iter=146, Loss=nan, Step=146, UsedTime=0:18:02, lr=0.097986]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 53%|██████████████████████████████████████▌                                  |163/309[18:09<17:38, loss=nan]Epoch: 1/4: 53%|███████████▌          |163/309[18:09<17:38, Iter=147, Loss=nan, Step=147, UsedTime=0:18:09, lr=0.097896]Epoch: 1/4: 53%|███████████▋          |164/309[18:09<17:33, Iter=147, Loss=nan, Step=147, UsedTime=0:18:09, lr=0.097896]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 53%|██████████████████████████████████████▋                                  |164/309[18:17<17:33, loss=nan]Epoch: 1/4: 53%|███████████▋          |164/309[18:17<17:33, Iter=148, Loss=nan, Step=148, UsedTime=0:18:17, lr=0.097807]Epoch: 1/4: 53%|███████████▋          |165/309[18:17<17:29, Iter=148, Loss=nan, Step=148, UsedTime=0:18:17, lr=0.097807]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 53%|██████████████████████████████████████▉                                  |165/309[18:24<17:29, loss=nan]Epoch: 1/4: 53%|███████████▋          |165/309[18:24<17:29, Iter=149, Loss=nan, Step=149, UsedTime=0:18:24, lr=0.097717]Epoch: 1/4: 54%|███████████▊          |166/309[18:24<17:23, Iter=149, Loss=nan, Step=149, UsedTime=0:18:24, lr=0.097717]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 54%|███████████████████████████████████████▏                                 |166/309[18:31<17:23, loss=nan]Epoch: 1/4: 54%|███████████▊          |166/309[18:31<17:23, Iter=150, Loss=nan, Step=150, UsedTime=0:18:31, lr=0.097627]Epoch: 1/4: 54%|███████████▉          |167/309[18:31<17:17, Iter=150, Loss=nan, Step=150, UsedTime=0:18:31, lr=0.097627]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 54%|███████████████████████████████████████▍                                 |167/309[18:38<17:17, loss=nan]Epoch: 1/4: 54%|███████████▉          |167/309[18:38<17:17, Iter=151, Loss=nan, Step=151, UsedTime=0:18:38, lr=0.097537]Epoch: 1/4: 54%|███████████▉          |168/309[18:38<17:10, Iter=151, Loss=nan, Step=151, UsedTime=0:18:38, lr=0.097537]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 54%|███████████████████████████████████████▋                                 |168/309[18:46<17:10, loss=nan]Epoch: 1/4: 54%|███████████▉          |168/309[18:46<17:10, Iter=152, Loss=nan, Step=152, UsedTime=0:18:46, lr=0.097447]Epoch: 1/4: 55%|████████████          |169/309[18:46<17:05, Iter=152, Loss=nan, Step=152, UsedTime=0:18:46, lr=0.097447]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 55%|████████████          |170/309[18:49<14:20, Iter=152, Loss=nan, Step=152, UsedTime=0:18:46, lr=0.097447]Epoch: 1/4: 55%|████████████████████████████████████████▏                                |170/309[18:53<14:20, loss=nan]Epoch: 1/4: 55%|████████████          |170/309[18:53<14:20, Iter=153, Loss=nan, Step=153, UsedTime=0:18:53, lr=0.097357]Epoch: 1/4: 55%|████████████▏         |171/309[18:53<12:35, Iter=153, Loss=nan, Step=153, UsedTime=0:18:53, lr=0.097357]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 55%|████████████████████████████████████████▍                                |171/309[19:01<12:35, loss=nan]Epoch: 1/4: 55%|████████████▏         |171/309[19:01<12:35, Iter=154, Loss=nan, Step=154, UsedTime=0:19:01, lr=0.097267]Epoch: 1/4: 56%|████████████▏         |172/309[19:01<13:46, Iter=154, Loss=nan, Step=154, UsedTime=0:19:01, lr=0.097267]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 56%|████████████████████████████████████████▋                                |172/309[19:08<13:46, loss=nan]Epoch: 1/4: 56%|████████████▏         |172/309[19:08<13:46, Iter=155, Loss=nan, Step=155, UsedTime=0:19:08, lr=0.097177]Epoch: 1/4: 56%|████████████▎         |173/309[19:08<14:34, Iter=155, Loss=nan, Step=155, UsedTime=0:19:08, lr=0.097177]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 56%|████████████████████████████████████████▊                                |173/309[19:15<14:34, loss=nan]Epoch: 1/4: 56%|████████████▎         |173/309[19:15<14:34, Iter=156, Loss=nan, Step=156, UsedTime=0:19:15, lr=0.097087]Epoch: 1/4: 56%|████████████▍         |174/309[19:15<15:05, Iter=156, Loss=nan, Step=156, UsedTime=0:19:15, lr=0.097087]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 56%|█████████████████████████████████████████                                |174/309[19:23<15:05, loss=nan]Epoch: 1/4: 56%|████████████▍         |174/309[19:23<15:05, Iter=157, Loss=nan, Step=157, UsedTime=0:19:23, lr=0.096997]Epoch: 1/4: 57%|████████████▍         |175/309[19:23<15:22, Iter=157, Loss=nan, Step=157, UsedTime=0:19:23, lr=0.096997]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 57%|█████████████████████████████████████████▎                               |175/309[19:30<15:22, loss=nan]Epoch: 1/4: 57%|████████████▍         |175/309[19:30<15:22, Iter=158, Loss=nan, Step=158, UsedTime=0:19:30, lr=0.096908]Epoch: 1/4: 57%|████████████▌         |176/309[19:30<15:31, Iter=158, Loss=nan, Step=158, UsedTime=0:19:30, lr=0.096908]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 57%|█████████████████████████████████████████▌                               |176/309[19:37<15:31, loss=nan]Epoch: 1/4: 57%|████████████▌         |176/309[19:37<15:31, Iter=159, Loss=nan, Step=159, UsedTime=0:19:37, lr=0.096818]Epoch: 1/4: 57%|████████████▌         |177/309[19:37<15:37, Iter=159, Loss=nan, Step=159, UsedTime=0:19:37, lr=0.096818]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 57%|█████████████████████████████████████████▊                               |177/309[19:45<15:37, loss=nan]Epoch: 1/4: 57%|████████████▌         |177/309[19:45<15:37, Iter=160, Loss=nan, Step=160, UsedTime=0:19:45, lr=0.096728]Epoch: 1/4: 58%|████████████▋         |178/309[19:45<15:40, Iter=160, Loss=nan, Step=160, UsedTime=0:19:45, lr=0.096728]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 58%|██████████████████████████████████████████                               |178/309[19:52<15:40, loss=nan]Epoch: 1/4: 58%|████████████▋         |178/309[19:52<15:40, Iter=161, Loss=nan, Step=161, UsedTime=0:19:52, lr=0.096638]Epoch: 1/4: 58%|████████████▋         |179/309[19:52<15:39, Iter=161, Loss=nan, Step=161, UsedTime=0:19:52, lr=0.096638]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 58%|██████████████████████████████████████████▎                              |179/309[19:59<15:39, loss=nan]Epoch: 1/4: 58%|████████████▋         |179/309[19:59<15:39, Iter=162, Loss=nan, Step=162, UsedTime=0:19:59, lr=0.096548]Epoch: 1/4: 58%|████████████▊         |180/309[19:59<15:37, Iter=162, Loss=nan, Step=162, UsedTime=0:19:59, lr=0.096548]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 58%|██████████████████████████████████████████▌                              |180/309[20:07<15:37, loss=nan]Epoch: 1/4: 58%|████████████▊         |180/309[20:07<15:37, Iter=163, Loss=nan, Step=163, UsedTime=0:20:07, lr=0.096458]Epoch: 1/4: 59%|████████████▉         |181/309[20:07<15:34, Iter=163, Loss=nan, Step=163, UsedTime=0:20:07, lr=0.096458]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 59%|██████████████████████████████████████████▊                              |181/309[20:14<15:34, loss=nan]Epoch: 1/4: 59%|████████████▉         |181/309[20:14<15:34, Iter=164, Loss=nan, Step=164, UsedTime=0:20:14, lr=0.096368]Epoch: 1/4: 59%|████████████▉         |182/309[20:14<15:30, Iter=164, Loss=nan, Step=164, UsedTime=0:20:14, lr=0.096368]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 59%|██████████████████████████████████████████▉                              |182/309[20:21<15:30, loss=nan]Epoch: 1/4: 59%|████████████▉         |182/309[20:21<15:30, Iter=165, Loss=nan, Step=165, UsedTime=0:20:21, lr=0.096278]Epoch: 1/4: 59%|█████████████         |183/309[20:21<15:24, Iter=165, Loss=nan, Step=165, UsedTime=0:20:21, lr=0.096278]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 59%|███████████████████████████████████████████▏                             |183/309[20:29<15:24, loss=nan]Epoch: 1/4: 59%|█████████████         |183/309[20:29<15:24, Iter=166, Loss=nan, Step=166, UsedTime=0:20:29, lr=0.096188]Epoch: 1/4: 60%|█████████████         |184/309[20:29<15:22, Iter=166, Loss=nan, Step=166, UsedTime=0:20:29, lr=0.096188]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 60%|███████████████████████████████████████████▍                             |184/309[20:36<15:22, loss=nan]Epoch: 1/4: 60%|█████████████         |184/309[20:36<15:22, Iter=167, Loss=nan, Step=167, UsedTime=0:20:36, lr=0.096099]Epoch: 1/4: 60%|█████████████▏        |185/309[20:36<15:19, Iter=167, Loss=nan, Step=167, UsedTime=0:20:36, lr=0.096099]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 60%|███████████████████████████████████████████▋                             |185/309[20:44<15:19, loss=nan]Epoch: 1/4: 60%|█████████████▏        |185/309[20:44<15:19, Iter=168, Loss=nan, Step=168, UsedTime=0:20:44, lr=0.096009]Epoch: 1/4: 60%|█████████████▏        |186/309[20:44<15:13, Iter=168, Loss=nan, Step=168, UsedTime=0:20:44, lr=0.096009]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 60%|███████████████████████████████████████████▉                             |186/309[20:51<15:13, loss=nan]Epoch: 1/4: 60%|█████████████▏        |186/309[20:51<15:13, Iter=169, Loss=nan, Step=169, UsedTime=0:20:51, lr=0.095919]Epoch: 1/4: 61%|█████████████▎        |187/309[20:51<15:07, Iter=169, Loss=nan, Step=169, UsedTime=0:20:51, lr=0.095919]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 61%|████████████████████████████████████████████▏                            |187/309[20:59<15:07, loss=nan]Epoch: 1/4: 61%|█████████████▎        |187/309[20:59<15:07, Iter=170, Loss=nan, Step=170, UsedTime=0:20:59, lr=0.095829]Epoch: 1/4: 61%|█████████████▍        |188/309[20:59<14:58, Iter=170, Loss=nan, Step=170, UsedTime=0:20:59, lr=0.095829]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 61%|█████████████▍        |189/309[21:02<12:33, Iter=170, Loss=nan, Step=170, UsedTime=0:20:59, lr=0.095829]Epoch: 1/4: 61%|████████████████████████████████████████████▋                            |189/309[21:06<12:33, loss=nan]Epoch: 1/4: 61%|█████████████▍        |189/309[21:06<12:33, Iter=171, Loss=nan, Step=171, UsedTime=0:21:06, lr=0.095739]Epoch: 1/4: 61%|█████████████▌        |190/309[21:06<11:00, Iter=171, Loss=nan, Step=171, UsedTime=0:21:06, lr=0.095739]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 61%|████████████████████████████████████████████▉                            |190/309[21:14<11:00, loss=nan]Epoch: 1/4: 61%|█████████████▌        |190/309[21:14<11:00, Iter=172, Loss=nan, Step=172, UsedTime=0:21:14, lr=0.095649]Epoch: 1/4: 62%|█████████████▌        |191/309[21:14<12:02, Iter=172, Loss=nan, Step=172, UsedTime=0:21:14, lr=0.095649]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 62%|█████████████████████████████████████████████                            |191/309[21:21<12:02, loss=nan]Epoch: 1/4: 62%|█████████████▌        |191/309[21:21<12:02, Iter=173, Loss=nan, Step=173, UsedTime=0:21:21, lr=0.095559]Epoch: 1/4: 62%|█████████████▋        |192/309[21:21<12:42, Iter=173, Loss=nan, Step=173, UsedTime=0:21:21, lr=0.095559]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 62%|█████████████████████████████████████████████▎                           |192/309[21:28<12:42, loss=nan]Epoch: 1/4: 62%|█████████████▋        |192/309[21:28<12:42, Iter=174, Loss=nan, Step=174, UsedTime=0:21:28, lr=0.095469]Epoch: 1/4: 62%|█████████████▋        |193/309[21:28<13:07, Iter=174, Loss=nan, Step=174, UsedTime=0:21:28, lr=0.095469]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 62%|█████████████████████████████████████████████▌                           |193/309[21:36<13:07, loss=nan]Epoch: 1/4: 62%|█████████████▋        |193/309[21:36<13:07, Iter=175, Loss=nan, Step=175, UsedTime=0:21:36, lr=0.095379]Epoch: 1/4: 63%|█████████████▊        |194/309[21:36<13:23, Iter=175, Loss=nan, Step=175, UsedTime=0:21:36, lr=0.095379]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 63%|█████████████████████████████████████████████▊                           |194/309[21:43<13:23, loss=nan]Epoch: 1/4: 63%|█████████████▊        |194/309[21:43<13:23, Iter=176, Loss=nan, Step=176, UsedTime=0:21:43, lr=0.095289]Epoch: 1/4: 63%|█████████████▉        |195/309[21:43<13:32, Iter=176, Loss=nan, Step=176, UsedTime=0:21:43, lr=0.095289]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 63%|██████████████████████████████████████████████                           |195/309[21:51<13:32, loss=nan]Epoch: 1/4: 63%|█████████████▉        |195/309[21:51<13:32, Iter=177, Loss=nan, Step=177, UsedTime=0:21:51, lr=0.095200]Epoch: 1/4: 63%|█████████████▉        |196/309[21:51<13:39, Iter=177, Loss=nan, Step=177, UsedTime=0:21:51, lr=0.095200]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 63%|██████████████████████████████████████████████▎                          |196/309[21:58<13:39, loss=nan]Epoch: 1/4: 63%|█████████████▉        |196/309[21:58<13:39, Iter=178, Loss=nan, Step=178, UsedTime=0:21:58, lr=0.095110]Epoch: 1/4: 64%|██████████████        |197/309[21:58<13:38, Iter=178, Loss=nan, Step=178, UsedTime=0:21:58, lr=0.095110]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 64%|██████████████████████████████████████████████▌                          |197/309[22:06<13:38, loss=nan]Epoch: 1/4: 64%|██████████████        |197/309[22:06<13:38, Iter=179, Loss=nan, Step=179, UsedTime=0:22:06, lr=0.095020]Epoch: 1/4: 64%|██████████████        |198/309[22:06<13:35, Iter=179, Loss=nan, Step=179, UsedTime=0:22:06, lr=0.095020]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 64%|██████████████████████████████████████████████▊                          |198/309[22:13<13:35, loss=nan]Epoch: 1/4: 64%|██████████████        |198/309[22:13<13:35, Iter=180, Loss=nan, Step=180, UsedTime=0:22:13, lr=0.094930]Epoch: 1/4: 64%|██████████████▏       |199/309[22:13<13:30, Iter=180, Loss=nan, Step=180, UsedTime=0:22:13, lr=0.094930]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 64%|███████████████████████████████████████████████                          |199/309[22:21<13:30, loss=nan]Epoch: 1/4: 64%|██████████████▏       |199/309[22:21<13:30, Iter=181, Loss=nan, Step=181, UsedTime=0:22:21, lr=0.094840]Epoch: 1/4: 65%|██████████████▏       |200/309[22:21<13:25, Iter=181, Loss=nan, Step=181, UsedTime=0:22:21, lr=0.094840]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 65%|███████████████████████████████████████████████▏                         |200/309[22:28<13:25, loss=nan]Epoch: 1/4: 65%|██████████████▏       |200/309[22:28<13:25, Iter=182, Loss=nan, Step=182, UsedTime=0:22:28, lr=0.094750]Epoch: 1/4: 65%|██████████████▎       |201/309[22:28<13:18, Iter=182, Loss=nan, Step=182, UsedTime=0:22:28, lr=0.094750]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 65%|███████████████████████████████████████████████▍                         |201/309[22:35<13:18, loss=nan]Epoch: 1/4: 65%|██████████████▎       |201/309[22:35<13:18, Iter=183, Loss=nan, Step=183, UsedTime=0:22:35, lr=0.094660]Epoch: 1/4: 65%|██████████████▍       |202/309[22:35<13:13, Iter=183, Loss=nan, Step=183, UsedTime=0:22:35, lr=0.094660]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 65%|███████████████████████████████████████████████▋                         |202/309[22:43<13:13, loss=nan]Epoch: 1/4: 65%|██████████████▍       |202/309[22:43<13:13, Iter=184, Loss=nan, Step=184, UsedTime=0:22:43, lr=0.094570]Epoch: 1/4: 66%|██████████████▍       |203/309[22:43<13:04, Iter=184, Loss=nan, Step=184, UsedTime=0:22:43, lr=0.094570]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 66%|███████████████████████████████████████████████▉                         |203/309[22:50<13:04, loss=nan]Epoch: 1/4: 66%|██████████████▍       |203/309[22:50<13:04, Iter=185, Loss=nan, Step=185, UsedTime=0:22:50, lr=0.094480]Epoch: 1/4: 66%|██████████████▌       |204/309[22:50<12:57, Iter=185, Loss=nan, Step=185, UsedTime=0:22:50, lr=0.094480]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 66%|████████████████████████████████████████████████▏                        |204/309[22:58<12:57, loss=nan]Epoch: 1/4: 66%|██████████████▌       |204/309[22:58<12:57, Iter=186, Loss=nan, Step=186, UsedTime=0:22:58, lr=0.094391]Epoch: 1/4: 66%|██████████████▌       |205/309[22:58<12:53, Iter=186, Loss=nan, Step=186, UsedTime=0:22:58, lr=0.094391]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 66%|████████████████████████████████████████████████▍                        |205/309[23:05<12:53, loss=nan]Epoch: 1/4: 66%|██████████████▌       |205/309[23:05<12:53, Iter=187, Loss=nan, Step=187, UsedTime=0:23:05, lr=0.094301]Epoch: 1/4: 67%|██████████████▋       |206/309[23:05<12:45, Iter=187, Loss=nan, Step=187, UsedTime=0:23:05, lr=0.094301]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 67%|████████████████████████████████████████████████▋                        |206/309[23:13<12:45, loss=nan]Epoch: 1/4: 67%|██████████████▋       |206/309[23:13<12:45, Iter=188, Loss=nan, Step=188, UsedTime=0:23:13, lr=0.094211]Epoch: 1/4: 67%|██████████████▋       |207/309[23:13<12:38, Iter=188, Loss=nan, Step=188, UsedTime=0:23:13, lr=0.094211]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 67%|████████████████████████████████████████████████▉                        |207/309[23:20<12:38, loss=nan]Epoch: 1/4: 67%|██████████████▋       |207/309[23:20<12:38, Iter=189, Loss=nan, Step=189, UsedTime=0:23:20, lr=0.094121]Epoch: 1/4: 67%|██████████████▊       |208/309[23:20<12:30, Iter=189, Loss=nan, Step=189, UsedTime=0:23:20, lr=0.094121]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 68%|██████████████▉       |209/309[23:24<10:28, Iter=189, Loss=nan, Step=189, UsedTime=0:23:20, lr=0.094121]Epoch: 1/4: 68%|█████████████████████████████████████████████████▍                       |209/309[23:27<10:28, loss=nan]Epoch: 1/4: 68%|██████████████▉       |209/309[23:27<10:28, Iter=190, Loss=nan, Step=190, UsedTime=0:23:27, lr=0.094031]Epoch: 1/4: 68%|██████████████▉       |210/309[23:27<09:10, Iter=190, Loss=nan, Step=190, UsedTime=0:23:27, lr=0.094031]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 68%|█████████████████████████████████████████████████▌                       |210/309[23:35<09:10, loss=nan]Epoch: 1/4: 68%|██████████████▉       |210/309[23:35<09:10, Iter=191, Loss=nan, Step=191, UsedTime=0:23:35, lr=0.093941]Epoch: 1/4: 68%|███████████████       |211/309[23:35<10:01, Iter=191, Loss=nan, Step=191, UsedTime=0:23:35, lr=0.093941]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 68%|█████████████████████████████████████████████████▊                       |211/309[23:42<10:01, loss=nan]Epoch: 1/4: 68%|███████████████       |211/309[23:42<10:01, Iter=192, Loss=nan, Step=192, UsedTime=0:23:42, lr=0.093851]Epoch: 1/4: 69%|███████████████       |212/309[23:42<10:32, Iter=192, Loss=nan, Step=192, UsedTime=0:23:42, lr=0.093851]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 69%|██████████████████████████████████████████████████                       |212/309[23:50<10:32, loss=nan]Epoch: 1/4: 69%|███████████████       |212/309[23:50<10:32, Iter=193, Loss=nan, Step=193, UsedTime=0:23:50, lr=0.093761]Epoch: 1/4: 69%|███████████████▏      |213/309[23:50<10:53, Iter=193, Loss=nan, Step=193, UsedTime=0:23:50, lr=0.093761]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 69%|██████████████████████████████████████████████████▎                      |213/309[23:57<10:53, loss=nan]Epoch: 1/4: 69%|███████████████▏      |213/309[23:57<10:53, Iter=194, Loss=nan, Step=194, UsedTime=0:23:57, lr=0.093671]Epoch: 1/4: 69%|███████████████▏      |214/309[23:57<11:06, Iter=194, Loss=nan, Step=194, UsedTime=0:23:57, lr=0.093671]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 69%|██████████████████████████████████████████████████▌                      |214/309[24:05<11:06, loss=nan]Epoch: 1/4: 69%|███████████████▏      |214/309[24:05<11:06, Iter=195, Loss=nan, Step=195, UsedTime=0:24:05, lr=0.093581]Epoch: 1/4: 70%|███████████████▎      |215/309[24:05<11:10, Iter=195, Loss=nan, Step=195, UsedTime=0:24:05, lr=0.093581]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 70%|██████████████████████████████████████████████████▊                      |215/309[24:12<11:10, loss=nan]Epoch: 1/4: 70%|███████████████▎      |215/309[24:12<11:10, Iter=196, Loss=nan, Step=196, UsedTime=0:24:12, lr=0.093492]Epoch: 1/4: 70%|███████████████▍      |216/309[24:12<11:11, Iter=196, Loss=nan, Step=196, UsedTime=0:24:12, lr=0.093492]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 70%|███████████████████████████████████████████████████                      |216/309[24:20<11:11, loss=nan]Epoch: 1/4: 70%|███████████████▍      |216/309[24:20<11:11, Iter=197, Loss=nan, Step=197, UsedTime=0:24:20, lr=0.093402]Epoch: 1/4: 70%|███████████████▍      |217/309[24:20<11:09, Iter=197, Loss=nan, Step=197, UsedTime=0:24:20, lr=0.093402]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 70%|███████████████████████████████████████████████████▎                     |217/309[24:27<11:09, loss=nan]Epoch: 1/4: 70%|███████████████▍      |217/309[24:27<11:09, Iter=198, Loss=nan, Step=198, UsedTime=0:24:27, lr=0.093312]Epoch: 1/4: 71%|███████████████▌      |218/309[24:27<11:06, Iter=198, Loss=nan, Step=198, UsedTime=0:24:27, lr=0.093312]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 71%|███████████████████████████████████████████████████▌                     |218/309[24:34<11:06, loss=nan]Epoch: 1/4: 71%|███████████████▌      |218/309[24:34<11:06, Iter=199, Loss=nan, Step=199, UsedTime=0:24:34, lr=0.093222]Epoch: 1/4: 71%|███████████████▌      |219/309[24:34<11:02, Iter=199, Loss=nan, Step=199, UsedTime=0:24:34, lr=0.093222]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 71%|███████████████████████████████████████████████████▋                     |219/309[24:42<11:02, loss=nan]Epoch: 1/4: 71%|███████████████▌      |219/309[24:42<11:02, Iter=200, Loss=nan, Step=200, UsedTime=0:24:42, lr=0.093132]Epoch: 1/4: 71%|███████████████▋      |220/309[24:42<10:57, Iter=200, Loss=nan, Step=200, UsedTime=0:24:42, lr=0.093132]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 71%|███████████████████████████████████████████████████▉                     |220/309[24:49<10:57, loss=nan]Epoch: 1/4: 71%|███████████████▋      |220/309[24:49<10:57, Iter=201, Loss=nan, Step=201, UsedTime=0:24:49, lr=0.093042]Epoch: 1/4: 72%|███████████████▋      |221/309[24:49<10:45, Iter=201, Loss=nan, Step=201, UsedTime=0:24:49, lr=0.093042]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 72%|████████████████████████████████████████████████████▏                    |221/309[24:56<10:45, loss=nan]Epoch: 1/4: 72%|███████████████▋      |221/309[24:56<10:45, Iter=202, Loss=nan, Step=202, UsedTime=0:24:56, lr=0.092952]Epoch: 1/4: 72%|███████████████▊      |222/309[24:56<10:38, Iter=202, Loss=nan, Step=202, UsedTime=0:24:56, lr=0.092952]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 72%|████████████████████████████████████████████████████▍                    |222/309[25:04<10:38, loss=nan]Epoch: 1/4: 72%|███████████████▊      |222/309[25:04<10:38, Iter=203, Loss=nan, Step=203, UsedTime=0:25:04, lr=0.092862]Epoch: 1/4: 72%|███████████████▉      |223/309[25:04<10:30, Iter=203, Loss=nan, Step=203, UsedTime=0:25:04, lr=0.092862]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 72%|████████████████████████████████████████████████████▋                    |223/309[25:11<10:30, loss=nan]Epoch: 1/4: 72%|███████████████▉      |223/309[25:11<10:30, Iter=204, Loss=nan, Step=204, UsedTime=0:25:11, lr=0.092772]Epoch: 1/4: 72%|███████████████▉      |224/309[25:11<10:23, Iter=204, Loss=nan, Step=204, UsedTime=0:25:11, lr=0.092772]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 72%|████████████████████████████████████████████████████▉                    |224/309[25:18<10:23, loss=nan]Epoch: 1/4: 72%|███████████████▉      |224/309[25:18<10:23, Iter=205, Loss=nan, Step=205, UsedTime=0:25:18, lr=0.092682]Epoch: 1/4: 73%|████████████████      |225/309[25:18<10:16, Iter=205, Loss=nan, Step=205, UsedTime=0:25:18, lr=0.092682]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 73%|█████████████████████████████████████████████████████▏                   |225/309[25:26<10:16, loss=nan]Epoch: 1/4: 73%|████████████████      |225/309[25:26<10:16, Iter=206, Loss=nan, Step=206, UsedTime=0:25:26, lr=0.092593]Epoch: 1/4: 73%|████████████████      |226/309[25:26<10:08, Iter=206, Loss=nan, Step=206, UsedTime=0:25:26, lr=0.092593]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 73%|█████████████████████████████████████████████████████▍                   |226/309[25:33<10:08, loss=nan]Epoch: 1/4: 73%|████████████████      |226/309[25:33<10:08, Iter=207, Loss=nan, Step=207, UsedTime=0:25:33, lr=0.092503]Epoch: 1/4: 73%|████████████████▏     |227/309[25:33<10:01, Iter=207, Loss=nan, Step=207, UsedTime=0:25:33, lr=0.092503]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 73%|█████████████████████████████████████████████████████▋                   |227/309[25:41<10:01, loss=nan]Epoch: 1/4: 73%|████████████████▏     |227/309[25:41<10:01, Iter=208, Loss=nan, Step=208, UsedTime=0:25:41, lr=0.092413]Epoch: 1/4: 74%|████████████████▏     |228/309[25:41<09:55, Iter=208, Loss=nan, Step=208, UsedTime=0:25:41, lr=0.092413]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 74%|█████████████████████████████████████████████████████▊                   |228/309[25:48<09:55, loss=nan]Epoch: 1/4: 74%|████████████████▏     |228/309[25:48<09:55, Iter=209, Loss=nan, Step=209, UsedTime=0:25:48, lr=0.092323]Epoch: 1/4: 74%|████████████████▎     |229/309[25:48<09:49, Iter=209, Loss=nan, Step=209, UsedTime=0:25:48, lr=0.092323]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 74%|████████████████▍     |230/309[25:51<08:11, Iter=209, Loss=nan, Step=209, UsedTime=0:25:48, lr=0.092323]Epoch: 1/4: 74%|██████████████████████████████████████████████████████▎                  |230/309[25:55<08:11, loss=nan]Epoch: 1/4: 74%|████████████████▍     |230/309[25:55<08:11, Iter=210, Loss=nan, Step=210, UsedTime=0:25:55, lr=0.092233]Epoch: 1/4: 75%|████████████████▍     |231/309[25:55<07:08, Iter=210, Loss=nan, Step=210, UsedTime=0:25:55, lr=0.092233]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 75%|██████████████████████████████████████████████████████▌                  |231/309[26:03<07:08, loss=nan]Epoch: 1/4: 75%|████████████████▍     |231/309[26:03<07:08, Iter=211, Loss=nan, Step=211, UsedTime=0:26:03, lr=0.092143]Epoch: 1/4: 75%|████████████████▌     |232/309[26:03<07:45, Iter=211, Loss=nan, Step=211, UsedTime=0:26:03, lr=0.092143]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 75%|██████████████████████████████████████████████████████▊                  |232/309[26:10<07:45, loss=nan]Epoch: 1/4: 75%|████████████████▌     |232/309[26:10<07:45, Iter=212, Loss=nan, Step=212, UsedTime=0:26:10, lr=0.092053]Epoch: 1/4: 75%|████████████████▌     |233/309[26:10<08:06, Iter=212, Loss=nan, Step=212, UsedTime=0:26:10, lr=0.092053]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 75%|███████████████████████████████████████████████████████                  |233/309[26:17<08:06, loss=nan]Epoch: 1/4: 75%|████████████████▌     |233/309[26:17<08:06, Iter=213, Loss=nan, Step=213, UsedTime=0:26:17, lr=0.091963]Epoch: 1/4: 76%|████████████████▋     |234/309[26:17<08:21, Iter=213, Loss=nan, Step=213, UsedTime=0:26:17, lr=0.091963]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 76%|███████████████████████████████████████████████████████▎                 |234/309[26:24<08:21, loss=nan]Epoch: 1/4: 76%|████████████████▋     |234/309[26:24<08:21, Iter=214, Loss=nan, Step=214, UsedTime=0:26:24, lr=0.091873]Epoch: 1/4: 76%|████████████████▋     |235/309[26:24<08:28, Iter=214, Loss=nan, Step=214, UsedTime=0:26:24, lr=0.091873]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 76%|███████████████████████████████████████████████████████▌                 |235/309[26:32<08:28, loss=nan]Epoch: 1/4: 76%|████████████████▋     |235/309[26:32<08:28, Iter=215, Loss=nan, Step=215, UsedTime=0:26:32, lr=0.091784]Epoch: 1/4: 76%|████████████████▊     |236/309[26:32<08:32, Iter=215, Loss=nan, Step=215, UsedTime=0:26:32, lr=0.091784]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 76%|███████████████████████████████████████████████████████▊                 |236/309[26:39<08:32, loss=nan]Epoch: 1/4: 76%|████████████████▊     |236/309[26:39<08:32, Iter=216, Loss=nan, Step=216, UsedTime=0:26:39, lr=0.091694]Epoch: 1/4: 77%|████████████████▊     |237/309[26:39<08:30, Iter=216, Loss=nan, Step=216, UsedTime=0:26:39, lr=0.091694]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 77%|███████████████████████████████████████████████████████▉                 |237/309[26:47<08:30, loss=nan]Epoch: 1/4: 77%|████████████████▊     |237/309[26:47<08:30, Iter=217, Loss=nan, Step=217, UsedTime=0:26:47, lr=0.091604]Epoch: 1/4: 77%|████████████████▉     |238/309[26:47<08:29, Iter=217, Loss=nan, Step=217, UsedTime=0:26:47, lr=0.091604]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 77%|████████████████████████████████████████████████████████▏                |238/309[26:54<08:29, loss=nan]Epoch: 1/4: 77%|████████████████▉     |238/309[26:54<08:29, Iter=218, Loss=nan, Step=218, UsedTime=0:26:54, lr=0.091514]Epoch: 1/4: 77%|█████████████████     |239/309[26:54<08:26, Iter=218, Loss=nan, Step=218, UsedTime=0:26:54, lr=0.091514]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 77%|████████████████████████████████████████████████████████▍                |239/309[27:01<08:26, loss=nan]Epoch: 1/4: 77%|█████████████████     |239/309[27:01<08:26, Iter=219, Loss=nan, Step=219, UsedTime=0:27:01, lr=0.091424]Epoch: 1/4: 78%|█████████████████     |240/309[27:01<08:22, Iter=219, Loss=nan, Step=219, UsedTime=0:27:01, lr=0.091424]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 78%|████████████████████████████████████████████████████████▋                |240/309[27:09<08:22, loss=nan]Epoch: 1/4: 78%|█████████████████     |240/309[27:09<08:22, Iter=220, Loss=nan, Step=220, UsedTime=0:27:09, lr=0.091334]Epoch: 1/4: 78%|█████████████████▏    |241/309[27:09<08:16, Iter=220, Loss=nan, Step=220, UsedTime=0:27:09, lr=0.091334]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 78%|████████████████████████████████████████████████████████▉                |241/309[27:16<08:16, loss=nan]Epoch: 1/4: 78%|█████████████████▏    |241/309[27:16<08:16, Iter=221, Loss=nan, Step=221, UsedTime=0:27:16, lr=0.091244]Epoch: 1/4: 78%|█████████████████▏    |242/309[27:16<08:11, Iter=221, Loss=nan, Step=221, UsedTime=0:27:16, lr=0.091244]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 78%|█████████████████████████████████████████████████████████▏               |242/309[27:23<08:11, loss=nan]Epoch: 1/4: 78%|█████████████████▏    |242/309[27:23<08:11, Iter=222, Loss=nan, Step=222, UsedTime=0:27:23, lr=0.091154]Epoch: 1/4: 79%|█████████████████▎    |243/309[27:23<08:03, Iter=222, Loss=nan, Step=222, UsedTime=0:27:23, lr=0.091154]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 79%|█████████████████████████████████████████████████████████▍               |243/309[27:31<08:03, loss=nan]Epoch: 1/4: 79%|█████████████████▎    |243/309[27:31<08:03, Iter=223, Loss=nan, Step=223, UsedTime=0:27:31, lr=0.091064]Epoch: 1/4: 79%|█████████████████▎    |244/309[27:31<07:56, Iter=223, Loss=nan, Step=223, UsedTime=0:27:31, lr=0.091064]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 79%|█████████████████████████████████████████████████████████▋               |244/309[27:38<07:56, loss=nan]Epoch: 1/4: 79%|█████████████████▎    |244/309[27:38<07:56, Iter=224, Loss=nan, Step=224, UsedTime=0:27:38, lr=0.090974]Epoch: 1/4: 79%|█████████████████▍    |245/309[27:38<07:50, Iter=224, Loss=nan, Step=224, UsedTime=0:27:38, lr=0.090974]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 79%|█████████████████████████████████████████████████████████▉               |245/309[27:45<07:50, loss=nan]Epoch: 1/4: 79%|█████████████████▍    |245/309[27:45<07:50, Iter=225, Loss=nan, Step=225, UsedTime=0:27:45, lr=0.090885]Epoch: 1/4: 80%|█████████████████▌    |246/309[27:45<07:41, Iter=225, Loss=nan, Step=225, UsedTime=0:27:45, lr=0.090885]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 80%|██████████████████████████████████████████████████████████               |246/309[27:53<07:41, loss=nan]Epoch: 1/4: 80%|█████████████████▌    |246/309[27:53<07:41, Iter=226, Loss=nan, Step=226, UsedTime=0:27:53, lr=0.090795]Epoch: 1/4: 80%|█████████████████▌    |247/309[27:53<07:34, Iter=226, Loss=nan, Step=226, UsedTime=0:27:53, lr=0.090795]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 80%|██████████████████████████████████████████████████████████▎              |247/309[28:00<07:34, loss=nan]Epoch: 1/4: 80%|█████████████████▌    |247/309[28:00<07:34, Iter=227, Loss=nan, Step=227, UsedTime=0:28:00, lr=0.090705]Epoch: 1/4: 80%|█████████████████▋    |248/309[28:00<07:26, Iter=227, Loss=nan, Step=227, UsedTime=0:28:00, lr=0.090705]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 80%|██████████████████████████████████████████████████████████▌              |248/309[28:07<07:26, loss=nan]Epoch: 1/4: 80%|█████████████████▋    |248/309[28:07<07:26, Iter=228, Loss=nan, Step=228, UsedTime=0:28:07, lr=0.090615]Epoch: 1/4: 81%|█████████████████▋    |249/309[28:07<07:18, Iter=228, Loss=nan, Step=228, UsedTime=0:28:07, lr=0.090615]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 81%|██████████████████████████████████████████████████████████▊              |249/309[28:15<07:18, loss=nan]Epoch: 1/4: 81%|█████████████████▋    |249/309[28:15<07:18, Iter=229, Loss=nan, Step=229, UsedTime=0:28:15, lr=0.090525]Epoch: 1/4: 81%|█████████████████▊    |250/309[28:15<07:12, Iter=229, Loss=nan, Step=229, UsedTime=0:28:15, lr=0.090525]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 81%|███████████████████████████████████████████████████████████              |250/309[28:22<07:12, loss=nan]Epoch: 1/4: 81%|█████████████████▊    |250/309[28:22<07:12, Iter=230, Loss=nan, Step=230, UsedTime=0:28:22, lr=0.090435]Epoch: 1/4: 81%|█████████████████▊    |251/309[28:22<07:05, Iter=230, Loss=nan, Step=230, UsedTime=0:28:22, lr=0.090435]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 82%|█████████████████▉    |252/309[28:26<05:53, Iter=230, Loss=nan, Step=230, UsedTime=0:28:22, lr=0.090435]Epoch: 1/4: 82%|███████████████████████████████████████████████████████████▌             |252/309[28:29<05:53, loss=nan]Epoch: 1/4: 82%|█████████████████▉    |252/309[28:29<05:53, Iter=231, Loss=nan, Step=231, UsedTime=0:28:29, lr=0.090345]Epoch: 1/4: 82%|██████████████████    |253/309[28:29<05:06, Iter=231, Loss=nan, Step=231, UsedTime=0:28:29, lr=0.090345]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 82%|███████████████████████████████████████████████████████████▊             |253/309[28:37<05:06, loss=nan]Epoch: 1/4: 82%|██████████████████    |253/309[28:37<05:06, Iter=232, Loss=nan, Step=232, UsedTime=0:28:37, lr=0.090255]Epoch: 1/4: 82%|██████████████████    |254/309[28:37<05:33, Iter=232, Loss=nan, Step=232, UsedTime=0:28:37, lr=0.090255]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 82%|████████████████████████████████████████████████████████████             |254/309[28:44<05:33, loss=nan]Epoch: 1/4: 82%|██████████████████    |254/309[28:44<05:33, Iter=233, Loss=nan, Step=233, UsedTime=0:28:44, lr=0.090165]Epoch: 1/4: 83%|██████████████████▏   |255/309[28:44<05:47, Iter=233, Loss=nan, Step=233, UsedTime=0:28:44, lr=0.090165]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 83%|████████████████████████████████████████████████████████████▏            |255/309[28:51<05:47, loss=nan]Epoch: 1/4: 83%|██████████████████▏   |255/309[28:51<05:47, Iter=234, Loss=nan, Step=234, UsedTime=0:28:51, lr=0.090076]Epoch: 1/4: 83%|██████████████████▏   |256/309[28:51<05:55, Iter=234, Loss=nan, Step=234, UsedTime=0:28:51, lr=0.090076]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 83%|████████████████████████████████████████████████████████████▍            |256/309[28:59<05:55, loss=nan]Epoch: 1/4: 83%|██████████████████▏   |256/309[28:59<05:55, Iter=235, Loss=nan, Step=235, UsedTime=0:28:59, lr=0.089986]Epoch: 1/4: 83%|██████████████████▎   |257/309[28:59<05:57, Iter=235, Loss=nan, Step=235, UsedTime=0:28:59, lr=0.089986]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 83%|████████████████████████████████████████████████████████████▋            |257/309[29:06<05:57, loss=nan]Epoch: 1/4: 83%|██████████████████▎   |257/309[29:06<05:57, Iter=236, Loss=nan, Step=236, UsedTime=0:29:06, lr=0.089896]Epoch: 1/4: 83%|██████████████████▎   |258/309[29:06<05:56, Iter=236, Loss=nan, Step=236, UsedTime=0:29:06, lr=0.089896]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 83%|████████████████████████████████████████████████████████████▉            |258/309[29:13<05:56, loss=nan]Epoch: 1/4: 83%|██████████████████▎   |258/309[29:13<05:56, Iter=237, Loss=nan, Step=237, UsedTime=0:29:13, lr=0.089806]Epoch: 1/4: 84%|██████████████████▍   |259/309[29:13<05:54, Iter=237, Loss=nan, Step=237, UsedTime=0:29:13, lr=0.089806]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 84%|█████████████████████████████████████████████████████████████▏           |259/309[29:21<05:54, loss=nan]Epoch: 1/4: 84%|██████████████████▍   |259/309[29:21<05:54, Iter=238, Loss=nan, Step=238, UsedTime=0:29:21, lr=0.089716]Epoch: 1/4: 84%|██████████████████▌   |260/309[29:21<05:51, Iter=238, Loss=nan, Step=238, UsedTime=0:29:21, lr=0.089716]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 84%|█████████████████████████████████████████████████████████████▍           |260/309[29:28<05:51, loss=nan]Epoch: 1/4: 84%|██████████████████▌   |260/309[29:28<05:51, Iter=239, Loss=nan, Step=239, UsedTime=0:29:28, lr=0.089626]Epoch: 1/4: 84%|██████████████████▌   |261/309[29:28<05:45, Iter=239, Loss=nan, Step=239, UsedTime=0:29:28, lr=0.089626]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 84%|█████████████████████████████████████████████████████████████▋           |261/309[29:35<05:45, loss=nan]Epoch: 1/4: 84%|██████████████████▌   |261/309[29:35<05:45, Iter=240, Loss=nan, Step=240, UsedTime=0:29:35, lr=0.089536]Epoch: 1/4: 85%|██████████████████▋   |262/309[29:35<05:40, Iter=240, Loss=nan, Step=240, UsedTime=0:29:35, lr=0.089536]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 85%|█████████████████████████████████████████████████████████████▉           |262/309[29:43<05:40, loss=nan]Epoch: 1/4: 85%|██████████████████▋   |262/309[29:43<05:40, Iter=241, Loss=nan, Step=241, UsedTime=0:29:43, lr=0.089446]Epoch: 1/4: 85%|██████████████████▋   |263/309[29:43<05:34, Iter=241, Loss=nan, Step=241, UsedTime=0:29:43, lr=0.089446]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 85%|██████████████████████████████████████████████████████████████▏          |263/309[29:50<05:34, loss=nan]Epoch: 1/4: 85%|██████████████████▋   |263/309[29:50<05:34, Iter=242, Loss=nan, Step=242, UsedTime=0:29:50, lr=0.089356]Epoch: 1/4: 85%|██████████████████▊   |264/309[29:50<05:27, Iter=242, Loss=nan, Step=242, UsedTime=0:29:50, lr=0.089356]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 85%|██████████████████████████████████████████████████████████████▎          |264/309[29:57<05:27, loss=nan]Epoch: 1/4: 85%|██████████████████▊   |264/309[29:57<05:27, Iter=243, Loss=nan, Step=243, UsedTime=0:29:57, lr=0.089266]Epoch: 1/4: 86%|██████████████████▊   |265/309[29:57<05:20, Iter=243, Loss=nan, Step=243, UsedTime=0:29:57, lr=0.089266]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 86%|██████████████████████████████████████████████████████████████▌          |265/309[30:05<05:20, loss=nan]Epoch: 1/4: 86%|██████████████████▊   |265/309[30:05<05:20, Iter=244, Loss=nan, Step=244, UsedTime=0:30:05, lr=0.089177]Epoch: 1/4: 86%|██████████████████▉   |266/309[30:05<05:13, Iter=244, Loss=nan, Step=244, UsedTime=0:30:05, lr=0.089177]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 86%|██████████████████████████████████████████████████████████████▊          |266/309[30:12<05:13, loss=nan]Epoch: 1/4: 86%|██████████████████▉   |266/309[30:12<05:13, Iter=245, Loss=nan, Step=245, UsedTime=0:30:12, lr=0.089087]Epoch: 1/4: 86%|███████████████████   |267/309[30:12<05:07, Iter=245, Loss=nan, Step=245, UsedTime=0:30:12, lr=0.089087]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 86%|███████████████████████████████████████████████████████████████          |267/309[30:19<05:07, loss=nan]Epoch: 1/4: 86%|███████████████████   |267/309[30:19<05:07, Iter=246, Loss=nan, Step=246, UsedTime=0:30:19, lr=0.088997]Epoch: 1/4: 87%|███████████████████   |268/309[30:19<04:59, Iter=246, Loss=nan, Step=246, UsedTime=0:30:19, lr=0.088997]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 87%|███████████████████████████████████████████████████████████████▎         |268/309[30:26<04:59, loss=nan]Epoch: 1/4: 87%|███████████████████   |268/309[30:26<04:59, Iter=247, Loss=nan, Step=247, UsedTime=0:30:26, lr=0.088907]Epoch: 1/4: 87%|███████████████████▏  |269/309[30:26<04:52, Iter=247, Loss=nan, Step=247, UsedTime=0:30:26, lr=0.088907]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 87%|███████████████████████████████████████████████████████████████▌         |269/309[30:34<04:52, loss=nan]Epoch: 1/4: 87%|███████████████████▏  |269/309[30:34<04:52, Iter=248, Loss=nan, Step=248, UsedTime=0:30:34, lr=0.088817]Epoch: 1/4: 87%|███████████████████▏  |270/309[30:34<04:45, Iter=248, Loss=nan, Step=248, UsedTime=0:30:34, lr=0.088817]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 87%|███████████████████████████████████████████████████████████████▊         |270/309[30:41<04:45, loss=nan]Epoch: 1/4: 87%|███████████████████▏  |270/309[30:41<04:45, Iter=249, Loss=nan, Step=249, UsedTime=0:30:41, lr=0.088727]Epoch: 1/4: 88%|███████████████████▎  |271/309[30:41<04:38, Iter=249, Loss=nan, Step=249, UsedTime=0:30:41, lr=0.088727]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 88%|████████████████████████████████████████████████████████████████         |271/309[30:48<04:38, loss=nan]Epoch: 1/4: 88%|███████████████████▎  |271/309[30:48<04:38, Iter=250, Loss=nan, Step=250, UsedTime=0:30:48, lr=0.088637]Epoch: 1/4: 88%|███████████████████▎  |272/309[30:48<04:31, Iter=250, Loss=nan, Step=250, UsedTime=0:30:48, lr=0.088637]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 88%|████████████████████████████████████████████████████████████████▎        |272/309[30:56<04:31, loss=nan]Epoch: 1/4: 88%|███████████████████▎  |272/309[30:56<04:31, Iter=251, Loss=nan, Step=251, UsedTime=0:30:56, lr=0.088547]Epoch: 1/4: 88%|███████████████████▍  |273/309[30:56<04:24, Iter=251, Loss=nan, Step=251, UsedTime=0:30:56, lr=0.088547]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 88%|████████████████████████████████████████████████████████████████▍        |273/309[31:03<04:24, loss=nan]Epoch: 1/4: 88%|███████████████████▍  |273/309[31:03<04:24, Iter=252, Loss=nan, Step=252, UsedTime=0:31:03, lr=0.088457]Epoch: 1/4: 89%|███████████████████▌  |274/309[31:03<04:16, Iter=252, Loss=nan, Step=252, UsedTime=0:31:03, lr=0.088457]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 89%|███████████████████▌  |275/309[31:07<03:31, Iter=252, Loss=nan, Step=252, UsedTime=0:31:03, lr=0.088457]Epoch: 1/4: 89%|████████████████████████████████████████████████████████████████▉        |275/309[31:11<03:31, loss=nan]Epoch: 1/4: 89%|███████████████████▌  |275/309[31:11<03:31, Iter=253, Loss=nan, Step=253, UsedTime=0:31:11, lr=0.088367]Epoch: 1/4: 89%|███████████████████▋  |276/309[31:11<03:01, Iter=253, Loss=nan, Step=253, UsedTime=0:31:11, lr=0.088367]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 89%|█████████████████████████████████████████████████████████████████▏       |276/309[31:18<03:01, loss=nan]Epoch: 1/4: 89%|███████████████████▋  |276/309[31:18<03:01, Iter=254, Loss=nan, Step=254, UsedTime=0:31:18, lr=0.088278]Epoch: 1/4: 90%|███████████████████▋  |277/309[31:18<03:12, Iter=254, Loss=nan, Step=254, UsedTime=0:31:18, lr=0.088278]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 90%|█████████████████████████████████████████████████████████████████▍       |277/309[31:25<03:12, loss=nan]Epoch: 1/4: 90%|███████████████████▋  |277/309[31:25<03:12, Iter=255, Loss=nan, Step=255, UsedTime=0:31:25, lr=0.088188]Epoch: 1/4: 90%|███████████████████▊  |278/309[31:25<03:18, Iter=255, Loss=nan, Step=255, UsedTime=0:31:25, lr=0.088188]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 90%|█████████████████████████████████████████████████████████████████▋       |278/309[31:32<03:18, loss=nan]Epoch: 1/4: 90%|███████████████████▊  |278/309[31:32<03:18, Iter=256, Loss=nan, Step=256, UsedTime=0:31:32, lr=0.088098]Epoch: 1/4: 90%|███████████████████▊  |279/309[31:32<03:20, Iter=256, Loss=nan, Step=256, UsedTime=0:31:32, lr=0.088098]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 90%|█████████████████████████████████████████████████████████████████▉       |279/309[31:40<03:20, loss=nan]Epoch: 1/4: 90%|███████████████████▊  |279/309[31:40<03:20, Iter=257, Loss=nan, Step=257, UsedTime=0:31:40, lr=0.088008]Epoch: 1/4: 91%|███████████████████▉  |280/309[31:40<03:18, Iter=257, Loss=nan, Step=257, UsedTime=0:31:40, lr=0.088008]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 91%|██████████████████████████████████████████████████████████████████▏      |280/309[31:47<03:18, loss=nan]Epoch: 1/4: 91%|███████████████████▉  |280/309[31:47<03:18, Iter=258, Loss=nan, Step=258, UsedTime=0:31:47, lr=0.087918]Epoch: 1/4: 91%|████████████████████  |281/309[31:47<03:16, Iter=258, Loss=nan, Step=258, UsedTime=0:31:47, lr=0.087918]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 91%|██████████████████████████████████████████████████████████████████▍      |281/309[31:54<03:16, loss=nan]Epoch: 1/4: 91%|████████████████████  |281/309[31:54<03:16, Iter=259, Loss=nan, Step=259, UsedTime=0:31:54, lr=0.087828]Epoch: 1/4: 91%|████████████████████  |282/309[31:54<03:11, Iter=259, Loss=nan, Step=259, UsedTime=0:31:54, lr=0.087828]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 91%|██████████████████████████████████████████████████████████████████▌      |282/309[32:02<03:11, loss=nan]Epoch: 1/4: 91%|████████████████████  |282/309[32:02<03:11, Iter=260, Loss=nan, Step=260, UsedTime=0:32:02, lr=0.087738]Epoch: 1/4: 92%|████████████████████▏ |283/309[32:02<03:05, Iter=260, Loss=nan, Step=260, UsedTime=0:32:02, lr=0.087738]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 92%|██████████████████████████████████████████████████████████████████▊      |283/309[32:09<03:05, loss=nan]Epoch: 1/4: 92%|████████████████████▏ |283/309[32:09<03:05, Iter=261, Loss=nan, Step=261, UsedTime=0:32:09, lr=0.087648]Epoch: 1/4: 92%|████████████████████▏ |284/309[32:09<03:00, Iter=261, Loss=nan, Step=261, UsedTime=0:32:09, lr=0.087648]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 92%|███████████████████████████████████████████████████████████████████      |284/309[32:16<03:00, loss=nan]Epoch: 1/4: 92%|████████████████████▏ |284/309[32:16<03:00, Iter=262, Loss=nan, Step=262, UsedTime=0:32:16, lr=0.087558]Epoch: 1/4: 92%|████████████████████▎ |285/309[32:16<02:53, Iter=262, Loss=nan, Step=262, UsedTime=0:32:16, lr=0.087558]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 92%|███████████████████████████████████████████████████████████████████▎     |285/309[32:24<02:53, loss=nan]Epoch: 1/4: 92%|████████████████████▎ |285/309[32:24<02:53, Iter=263, Loss=nan, Step=263, UsedTime=0:32:24, lr=0.087469]Epoch: 1/4: 93%|████████████████████▎ |286/309[32:24<02:47, Iter=263, Loss=nan, Step=263, UsedTime=0:32:24, lr=0.087469]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 93%|███████████████████████████████████████████████████████████████████▌     |286/309[32:31<02:47, loss=nan]Epoch: 1/4: 93%|████████████████████▎ |286/309[32:31<02:47, Iter=264, Loss=nan, Step=264, UsedTime=0:32:31, lr=0.087379]Epoch: 1/4: 93%|████████████████████▍ |287/309[32:31<02:40, Iter=264, Loss=nan, Step=264, UsedTime=0:32:31, lr=0.087379]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 93%|███████████████████████████████████████████████████████████████████▊     |287/309[32:38<02:40, loss=nan]Epoch: 1/4: 93%|████████████████████▍ |287/309[32:38<02:40, Iter=265, Loss=nan, Step=265, UsedTime=0:32:38, lr=0.087289]Epoch: 1/4: 93%|████████████████████▌ |288/309[32:38<02:33, Iter=265, Loss=nan, Step=265, UsedTime=0:32:38, lr=0.087289]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 93%|████████████████████████████████████████████████████████████████████     |288/309[32:46<02:33, loss=nan]Epoch: 1/4: 93%|████████████████████▌ |288/309[32:46<02:33, Iter=266, Loss=nan, Step=266, UsedTime=0:32:46, lr=0.087199]Epoch: 1/4: 94%|████████████████████▌ |289/309[32:46<02:26, Iter=266, Loss=nan, Step=266, UsedTime=0:32:46, lr=0.087199]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 94%|████████████████████████████████████████████████████████████████████▎    |289/309[32:53<02:26, loss=nan]Epoch: 1/4: 94%|████████████████████▌ |289/309[32:53<02:26, Iter=267, Loss=nan, Step=267, UsedTime=0:32:53, lr=0.087109]Epoch: 1/4: 94%|████████████████████▋ |290/309[32:53<02:19, Iter=267, Loss=nan, Step=267, UsedTime=0:32:53, lr=0.087109]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 94%|████████████████████████████████████████████████████████████████████▌    |290/309[33:00<02:19, loss=nan]Epoch: 1/4: 94%|████████████████████▋ |290/309[33:00<02:19, Iter=268, Loss=nan, Step=268, UsedTime=0:33:00, lr=0.087019]Epoch: 1/4: 94%|████████████████████▋ |291/309[33:00<02:12, Iter=268, Loss=nan, Step=268, UsedTime=0:33:00, lr=0.087019]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 94%|████████████████████████████████████████████████████████████████████▋    |291/309[33:08<02:12, loss=nan]Epoch: 1/4: 94%|████████████████████▋ |291/309[33:08<02:12, Iter=269, Loss=nan, Step=269, UsedTime=0:33:08, lr=0.086929]Epoch: 1/4: 94%|████████████████████▊ |292/309[33:08<02:04, Iter=269, Loss=nan, Step=269, UsedTime=0:33:08, lr=0.086929]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 94%|████████████████████████████████████████████████████████████████████▉    |292/309[33:15<02:04, loss=nan]Epoch: 1/4: 94%|████████████████████▊ |292/309[33:15<02:04, Iter=270, Loss=nan, Step=270, UsedTime=0:33:15, lr=0.086839]Epoch: 1/4: 95%|████████████████████▊ |293/309[33:15<01:57, Iter=270, Loss=nan, Step=270, UsedTime=0:33:15, lr=0.086839]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 95%|█████████████████████████████████████████████████████████████████████▏   |293/309[33:22<01:57, loss=nan]Epoch: 1/4: 95%|████████████████████▊ |293/309[33:22<01:57, Iter=271, Loss=nan, Step=271, UsedTime=0:33:22, lr=0.086749]Epoch: 1/4: 95%|████████████████████▉ |294/309[33:22<01:50, Iter=271, Loss=nan, Step=271, UsedTime=0:33:22, lr=0.086749]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 95%|█████████████████████████████████████████████████████████████████████▍   |294/309[33:30<01:50, loss=nan]Epoch: 1/4: 95%|████████████████████▉ |294/309[33:30<01:50, Iter=272, Loss=nan, Step=272, UsedTime=0:33:30, lr=0.086659]Epoch: 1/4: 95%|█████████████████████ |295/309[33:30<01:42, Iter=272, Loss=nan, Step=272, UsedTime=0:33:30, lr=0.086659]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 95%|█████████████████████████████████████████████████████████████████████▋   |295/309[33:37<01:42, loss=nan]Epoch: 1/4: 95%|█████████████████████ |295/309[33:37<01:42, Iter=273, Loss=nan, Step=273, UsedTime=0:33:37, lr=0.086570]Epoch: 1/4: 96%|█████████████████████ |296/309[33:37<01:34, Iter=273, Loss=nan, Step=273, UsedTime=0:33:37, lr=0.086570]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 96%|█████████████████████████████████████████████████████████████████████▉   |296/309[33:44<01:34, loss=nan]Epoch: 1/4: 96%|█████████████████████ |296/309[33:44<01:34, Iter=274, Loss=nan, Step=274, UsedTime=0:33:44, lr=0.086480]Epoch: 1/4: 96%|█████████████████████▏|297/309[33:44<01:27, Iter=274, Loss=nan, Step=274, UsedTime=0:33:44, lr=0.086480]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 96%|██████████████████████████████████████████████████████████████████████▏  |297/309[33:52<01:27, loss=nan]Epoch: 1/4: 96%|█████████████████████▏|297/309[33:52<01:27, Iter=275, Loss=nan, Step=275, UsedTime=0:33:52, lr=0.086390]Epoch: 1/4: 96%|█████████████████████▏|298/309[33:52<01:20, Iter=275, Loss=nan, Step=275, UsedTime=0:33:52, lr=0.086390]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 97%|█████████████████████▎|299/309[33:55<01:01, Iter=275, Loss=nan, Step=275, UsedTime=0:33:52, lr=0.086390]Epoch: 1/4: 97%|██████████████████████████████████████████████████████████████████████▋  |299/309[33:59<01:01, loss=nan]Epoch: 1/4: 97%|█████████████████████▎|299/309[33:59<01:01, Iter=276, Loss=nan, Step=276, UsedTime=0:33:59, lr=0.086300]Epoch: 1/4: 97%|█████████████████████▎|300/309[33:59<00:49, Iter=276, Loss=nan, Step=276, UsedTime=0:33:59, lr=0.086300]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 97%|██████████████████████████████████████████████████████████████████████▊  |300/309[34:06<00:49, loss=nan]Epoch: 1/4: 97%|█████████████████████▎|300/309[34:06<00:49, Iter=277, Loss=nan, Step=277, UsedTime=0:34:06, lr=0.086210]Epoch: 1/4: 97%|█████████████████████▍|301/309[34:06<00:48, Iter=277, Loss=nan, Step=277, UsedTime=0:34:06, lr=0.086210]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 97%|███████████████████████████████████████████████████████████████████████  |301/309[34:14<00:48, loss=nan]Epoch: 1/4: 97%|█████████████████████▍|301/309[34:14<00:48, Iter=278, Loss=nan, Step=278, UsedTime=0:34:14, lr=0.086120]Epoch: 1/4: 98%|█████████████████████▌|302/309[34:14<00:45, Iter=278, Loss=nan, Step=278, UsedTime=0:34:14, lr=0.086120]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 98%|███████████████████████████████████████████████████████████████████████▎ |302/309[34:21<00:45, loss=nan]Epoch: 1/4: 98%|█████████████████████▌|302/309[34:21<00:45, Iter=279, Loss=nan, Step=279, UsedTime=0:34:21, lr=0.086030]Epoch: 1/4: 98%|█████████████████████▌|303/309[34:21<00:40, Iter=279, Loss=nan, Step=279, UsedTime=0:34:21, lr=0.086030]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 98%|███████████████████████████████████████████████████████████████████████▌ |303/309[34:28<00:40, loss=nan]Epoch: 1/4: 98%|█████████████████████▌|303/309[34:28<00:40, Iter=280, Loss=nan, Step=280, UsedTime=0:34:28, lr=0.085940]Epoch: 1/4: 98%|█████████████████████▋|304/309[34:28<00:34, Iter=280, Loss=nan, Step=280, UsedTime=0:34:28, lr=0.085940]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 98%|███████████████████████████████████████████████████████████████████████▊ |304/309[34:36<00:34, loss=nan]Epoch: 1/4: 98%|█████████████████████▋|304/309[34:36<00:34, Iter=281, Loss=nan, Step=281, UsedTime=0:34:36, lr=0.085850]Epoch: 1/4: 99%|█████████████████████▋|305/309[34:36<00:28, Iter=281, Loss=nan, Step=281, UsedTime=0:34:36, lr=0.085850]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 99%|████████████████████████████████████████████████████████████████████████ |305/309[34:43<00:28, loss=nan]Epoch: 1/4: 99%|█████████████████████▋|305/309[34:43<00:28, Iter=282, Loss=nan, Step=282, UsedTime=0:34:43, lr=0.085761]Epoch: 1/4: 99%|█████████████████████▊|306/309[34:43<00:21, Iter=282, Loss=nan, Step=282, UsedTime=0:34:43, lr=0.085761]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 99%|████████████████████████████████████████████████████████████████████████▎|306/309[34:50<00:21, loss=nan]Epoch: 1/4: 99%|█████████████████████▊|306/309[34:50<00:21, Iter=283, Loss=nan, Step=283, UsedTime=0:34:50, lr=0.085671]Epoch: 1/4: 99%|█████████████████████▊|307/309[34:50<00:14, Iter=283, Loss=nan, Step=283, UsedTime=0:34:50, lr=0.085671]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 99%|████████████████████████████████████████████████████████████████████████▌|307/309[34:58<00:14, loss=nan]Epoch: 1/4: 99%|█████████████████████▊|307/309[34:58<00:14, Iter=284, Loss=nan, Step=284, UsedTime=0:34:58, lr=0.085581]Epoch: 1/4: 100%|████████████████████▉|308/309[34:58<00:07, Iter=284, Loss=nan, Step=284, UsedTime=0:34:58, lr=0.085581]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 100%|███████████████████████████████████████████████████████████████████████▊|308/309[35:05<00:07, loss=nan]Epoch: 1/4: 100%|████████████████████▉|308/309[35:05<00:07, Iter=285, Loss=nan, Step=285, UsedTime=0:35:05, lr=0.085491]Epoch: 1/4: 100%|█████████████████████|309/309[35:05<00:00, Iter=285, Loss=nan, Step=285, UsedTime=0:35:05, lr=0.085491]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 100%|████████████████████████████████████████████████████████████████████████|309/309[35:12<00:00, loss=nan]Epoch: 1/4: 100%|█████████████████████|309/309[35:12<00:00, Iter=286, Loss=nan, Step=286, UsedTime=0:35:12, lr=0.085401]Epoch: 1/4:  0%|                        |310/?[35:12<00:00, Iter=286, Loss=nan, Step=286, UsedTime=0:35:12, lr=0.085401]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4:  0%|                                                                           |310/?[35:19<00:00, loss=nan]Epoch: 1/4:  0%|                        |310/?[35:19<00:00, Iter=287, Loss=nan, Step=287, UsedTime=0:35:19, lr=0.085311]Epoch: 1/4:  0%|                        |311/?[35:19<00:00, Iter=287, Loss=nan, Step=287, UsedTime=0:35:19, lr=0.085311]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4:  0%|                                                                           |311/?[35:27<00:00, loss=nan]Epoch: 1/4:  0%|                        |311/?[35:27<00:00, Iter=288, Loss=nan, Step=288, UsedTime=0:35:27, lr=0.085221]Epoch: 1/4:  0%|                        |312/?[35:27<00:00, Iter=288, Loss=nan, Step=288, UsedTime=0:35:27, lr=0.085221]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4:  0%|                                                                           |312/?[35:34<00:00, loss=nan]Epoch: 1/4:  0%|                        |312/?[35:34<00:00, Iter=289, Loss=nan, Step=289, UsedTime=0:35:34, lr=0.085131]Epoch: 1/4:  0%|                        |313/?[35:34<00:00, Iter=289, Loss=nan, Step=289, UsedTime=0:35:34, lr=0.085131]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4:  0%|                                                                           |313/?[35:41<00:00, loss=nan]Epoch: 1/4:  0%|                        |313/?[35:41<00:00, Iter=290, Loss=nan, Step=290, UsedTime=0:35:41, lr=0.085041]Epoch: 1/4:  0%|                        |314/?[35:41<00:00, Iter=290, Loss=nan, Step=290, UsedTime=0:35:41, lr=0.085041]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4:  0%|                                                                           |314/?[35:49<00:00, loss=nan]Epoch: 1/4:  0%|                        |314/?[35:49<00:00, Iter=291, Loss=nan, Step=291, UsedTime=0:35:49, lr=0.084951]Epoch: 1/4:  0%|                        |315/?[35:49<00:00, Iter=291, Loss=nan, Step=291, UsedTime=0:35:49, lr=0.084951]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4:  0%|                                                                           |315/?[35:56<00:00, loss=nan]Epoch: 1/4:  0%|                        |315/?[35:56<00:00, Iter=292, Loss=nan, Step=292, UsedTime=0:35:56, lr=0.084862]Epoch: 1/4:  0%|                        |316/?[35:56<00:00, Iter=292, Loss=nan, Step=292, UsedTime=0:35:56, lr=0.084862]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4:  0%|                                                                           |316/?[36:03<00:00, loss=nan]Epoch: 1/4:  0%|                        |316/?[36:03<00:00, Iter=293, Loss=nan, Step=293, UsedTime=0:36:03, lr=0.084772]Epoch: 1/4:  0%|                        |317/?[36:03<00:00, Iter=293, Loss=nan, Step=293, UsedTime=0:36:03, lr=0.084772]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4:  0%|                                                                           |317/?[36:11<00:00, loss=nan]Epoch: 1/4:  0%|                        |317/?[36:11<00:00, Iter=294, Loss=nan, Step=294, UsedTime=0:36:11, lr=0.084682]Epoch: 1/4:  0%|                        |318/?[36:11<00:00, Iter=294, Loss=nan, Step=294, UsedTime=0:36:11, lr=0.084682]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4:  0%|                                                                           |318/?[36:18<00:00, loss=nan]Epoch: 1/4:  0%|                        |318/?[36:18<00:00, Iter=295, Loss=nan, Step=295, UsedTime=0:36:18, lr=0.084592]Epoch: 1/4:  0%|                        |319/?[36:18<00:00, Iter=295, Loss=nan, Step=295, UsedTime=0:36:18, lr=0.084592]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4:  0%|                                                                           |319/?[36:25<00:00, loss=nan]Epoch: 1/4:  0%|                        |319/?[36:25<00:00, Iter=296, Loss=nan, Step=296, UsedTime=0:36:25, lr=0.084502]Epoch: 1/4:  0%|                        |320/?[36:25<00:00, Iter=296, Loss=nan, Step=296, UsedTime=0:36:25, lr=0.084502]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4:  0%|                                                                           |320/?[36:33<00:00, loss=nan]Epoch: 1/4:  0%|                        |320/?[36:33<00:00, Iter=297, Loss=nan, Step=297, UsedTime=0:36:33, lr=0.084412]Epoch: 1/4:  0%|                        |321/?[36:33<00:00, Iter=297, Loss=nan, Step=297, UsedTime=0:36:33, lr=0.084412]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4:  0%|                                                                           |321/?[36:40<00:00, loss=nan]Epoch: 1/4:  0%|                        |321/?[36:40<00:00, Iter=298, Loss=nan, Step=298, UsedTime=0:36:40, lr=0.084322]Epoch: 1/4:  0%|                        |322/?[36:40<00:00, Iter=298, Loss=nan, Step=298, UsedTime=0:36:40, lr=0.084322]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4:  0%|                                                                           |322/?[36:47<00:00, loss=nan]Epoch: 1/4:  0%|                        |322/?[36:47<00:00, Iter=299, Loss=nan, Step=299, UsedTime=0:36:47, lr=0.084232]Epoch: 1/4:  0%|                        |323/?[36:47<00:00, Iter=299, Loss=nan, Step=299, UsedTime=0:36:47, lr=0.084232]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4:  0%|                        |324/?[36:51<00:00, Iter=299, Loss=nan, Step=299, UsedTime=0:36:47, lr=0.084232]Epoch: 1/4:  0%|                                                                           |324/?[36:55<00:00, loss=nan]Epoch: 1/4:  0%|                        |324/?[36:55<00:00, Iter=300, Loss=nan, Step=300, UsedTime=0:36:55, lr=0.084142]Epoch: 1/4:  0%|                        |325/?[36:55<00:00, Iter=300, Loss=nan, Step=300, UsedTime=0:36:55, lr=0.084142]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4:  0%|                                                                           |325/?[37:02<00:00, loss=nan]Epoch: 1/4:  0%|                        |325/?[37:02<00:00, Iter=301, Loss=nan, Step=301, UsedTime=0:37:02, lr=0.084052]Epoch: 1/4:  0%|                        |326/?[37:02<00:00, Iter=301, Loss=nan, Step=301, UsedTime=0:37:02, lr=0.084052]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4:  0%|                                                                           |326/?[37:10<00:00, loss=nan]Epoch: 1/4:  0%|                        |326/?[37:10<00:00, Iter=302, Loss=nan, Step=302, UsedTime=0:37:10, lr=0.083963]Epoch: 1/4:  0%|                        |327/?[37:10<00:00, Iter=302, Loss=nan, Step=302, UsedTime=0:37:10, lr=0.083963]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4:  0%|                                                                           |327/?[37:17<00:00, loss=nan]Epoch: 1/4:  0%|                        |327/?[37:17<00:00, Iter=303, Loss=nan, Step=303, UsedTime=0:37:17, lr=0.083873]Epoch: 1/4:  0%|                        |328/?[37:17<00:00, Iter=303, Loss=nan, Step=303, UsedTime=0:37:17, lr=0.083873]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4:  0%|                                                                           |328/?[37:24<00:00, loss=nan]Epoch: 1/4:  0%|                        |328/?[37:24<00:00, Iter=304, Loss=nan, Step=304, UsedTime=0:37:24, lr=0.083783]Epoch: 1/4:  0%|                        |329/?[37:24<00:00, Iter=304, Loss=nan, Step=304, UsedTime=0:37:24, lr=0.083783]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4:  0%|                                                                           |329/?[37:32<00:00, loss=nan]Epoch: 1/4:  0%|                        |329/?[37:32<00:00, Iter=305, Loss=nan, Step=305, UsedTime=0:37:32, lr=0.083693]Epoch: 1/4:  0%|                        |330/?[37:32<00:00, Iter=305, Loss=nan, Step=305, UsedTime=0:37:32, lr=0.083693]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4:  0%|                                                                           |330/?[37:39<00:00, loss=nan]Epoch: 1/4:  0%|                        |330/?[37:39<00:00, Iter=306, Loss=nan, Step=306, UsedTime=0:37:39, lr=0.083603]Epoch: 1/4:  0%|                        |331/?[37:39<00:00, Iter=306, Loss=nan, Step=306, UsedTime=0:37:39, lr=0.083603]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4:  0%|                                                                           |331/?[37:46<00:00, loss=nan]Epoch: 1/4:  0%|                        |331/?[37:46<00:00, Iter=307, Loss=nan, Step=307, UsedTime=0:37:46, lr=0.083513]Epoch: 1/4:  0%|                        |332/?[37:46<00:00, Iter=307, Loss=nan, Step=307, UsedTime=0:37:46, lr=0.083513]WARNING:root:NaN or Inf found in input tensor.
Epoch: 1/4: 100%|████████████████████▉|308/309[37:53<00:07, Iter=307, Loss=nan, Step=307, UsedTime=0:37:46, lr=0.083513]
Traceback (most recent call last):
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/runpy.py", line 183, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/runpy.py", line 109, in _get_module_details
    __import__(pkg_name)
  File "/data2/maqi/LongTextDatasets/LongTextModels/main/main.py", line 12, in <module>
    train_begin_time=train_begin)
  File "/data2/maqi/LongTextDatasets/LongTextModels/model/trainer.py", line 304, in run_train_epoch
    loss, _ = self.model(**inputs)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    return self.gather(outputs, self.output_device)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 180, in gather
    return gather(outputs, output_device, dim=self.dim)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py", line 76, in gather
    res = gather_map(outputs)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py", line 71, in gather_map
    return type(out)(map(gather_map, zip(*outputs)))
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py", line 71, in gather_map
    return type(out)(map(gather_map, zip(*outputs)))
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py", line 63, in gather_map
    return Gather.apply(target_device, dim, *outputs)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/parallel/_functions.py", line 72, in forward
    return comm.gather(inputs, ctx.dim, ctx.target_device)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/parallel/comm.py", line 235, in gather
    return torch._C._gather(tensors, dim, destination)
RuntimeError: Input tensor at index 2 has invalid shape [6, 3, 384], but expected [6, 5, 384]
nohup: ignoring input
Traceback (most recent call last):
  File "/home/omnisky/anaconda3/lib/python3.6/runpy.py", line 183, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File "/home/omnisky/anaconda3/lib/python3.6/runpy.py", line 109, in _get_module_details
    __import__(pkg_name)
  File "/data2/maqi/LongTextDatasets/LongTextModels/main/main.py", line 5, in <module>
    from LongTextModels.model.trainer import Trainer
  File "/data2/maqi/LongTextDatasets/LongTextModels/model/trainer.py", line 7, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'
创建文件夹...
nohup: ignoring input
Some weights of the model checkpoint at /data2/wangbingchao/database/bert_pretrained/bert-base-uncased were not used when initializing SentenceChoice: ['bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'cls.predictions.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'cls.predictions.decoder.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.embeddings.position_embeddings.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight']
- This IS expected if you are initializing SentenceChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SentenceChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of SentenceChoice were not initialized from the model checkpoint at /data2/wangbingchao/database/bert_pretrained/bert-base-uncased and are newly initialized: ['q_lstm.bias_ih_l1_reverse', 'c_lstm.2.weight_hh_l2_reverse', 'c_lstm.0.bias_hh_l2_reverse', 'c_lstm.1.bias_hh_l2_reverse', 'c_lstm.1.bias_ih_l1', 'q_lstm.weight_ih_l1_reverse', 'q_lstm.bias_hh_l0', 'c_lstm.0.weight_hh_l1_reverse', 'liner.affine.weight', 'c_lstm.0.bias_ih_l2_reverse', 'q_lstm.weight_hh_l0', 'c_lstm.1.weight_ih_l1', 'q_lstm.weight_hh_l2_reverse', 'c_lstm.0.weight_ih_l2_reverse', 'c_lstm.2.bias_hh_l1_reverse', 'c_lstm.2.weight_hh_l1_reverse', 'attention.q_proj.bias', 'c_lstm.2.bias_hh_l2', 'c_lstm.1.bias_ih_l0', 'c_lstm.0.bias_ih_l1_reverse', 'c_lstm.0.bias_ih_l0_reverse', 'c_lstm.2.bias_hh_l0_reverse', 'attention.q_proj.weight', 'c_lstm.0.bias_hh_l0', 'c_lstm.0.weight_ih_l2', 'c_lstm.1.bias_hh_l0_reverse', 'c_lstm.0.bias_hh_l2', 'liner.LayerNorm.beta', 'q_lstm.weight_ih_l2', 'c_lstm.2.bias_ih_l1_reverse', 'c_lstm.2.weight_hh_l0', 'c_lstm.1.weight_ih_l0_reverse', 'c_lstm.0.weight_hh_l0_reverse', 'c_lstm.1.weight_hh_l0', 'c_lstm.1.weight_hh_l1', 'c_lstm.1.weight_ih_l2_reverse', 'c_lstm.2.bias_ih_l1', 'liner.dense.bias', 'c_lstm.2.weight_ih_l2_reverse', 'liner.dense.weight', 'c_lstm.2.weight_hh_l0_reverse', 'q_lstm.weight_ih_l0_reverse', 'c_lstm.1.weight_hh_l2', 'q_lstm.bias_hh_l2', 'attention.out_proj.weight', 'q_lstm.bias_ih_l2_reverse', 'c_lstm.1.weight_hh_l2_reverse', 'c_lstm.1.weight_ih_l0', 'c_lstm.1.weight_ih_l2', 'c_lstm.2.weight_ih_l0_reverse', 'c_lstm.0.weight_hh_l1', 'c_lstm.2.weight_ih_l2', 'liner.affine.bias', 'q_lstm.weight_hh_l1_reverse', 'attention.v_proj.bias', 'q_lstm.bias_ih_l2', 'q_lstm.bias_hh_l1', 'c_lstm.2.bias_ih_l0_reverse', 'q_lstm.bias_hh_l0_reverse', 'c_lstm.1.bias_ih_l0_reverse', 'q_lstm.bias_ih_l1', 'c_lstm.0.bias_ih_l1', 'c_lstm.1.bias_hh_l2', 'c_lstm.1.weight_hh_l1_reverse', 'liner.LayerNorm.gamma', 'q_lstm.bias_ih_l0_reverse', 'q_lstm.weight_hh_l0_reverse', 'c_lstm.1.bias_ih_l1_reverse', 'c_lstm.1.weight_ih_l1_reverse', 'c_lstm.1.bias_ih_l2', 'c_lstm.2.weight_ih_l1_reverse', 'c_lstm.2.weight_hh_l1', 'c_lstm.0.weight_ih_l1', 'bert.weight', 'c_lstm.0.weight_ih_l0', 'c_lstm.2.bias_ih_l2_reverse', 'c_lstm.0.bias_ih_l0', 'c_lstm.1.bias_ih_l2_reverse', 'c_lstm.2.bias_ih_l2', 'attention.k_proj.bias', 'q_lstm.bias_hh_l2_reverse', 'c_lstm.0.bias_hh_l0_reverse', 'c_lstm.2.bias_hh_l2_reverse', 'q_lstm.weight_ih_l2_reverse', 'attention.out_proj.bias', 'c_lstm.2.bias_hh_l1', 'c_lstm.0.weight_hh_l2', 'q_lstm.weight_ih_l0', 'c_lstm.2.bias_ih_l0', 'c_lstm.0.bias_ih_l2', 'c_lstm.1.weight_hh_l0_reverse', 'c_lstm.0.bias_hh_l1_reverse', 'c_lstm.0.weight_ih_l0_reverse', 'c_lstm.1.bias_hh_l1', 'c_lstm.0.weight_hh_l0', 'q_lstm.weight_hh_l1', 'c_lstm.2.bias_hh_l0', 'attention.k_proj.weight', 'c_lstm.1.bias_hh_l0', 'attention.v_proj.weight', 'q_lstm.bias_hh_l1_reverse', 'c_lstm.2.weight_hh_l2', 'c_lstm.0.bias_hh_l1', 'c_lstm.2.weight_ih_l0', 'c_lstm.2.weight_ih_l1', 'q_lstm.bias_ih_l0', 'c_lstm.0.weight_hh_l2_reverse', 'c_lstm.1.bias_hh_l1_reverse', 'q_lstm.weight_hh_l2', 'q_lstm.weight_ih_l1', 'c_lstm.0.weight_ih_l1_reverse']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
创建文件夹...
[22-05-05 17:36:20] --Trainer-- INFO: Load pretrained model from file /data2/wangbingchao/database/bert_pretrained/bert-base-uncased ...
[22-05-05 17:36:20] --Trainer-- INFO: Load pretrained model config finished!!!
[22-05-05 17:36:20] --Trainer-- INFO: Define model...
[22-05-05 17:36:22] --Trainer-- INFO: use cuda to train
[22-05-05 17:36:26] --Trainer-- INFO: Use Multi-GPUs[0, 1, 2]
[22-05-05 17:36:26] --DropDataloader-- INFO: Loading dataset from cached file /data2/maqi/LongTextDatasets/LongTextModels/cache/cache_train_hotpot_dev_fullwiki_v1.json_bert-base-uncased
[22-05-05 17:36:27] --Trainer-- INFO: Define model finished!!!
[22-05-05 17:36:27] --Trainer-- INFO: model config output dir: /data2/maqi/LongTextDatasets/LongTextModels/output/exp_5_5/config.txt
 0%|                                                                                                     |0/309[00:00<?]Epoch: 1/4:  0%|                                                                                         |0/309[00:00<?]/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/modules/rnn.py:662: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1616554827596/work/aten/src/ATen/native/cudnn/RNN.cpp:915.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Epoch: 1/4:  0%|                                                                            |0/309[00:11<?, loss=0.0693]Epoch: 1/4:  0%|                           |0/309[00:11<?, Iter=0, Loss=0.069315, Step=0, UsedTime=0:00:11, lr=0.000000]Epoch: 1/4:  0%|                       |1/309[00:11<57:32, Iter=0, Loss=0.069315, Step=0, UsedTime=0:00:11, lr=0.000000]Epoch: 1/4:  0%|                     |1/309[00:12<1:02:49, Iter=0, Loss=0.069315, Step=0, UsedTime=0:00:11, lr=0.000000]
Traceback (most recent call last):
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/runpy.py", line 183, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/runpy.py", line 109, in _get_module_details
    __import__(pkg_name)
  File "/data2/maqi/LongTextDatasets/LongTextModels/main/main.py", line 12, in <module>
    train_begin_time=train_begin)
  File "/data2/maqi/LongTextDatasets/LongTextModels/model/trainer.py", line 304, in run_train_epoch
    loss, _ = self.model(**inputs)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 167, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 177, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/_utils.py", line 429, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/data2/maqi/LongTextDatasets/LongTextModels/model/model.py", line 106, in forward
    contexts_embedding, _ = self.c_lstm[i](new_contexts_embedding)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/data2/maqi/.conda/envs/cleqa/lib/python3.7/site-packages/torch/nn/modules/rnn.py", line 662, in forward
    self.dropout, self.training, self.bidirectional, self.batch_first)
RuntimeError: CUDA out of memory. Tried to allocate 1.68 GiB (GPU 0; 11.91 GiB total capacity; 10.74 GiB already allocated; 217.06 MiB free; 10.97 GiB reserved in total by PyTorch)

